{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark session initialized successfully.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CELL 1: IMPORT LIBRARIES AND INITIALIZE SPARK\n",
    "# ============================================================\n",
    "\n",
    "# Import PySpark libraries\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import VectorAssembler, StringIndexer, StandardScaler\n",
    "from pyspark.ml.classification import LogisticRegression, RandomForestClassifier, GBTClassifier, DecisionTreeClassifier, NaiveBayes, MultilayerPerceptronClassifier, LinearSVC\n",
    "from pyspark.ml.regression import LinearRegression, RandomForestRegressor, GBTRegressor, DecisionTreeRegressor\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator, RegressionEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, TrainValidationSplit, CrossValidator\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"AviationTrendAnalysis\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"Spark session initialized successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully: 245,955 rows, 23 columns\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CELL 2: LOAD DATA FROM CSV FILE\n",
    "# ============================================================\n",
    "\n",
    "# Load aviation data from CSV file\n",
    "file_path = \"../data/US Airline Flight Routes and Fares 1993-2024.csv\"\n",
    "\n",
    "df = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"multiLine\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .option(\"quote\", '\"') \\\n",
    "    .option(\"escape\", '\"') \\\n",
    "    .csv(file_path)\n",
    "\n",
    "print(f\"Data loaded successfully: {df.count():,} rows, {len(df.columns)} columns\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data cleaning completed.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CELL 3: DATA CLEANING - WHITESPACE HANDLING\n",
    "# ============================================================\n",
    "\n",
    "# Import functions for data cleaning\n",
    "from pyspark.sql.functions import col, trim, regexp_replace\n",
    "\n",
    "# Identify string columns\n",
    "string_columns = [field.name for field in df.schema.fields if field.dataType.typeName() == 'string']\n",
    "\n",
    "# Remove leading/trailing whitespace\n",
    "for col_name in string_columns:\n",
    "    df = df.withColumn(col_name, trim(col(col_name)))\n",
    "\n",
    "# Normalize whitespace (replace multiple spaces with single space)\n",
    "for col_name in string_columns:\n",
    "    df = df.withColumn(col_name, regexp_replace(col(col_name), \"\\\\s+\", \" \"))\n",
    "\n",
    "print(\"Data cleaning completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data type conversion completed: 13 columns converted to double.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CELL 4: DATA TYPE CONVERSION\n",
    "# ============================================================\n",
    "\n",
    "# Define numeric columns to convert\n",
    "numeric_columns = ['Year', 'quarter', 'citymarketid_1', 'citymarketid_2', \n",
    "                   'airportid_1', 'airportid_2', 'nsmiles', 'passengers', \n",
    "                   'fare', 'large_ms', 'fare_lg', 'lf_ms', 'fare_low']\n",
    "\n",
    "# Convert numeric columns to double\n",
    "converted_count = 0\n",
    "for col_name in numeric_columns:\n",
    "    if col_name in df.columns:\n",
    "        df = df.withColumn(col_name, col(col_name).cast(\"double\"))\n",
    "        converted_count += 1\n",
    "\n",
    "print(f\"Data type conversion completed: {converted_count} columns converted to double.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values analysis completed: 87,868 total missing values in 8 columns.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CELL 5: MISSING VALUES CHECK\n",
    "# ============================================================\n",
    "\n",
    "# Import functions for missing value analysis\n",
    "from pyspark.sql.functions import isnan, isnull\n",
    "\n",
    "# Analyze missing values\n",
    "missing_summary = []\n",
    "total_missing = 0\n",
    "\n",
    "for col_name in df.columns:\n",
    "    # Count null values\n",
    "    null_count = df.filter(col(col_name).isNull()).count()\n",
    "    \n",
    "    # Count NaN values (for numeric columns only)\n",
    "    nan_count = 0\n",
    "    if col_name in numeric_columns:\n",
    "        nan_count = df.filter(isnan(col(col_name))).count()\n",
    "    \n",
    "    # Total missing values\n",
    "    col_missing = null_count + nan_count\n",
    "    total_missing += col_missing\n",
    "    \n",
    "    if col_missing > 0:\n",
    "        missing_summary.append((col_name, col_missing, null_count, nan_count))\n",
    "\n",
    "print(f\"Missing values analysis completed: {total_missing:,} total missing values in {len(missing_summary)} columns.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values handling completed: 1,612 rows removed (0.66%)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CELL 6: MISSING VALUES HANDLING (EXCLUDING GEOCODED COLUMNS)\n",
    "# ============================================================\n",
    "\n",
    "# Define columns to exclude from missing value removal\n",
    "exclude_columns = ['Geocoded_City1', 'Geocoded_City2']\n",
    "\n",
    "# Create condition to check missing values (excluding excluded columns)\n",
    "missing_condition = None\n",
    "checked_columns = []\n",
    "\n",
    "for col_name in df.columns:\n",
    "    if col_name not in exclude_columns:\n",
    "        checked_columns.append(col_name)\n",
    "        if missing_condition is None:\n",
    "            missing_condition = col(col_name).isNull() | isnan(col(col_name))\n",
    "        else:\n",
    "            missing_condition = missing_condition | col(col_name).isNull() | isnan(col(col_name))\n",
    "\n",
    "# Remove rows with missing values in important columns\n",
    "df_clean = df.filter(~missing_condition)\n",
    "\n",
    "# Calculate statistics\n",
    "original_count = df.count()\n",
    "clean_count = df_clean.count()\n",
    "removed_count = original_count - clean_count\n",
    "removed_percentage = (removed_count / original_count) * 100\n",
    "\n",
    "print(f\"Missing values handling completed: {removed_count:,} rows removed ({removed_percentage:.2f}%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data verification completed: 244,343 rows, 23 columns\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CELL 7: DATA CLEANLINESS VERIFICATION\n",
    "# ============================================================\n",
    "\n",
    "# Check missing values in important columns\n",
    "important_columns = [col for col in df_clean.columns if col not in exclude_columns]\n",
    "all_clean = True\n",
    "\n",
    "for col_name in important_columns:\n",
    "    null_count = df_clean.filter(col(col_name).isNull()).count()\n",
    "    nan_count = df_clean.filter(isnan(col(col_name))).count()\n",
    "    total_missing = null_count + nan_count\n",
    "    \n",
    "    if total_missing > 0:\n",
    "        all_clean = False\n",
    "\n",
    "# Cache data for performance\n",
    "df_clean.cache()\n",
    "df_clean.count()  # Trigger caching\n",
    "\n",
    "print(f\"Data verification completed: {df_clean.count():,} rows, {len(df_clean.columns)} columns\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quarterly data preparation completed: 118 quarters aggregated.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CELL 8: PREPARE QUARTERLY DATA FOR MACHINE LEARNING\n",
    "# ============================================================\n",
    "\n",
    "# Import ML libraries\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import Window\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "from pyspark.ml.classification import LogisticRegression, RandomForestClassifier, GBTClassifier\n",
    "from pyspark.ml.regression import LinearRegression, RandomForestRegressor, GBTRegressor\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, RegressionEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml import Pipeline\n",
    "import time\n",
    "\n",
    "# Aggregate data by Year-Quarter\n",
    "df_quarterly = df_clean.groupBy('Year', 'quarter').agg(\n",
    "    # Volume metrics\n",
    "    count('*').alias('num_routes'),\n",
    "    sum('passengers').alias('total_passengers'),\n",
    "    avg('passengers').alias('avg_passengers_per_route'),\n",
    "\n",
    "    # Price metrics\n",
    "    avg('fare').alias('avg_fare'),\n",
    "    stddev('fare').alias('fare_std'),\n",
    "    min('fare').alias('fare_min'),\n",
    "    max('fare').alias('fare_max'),\n",
    "\n",
    "    # Distance metrics\n",
    "    avg('nsmiles').alias('avg_distance'),\n",
    "    stddev('nsmiles').alias('distance_std'),\n",
    "\n",
    "    # Competition metrics\n",
    "    countDistinct('carrier_lg').alias('num_carriers'),\n",
    "    avg('large_ms').alias('avg_market_share_large'),\n",
    "    avg('lf_ms').alias('avg_market_share_lowcost')\n",
    ").orderBy('Year', 'quarter')\n",
    "\n",
    "# Create time_period identifier\n",
    "df_quarterly = df_quarterly.withColumn('time_period',\n",
    "    concat(col('Year').cast('string'), lit('-Q'), col('quarter').cast('string'))\n",
    ")\n",
    "\n",
    "# Create labels (COVID = crisis)\n",
    "df_quarterly = df_quarterly.withColumn('is_crisis',\n",
    "    when((col('Year') >= 2020) & (col('Year') <= 2021), 1.0)\n",
    "    .otherwise(0.0)\n",
    ")\n",
    "\n",
    "print(f\"Quarterly data preparation completed: {df_quarterly.count()} quarters aggregated.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature engineering completed.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CELL 9: FEATURE ENGINEERING FOR QUARTERLY DATA\n",
    "# ============================================================\n",
    "\n",
    "# Window specs for time-based calculations\n",
    "window_qoq = Window.orderBy('Year', 'quarter')\n",
    "\n",
    "# Calculate QoQ (Quarter-over-Quarter) changes\n",
    "change_cols = ['num_routes', 'total_passengers', 'avg_fare', 'avg_distance']\n",
    "\n",
    "for col_name in change_cols:\n",
    "    # Get previous quarter value\n",
    "    df_quarterly = df_quarterly.withColumn(\n",
    "        f'{col_name}_prev_q',\n",
    "        lag(col(col_name), 1).over(window_qoq)\n",
    "    )\n",
    "\n",
    "    # Calculate % change\n",
    "    df_quarterly = df_quarterly.withColumn(\n",
    "        f'{col_name}_change_qoq',\n",
    "        when(col(f'{col_name}_prev_q').isNotNull() & (col(f'{col_name}_prev_q') != 0),\n",
    "             (col(col_name) - col(f'{col_name}_prev_q')) / col(f'{col_name}_prev_q'))\n",
    "        .otherwise(0.0)\n",
    "    )\n",
    "\n",
    "    # Drop temp column\n",
    "    df_quarterly = df_quarterly.drop(f'{col_name}_prev_q')\n",
    "\n",
    "# Calculate YoY (Year-over-Year) changes\n",
    "yoy_cols = ['num_routes', 'total_passengers', 'avg_fare']\n",
    "\n",
    "for col_name in yoy_cols:\n",
    "    df_quarterly = df_quarterly.withColumn(\n",
    "        f'{col_name}_prev_year',\n",
    "        lag(col(col_name), 4).over(window_qoq)\n",
    "    )\n",
    "\n",
    "    df_quarterly = df_quarterly.withColumn(\n",
    "        f'{col_name}_change_yoy',\n",
    "        when(col(f'{col_name}_prev_year').isNotNull() & (col(f'{col_name}_prev_year') != 0),\n",
    "             (col(col_name) - col(f'{col_name}_prev_year')) / col(f'{col_name}_prev_year'))\n",
    "        .otherwise(0.0)\n",
    "    )\n",
    "\n",
    "    df_quarterly = df_quarterly.drop(f'{col_name}_prev_year')\n",
    "\n",
    "# Create derived features\n",
    "# Volatility metrics\n",
    "df_quarterly = df_quarterly.withColumn('fare_volatility',\n",
    "    when(col('avg_fare') != 0, col('fare_std') / col('avg_fare')).otherwise(0.0)\n",
    ")\n",
    "\n",
    "df_quarterly = df_quarterly.withColumn('distance_volatility',\n",
    "    when(col('avg_distance') != 0, col('distance_std') / col('avg_distance')).otherwise(0.0)\n",
    ")\n",
    "\n",
    "# Range metrics\n",
    "df_quarterly = df_quarterly.withColumn('fare_range',\n",
    "    col('fare_max') - col('fare_min')\n",
    ")\n",
    "\n",
    "# Passenger efficiency\n",
    "df_quarterly = df_quarterly.withColumn('passenger_efficiency',\n",
    "    when(col('num_routes') != 0, col('total_passengers') / col('num_routes')).otherwise(0.0)\n",
    ")\n",
    "\n",
    "# Handle missing values\n",
    "df_quarterly = df_quarterly.fillna(0.0)\n",
    "\n",
    "print(\"Feature engineering completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quarterly data preparation completed: 20 features\n",
      "Training quarters: 105\n",
      "Test quarters: 13\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CELL 10: SELECT FEATURES AND PREPARE TRAINING DATA\n",
    "# ============================================================\n",
    "\n",
    "# Select final features for quarterly ML\n",
    "feature_cols_quarterly = [\n",
    "    # Core metrics (8)\n",
    "    'num_routes', 'total_passengers', 'avg_passengers_per_route',\n",
    "    'avg_fare', 'avg_distance', 'fare_volatility', 'fare_range',\n",
    "    'passenger_efficiency',\n",
    "    \n",
    "    # Market metrics (3)\n",
    "    'num_carriers', 'avg_market_share_large', 'avg_market_share_lowcost',\n",
    "    \n",
    "    # QoQ changes (4)\n",
    "    'num_routes_change_qoq', 'total_passengers_change_qoq', \n",
    "    'avg_fare_change_qoq', 'avg_distance_change_qoq',\n",
    "    \n",
    "    # YoY changes (3)\n",
    "    'num_routes_change_yoy', 'total_passengers_change_yoy', \n",
    "    'avg_fare_change_yoy',\n",
    "    \n",
    "    # Time features (2)\n",
    "    'Year', 'quarter'\n",
    "]\n",
    "\n",
    "# Vector Assembler for quarterly features\n",
    "assembler_quarterly = VectorAssembler(\n",
    "    inputCols=feature_cols_quarterly,\n",
    "    outputCol=\"features\"\n",
    ")\n",
    "\n",
    "# Standard Scaler for quarterly features\n",
    "scaler_quarterly = StandardScaler(\n",
    "    inputCol=\"features\",\n",
    "    outputCol=\"scaled_features\",\n",
    "    withStd=True,\n",
    "    withMean=True\n",
    ")\n",
    "\n",
    "# Temporal split for quarterly data\n",
    "train_data_quarterly = df_quarterly.filter(col('Year') <= 2020)  # Pre-COVID + 2020\n",
    "test_data_quarterly = df_quarterly.filter(col('Year') > 2020)    # 2021+ (Post-COVID)\n",
    "\n",
    "# Cache data for performance\n",
    "train_data_quarterly.cache()\n",
    "test_data_quarterly.cache()\n",
    "\n",
    "print(f\"Quarterly data preparation completed: {len(feature_cols_quarterly)} features\")\n",
    "print(f\"Training quarters: {train_data_quarterly.count():,}\")\n",
    "print(f\"Test quarters: {test_data_quarterly.count():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QUARTERLY DATASET OVERVIEW:\n",
      "  Total quarters: 118\n",
      "  Training quarters: 105\n",
      "  Test quarters: 13\n",
      "\n",
      "QUARTERLY CLASS DISTRIBUTION:\n",
      "  - Normal Quarters: 110 quarters (93.2%)\n",
      "  - COVID Crisis Quarters: 8 quarters (6.8%)\n",
      "\n",
      "Quarterly imbalance ratio: 13.8:1 (Normal:Crisis)\n",
      "SEVERE CLASS IMBALANCE (>10:1) - Quarterly Level\n",
      "\n",
      "Quarterly analysis completed.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CELL 11: CLASS IMBALANCE ANALYSIS FOR QUARTERLY DATA\n",
    "# ============================================================\n",
    "\n",
    "print(\"QUARTERLY DATASET OVERVIEW:\")\n",
    "print(f\"  Total quarters: {df_quarterly.count():,}\")\n",
    "print(f\"  Training quarters: {train_data_quarterly.count():,}\")\n",
    "print(f\"  Test quarters: {test_data_quarterly.count():,}\")\n",
    "\n",
    "# Class distribution analysis for quarterly data\n",
    "quarterly_distribution = df_quarterly.groupBy('is_crisis').count().collect()\n",
    "total_quarters = df_quarterly.count()\n",
    "\n",
    "print(\"\\nQUARTERLY CLASS DISTRIBUTION:\")\n",
    "for row in quarterly_distribution:\n",
    "    class_label = \"COVID Crisis Quarters\" if row['is_crisis'] == 1.0 else \"Normal Quarters\"\n",
    "    count = row['count']\n",
    "    percentage = (count / total_quarters) * 100\n",
    "    print(f\"  - {class_label}: {count} quarters ({percentage:.1f}%)\")\n",
    "\n",
    "# Calculate imbalance ratio for quarterly data\n",
    "crisis_quarters = next(row['count'] for row in quarterly_distribution if row['is_crisis'] == 1.0)\n",
    "normal_quarters = next(row['count'] for row in quarterly_distribution if row['is_crisis'] == 0.0)\n",
    "imbalance_ratio_quarterly = normal_quarters / crisis_quarters\n",
    "\n",
    "print(f\"\\nQuarterly imbalance ratio: {imbalance_ratio_quarterly:.1f}:1 (Normal:Crisis)\")\n",
    "\n",
    "if imbalance_ratio_quarterly > 10:\n",
    "    print(\"SEVERE CLASS IMBALANCE (>10:1) - Quarterly Level\")\n",
    "elif imbalance_ratio_quarterly > 5:\n",
    "    print(\"SIGNIFICANT CLASS IMBALANCE (>5:1) - Quarterly Level\")\n",
    "else:\n",
    "    print(\"Class balance is acceptable - Quarterly Level\")\n",
    "\n",
    "print(\"\\nQuarterly analysis completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quarterly class weights - Crisis: 7.38, Normal: 0.54\n",
      "Weighted quarterly dataset created: 105 quarters\n",
      "\n",
      "Quarterly class weight distribution:\n",
      "+---------+------------------+-----+\n",
      "|is_crisis|      class_weight|count|\n",
      "+---------+------------------+-----+\n",
      "|      0.0|0.5363636363636364|  101|\n",
      "|      1.0|             7.375|    4|\n",
      "+---------+------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CELL 12: CLASS WEIGHTING FOR QUARTERLY DATA\n",
    "# ============================================================\n",
    "\n",
    "from pyspark.sql.functions import when, col\n",
    "\n",
    "# Calculate class weights for quarterly data\n",
    "crisis_quarters_count = crisis_quarters\n",
    "normal_quarters_count = normal_quarters\n",
    "total_quarters_count = crisis_quarters_count + normal_quarters_count\n",
    "\n",
    "# Calculate weights (inverse frequency)\n",
    "weight_crisis_quarters = total_quarters_count / (2 * crisis_quarters_count)\n",
    "weight_normal_quarters = total_quarters_count / (2 * normal_quarters_count)\n",
    "\n",
    "print(f\"Quarterly class weights - Crisis: {weight_crisis_quarters:.2f}, Normal: {weight_normal_quarters:.2f}\")\n",
    "\n",
    "# Create weighted dataset for quarterly training\n",
    "train_data_quarterly_weighted = train_data_quarterly.withColumn(\n",
    "    \"class_weight\",\n",
    "    when(col(\"is_crisis\") == 1.0, weight_crisis_quarters).otherwise(weight_normal_quarters)\n",
    ")\n",
    "\n",
    "# Verify the weighting\n",
    "print(f\"Weighted quarterly dataset created: {train_data_quarterly_weighted.count():,} quarters\")\n",
    "\n",
    "# Check the class_weight distribution\n",
    "print(\"\\nQuarterly class weight distribution:\")\n",
    "train_data_quarterly_weighted.groupBy(\"is_crisis\", \"class_weight\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MODEL 1: LOGISTIC REGRESSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Logistic Regression (Quarterly, Class Weighted)\n",
      "AUC: 0.2778\n",
      "Accuracy: 0.7692\n",
      "Precision: 0.8269\n",
      "Recall: 0.7692\n",
      "F1-Score: 0.7165\n",
      "Training time: 2.76 seconds\n",
      "\n",
      "Quarterly Prediction Distribution:\n",
      "  Crisis Quarters: 1 quarters\n",
      "  Normal Quarters: 12 quarters\n",
      "\n",
      "Quarterly Confusion Matrix:\n",
      "  Crisis Quarters → Crisis Quarters: 1 quarters\n",
      "  Crisis Quarters → Normal Quarters: 3 quarters\n",
      "  Normal Quarters → Normal Quarters: 9 quarters\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CELL 13: MODEL 1 - LOGISTIC REGRESSION (QUARTERLY)\n",
    "# ============================================================\n",
    "\n",
    "# Create pipeline with class weighting for quarterly data\n",
    "logistic_reg_quarterly = LogisticRegression(\n",
    "    featuresCol=\"scaled_features\", \n",
    "    labelCol=\"is_crisis\",\n",
    "    weightCol=\"class_weight\",\n",
    "    maxIter=100,\n",
    "    regParam=0.01\n",
    ")\n",
    "logistic_pipeline_quarterly = Pipeline(stages=[assembler_quarterly, scaler_quarterly, logistic_reg_quarterly])\n",
    "\n",
    "# Train model with weighted quarterly data\n",
    "start_time = time.time()\n",
    "logistic_model_quarterly = logistic_pipeline_quarterly.fit(train_data_quarterly_weighted)\n",
    "logistic_time_quarterly = time.time() - start_time\n",
    "\n",
    "# Predictions on quarterly test data\n",
    "logistic_predictions_quarterly = logistic_model_quarterly.transform(test_data_quarterly)\n",
    "\n",
    "# Evaluate with comprehensive metrics\n",
    "auc_evaluator = BinaryClassificationEvaluator(labelCol=\"is_crisis\", metricName=\"areaUnderROC\")\n",
    "accuracy_evaluator = MulticlassClassificationEvaluator(labelCol=\"is_crisis\", metricName=\"accuracy\")\n",
    "precision_evaluator = MulticlassClassificationEvaluator(labelCol=\"is_crisis\", metricName=\"weightedPrecision\")\n",
    "recall_evaluator = MulticlassClassificationEvaluator(labelCol=\"is_crisis\", metricName=\"weightedRecall\")\n",
    "f1_evaluator = MulticlassClassificationEvaluator(labelCol=\"is_crisis\", metricName=\"f1\")\n",
    "\n",
    "auc = auc_evaluator.evaluate(logistic_predictions_quarterly)\n",
    "accuracy = accuracy_evaluator.evaluate(logistic_predictions_quarterly)\n",
    "precision = precision_evaluator.evaluate(logistic_predictions_quarterly)\n",
    "recall = recall_evaluator.evaluate(logistic_predictions_quarterly)\n",
    "f1 = f1_evaluator.evaluate(logistic_predictions_quarterly)\n",
    "\n",
    "# Results\n",
    "print(\"Model: Logistic Regression (Quarterly, Class Weighted)\")\n",
    "print(f\"AUC: {auc:.4f}\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1-Score: {f1:.4f}\")\n",
    "print(f\"Training time: {logistic_time_quarterly:.2f} seconds\")\n",
    "\n",
    "# Check prediction distribution\n",
    "print(\"\\nQuarterly Prediction Distribution:\")\n",
    "pred_dist = logistic_predictions_quarterly.groupBy(\"prediction\").count().collect()\n",
    "for row in pred_dist:\n",
    "    class_name = \"Crisis Quarters\" if row['prediction'] == 1.0 else \"Normal Quarters\"\n",
    "    print(f\"  {class_name}: {row['count']:,} quarters\")\n",
    "\n",
    "# Confusion Matrix\n",
    "print(\"\\nQuarterly Confusion Matrix:\")\n",
    "confusion_matrix = logistic_predictions_quarterly.groupBy(\"is_crisis\", \"prediction\").count().collect()\n",
    "for row in confusion_matrix:\n",
    "    actual = \"Crisis Quarters\" if row['is_crisis'] == 1.0 else \"Normal Quarters\"\n",
    "    predicted = \"Crisis Quarters\" if row['prediction'] == 1.0 else \"Normal Quarters\"\n",
    "    print(f\"  {actual} → {predicted}: {row['count']:,} quarters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MODEL 2: DECISION TREE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Decision Tree (Quarterly, Class Weighted)\n",
      "AUC: 0.6250\n",
      "Accuracy: 0.7692\n",
      "Precision: 0.8269\n",
      "Recall: 0.7692\n",
      "F1-Score: 0.7165\n",
      "Training time: 0.69 seconds\n",
      "\n",
      "Quarterly Prediction Distribution:\n",
      "  Crisis Quarters: 1 quarters\n",
      "  Normal Quarters: 12 quarters\n",
      "\n",
      "Quarterly Confusion Matrix:\n",
      "  Crisis Quarters → Crisis Quarters: 1 quarters\n",
      "  Crisis Quarters → Normal Quarters: 3 quarters\n",
      "  Normal Quarters → Normal Quarters: 9 quarters\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CELL 14: MODEL 2 - DECISION TREE (QUARTERLY)\n",
    "# ============================================================\n",
    "\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "\n",
    "# Create pipeline with Decision Tree for quarterly data\n",
    "dt_classifier_quarterly = DecisionTreeClassifier(\n",
    "    featuresCol=\"features\", \n",
    "    labelCol=\"is_crisis\",\n",
    "    weightCol=\"class_weight\",\n",
    "    maxDepth=15,\n",
    "    maxBins=64,\n",
    "    minInstancesPerNode=5\n",
    ")\n",
    "dt_pipeline_quarterly = Pipeline(stages=[assembler_quarterly, dt_classifier_quarterly])\n",
    "\n",
    "# Train model with weighted quarterly data\n",
    "start_time = time.time()\n",
    "dt_model_quarterly = dt_pipeline_quarterly.fit(train_data_quarterly_weighted)\n",
    "dt_time_quarterly = time.time() - start_time\n",
    "\n",
    "# Predictions on quarterly test data\n",
    "dt_predictions_quarterly = dt_model_quarterly.transform(test_data_quarterly)\n",
    "\n",
    "# Evaluate with comprehensive metrics\n",
    "auc_evaluator = BinaryClassificationEvaluator(labelCol=\"is_crisis\", metricName=\"areaUnderROC\")\n",
    "accuracy_evaluator = MulticlassClassificationEvaluator(labelCol=\"is_crisis\", metricName=\"accuracy\")\n",
    "precision_evaluator = MulticlassClassificationEvaluator(labelCol=\"is_crisis\", metricName=\"weightedPrecision\")\n",
    "recall_evaluator = MulticlassClassificationEvaluator(labelCol=\"is_crisis\", metricName=\"weightedRecall\")\n",
    "f1_evaluator = MulticlassClassificationEvaluator(labelCol=\"is_crisis\", metricName=\"f1\")\n",
    "\n",
    "auc = auc_evaluator.evaluate(dt_predictions_quarterly)\n",
    "accuracy = accuracy_evaluator.evaluate(dt_predictions_quarterly)\n",
    "precision = precision_evaluator.evaluate(dt_predictions_quarterly)\n",
    "recall = recall_evaluator.evaluate(dt_predictions_quarterly)\n",
    "f1 = f1_evaluator.evaluate(dt_predictions_quarterly)\n",
    "\n",
    "# Results\n",
    "print(\"Model: Decision Tree (Quarterly, Class Weighted)\")\n",
    "print(f\"AUC: {auc:.4f}\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1-Score: {f1:.4f}\")\n",
    "print(f\"Training time: {dt_time_quarterly:.2f} seconds\")\n",
    "\n",
    "# Check prediction distribution\n",
    "print(\"\\nQuarterly Prediction Distribution:\")\n",
    "pred_dist = dt_predictions_quarterly.groupBy(\"prediction\").count().collect()\n",
    "for row in pred_dist:\n",
    "    class_name = \"Crisis Quarters\" if row['prediction'] == 1.0 else \"Normal Quarters\"\n",
    "    print(f\"  {class_name}: {row['count']:,} quarters\")\n",
    "\n",
    "# Confusion Matrix\n",
    "print(\"\\nQuarterly Confusion Matrix:\")\n",
    "confusion_matrix = dt_predictions_quarterly.groupBy(\"is_crisis\", \"prediction\").count().collect()\n",
    "for row in confusion_matrix:\n",
    "    actual = \"Crisis Quarters\" if row['is_crisis'] == 1.0 else \"Normal Quarters\"\n",
    "    predicted = \"Crisis Quarters\" if row['prediction'] == 1.0 else \"Normal Quarters\"\n",
    "    print(f\"  {actual} → {predicted}: {row['count']:,} quarters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MODEL 3: RANDOM FOREST\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Random Forest (Quarterly, Class Weighted)\n",
      "AUC: 0.7500\n",
      "Accuracy: 0.7692\n",
      "Precision: 0.8269\n",
      "Recall: 0.7692\n",
      "F1-Score: 0.7165\n",
      "Training time: 0.91 seconds\n",
      "\n",
      "Quarterly Prediction Distribution:\n",
      "  Crisis Quarters: 1 quarters\n",
      "  Normal Quarters: 12 quarters\n",
      "\n",
      "Quarterly Confusion Matrix:\n",
      "  Crisis Quarters → Crisis Quarters: 1 quarters\n",
      "  Crisis Quarters → Normal Quarters: 3 quarters\n",
      "  Normal Quarters → Normal Quarters: 9 quarters\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CELL 15: MODEL 3 - RANDOM FOREST (QUARTERLY)\n",
    "# ============================================================\n",
    "\n",
    "# Create pipeline with Random Forest for quarterly data\n",
    "rf_classifier_quarterly = RandomForestClassifier(\n",
    "    featuresCol=\"features\", \n",
    "    labelCol=\"is_crisis\",\n",
    "    weightCol=\"class_weight\",\n",
    "    numTrees=200,\n",
    "    maxDepth=15,\n",
    "    maxBins=64,\n",
    "    minInstancesPerNode=5,\n",
    "    seed=42\n",
    ")\n",
    "rf_pipeline_quarterly = Pipeline(stages=[assembler_quarterly, rf_classifier_quarterly])\n",
    "\n",
    "# Train model with weighted quarterly data\n",
    "start_time = time.time()\n",
    "rf_model_quarterly = rf_pipeline_quarterly.fit(train_data_quarterly_weighted)\n",
    "rf_time_quarterly = time.time() - start_time\n",
    "\n",
    "# Predictions on quarterly test data\n",
    "rf_predictions_quarterly = rf_model_quarterly.transform(test_data_quarterly)\n",
    "\n",
    "# Evaluate with comprehensive metrics\n",
    "auc_evaluator = BinaryClassificationEvaluator(labelCol=\"is_crisis\", metricName=\"areaUnderROC\")\n",
    "accuracy_evaluator = MulticlassClassificationEvaluator(labelCol=\"is_crisis\", metricName=\"accuracy\")\n",
    "precision_evaluator = MulticlassClassificationEvaluator(labelCol=\"is_crisis\", metricName=\"weightedPrecision\")\n",
    "recall_evaluator = MulticlassClassificationEvaluator(labelCol=\"is_crisis\", metricName=\"weightedRecall\")\n",
    "f1_evaluator = MulticlassClassificationEvaluator(labelCol=\"is_crisis\", metricName=\"f1\")\n",
    "\n",
    "auc = auc_evaluator.evaluate(rf_predictions_quarterly)\n",
    "accuracy = accuracy_evaluator.evaluate(rf_predictions_quarterly)\n",
    "precision = precision_evaluator.evaluate(rf_predictions_quarterly)\n",
    "recall = recall_evaluator.evaluate(rf_predictions_quarterly)\n",
    "f1 = f1_evaluator.evaluate(rf_predictions_quarterly)\n",
    "\n",
    "# Results\n",
    "print(\"Model: Random Forest (Quarterly, Class Weighted)\")\n",
    "print(f\"AUC: {auc:.4f}\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1-Score: {f1:.4f}\")\n",
    "print(f\"Training time: {rf_time_quarterly:.2f} seconds\")\n",
    "\n",
    "# Check prediction distribution\n",
    "print(\"\\nQuarterly Prediction Distribution:\")\n",
    "pred_dist = rf_predictions_quarterly.groupBy(\"prediction\").count().collect()\n",
    "for row in pred_dist:\n",
    "    class_name = \"Crisis Quarters\" if row['prediction'] == 1.0 else \"Normal Quarters\"\n",
    "    print(f\"  {class_name}: {row['count']:,} quarters\")\n",
    "\n",
    "# Confusion Matrix\n",
    "print(\"\\nQuarterly Confusion Matrix:\")\n",
    "confusion_matrix = rf_predictions_quarterly.groupBy(\"is_crisis\", \"prediction\").count().collect()\n",
    "for row in confusion_matrix:\n",
    "    actual = \"Crisis Quarters\" if row['is_crisis'] == 1.0 else \"Normal Quarters\"\n",
    "    predicted = \"Crisis Quarters\" if row['prediction'] == 1.0 else \"Normal Quarters\"\n",
    "    print(f\"  {actual} → {predicted}: {row['count']:,} quarters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MODEL 4: GRADIENT BOOSTING TREES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Gradient Boosting Trees (Quarterly, Class Weighted)\n",
      "AUC: 0.6944\n",
      "Accuracy: 0.7692\n",
      "Precision: 0.8269\n",
      "Recall: 0.7692\n",
      "F1-Score: 0.7165\n",
      "Training time: 27.41 seconds\n",
      "\n",
      "Quarterly Prediction Distribution:\n",
      "  Crisis Quarters: 1 quarters\n",
      "  Normal Quarters: 12 quarters\n",
      "\n",
      "Quarterly Confusion Matrix:\n",
      "  Crisis Quarters → Crisis Quarters: 1 quarters\n",
      "  Crisis Quarters → Normal Quarters: 3 quarters\n",
      "  Normal Quarters → Normal Quarters: 9 quarters\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CELL 16: MODEL 4 - GRADIENT BOOSTING TREES (QUARTERLY)\n",
    "# ============================================================\n",
    "\n",
    "# Create pipeline with Gradient Boosting for quarterly data\n",
    "gbt_classifier_quarterly = GBTClassifier(\n",
    "    featuresCol=\"features\", \n",
    "    labelCol=\"is_crisis\",\n",
    "    weightCol=\"class_weight\",\n",
    "    maxIter=100,\n",
    "    maxDepth=8,\n",
    "    maxBins=64,\n",
    "    minInstancesPerNode=5,\n",
    "    seed=42\n",
    ")\n",
    "gbt_pipeline_quarterly = Pipeline(stages=[assembler_quarterly, gbt_classifier_quarterly])\n",
    "\n",
    "# Train model with weighted quarterly data\n",
    "start_time = time.time()\n",
    "gbt_model_quarterly = gbt_pipeline_quarterly.fit(train_data_quarterly_weighted)\n",
    "gbt_time_quarterly = time.time() - start_time\n",
    "\n",
    "# Predictions on quarterly test data\n",
    "gbt_predictions_quarterly = gbt_model_quarterly.transform(test_data_quarterly)\n",
    "\n",
    "# Evaluate with comprehensive metrics\n",
    "auc_evaluator = BinaryClassificationEvaluator(labelCol=\"is_crisis\", metricName=\"areaUnderROC\")\n",
    "accuracy_evaluator = MulticlassClassificationEvaluator(labelCol=\"is_crisis\", metricName=\"accuracy\")\n",
    "precision_evaluator = MulticlassClassificationEvaluator(labelCol=\"is_crisis\", metricName=\"weightedPrecision\")\n",
    "recall_evaluator = MulticlassClassificationEvaluator(labelCol=\"is_crisis\", metricName=\"weightedRecall\")\n",
    "f1_evaluator = MulticlassClassificationEvaluator(labelCol=\"is_crisis\", metricName=\"f1\")\n",
    "\n",
    "auc = auc_evaluator.evaluate(gbt_predictions_quarterly)\n",
    "accuracy = accuracy_evaluator.evaluate(gbt_predictions_quarterly)\n",
    "precision = precision_evaluator.evaluate(gbt_predictions_quarterly)\n",
    "recall = recall_evaluator.evaluate(gbt_predictions_quarterly)\n",
    "f1 = f1_evaluator.evaluate(gbt_predictions_quarterly)\n",
    "\n",
    "# Results\n",
    "print(\"Model: Gradient Boosting Trees (Quarterly, Class Weighted)\")\n",
    "print(f\"AUC: {auc:.4f}\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1-Score: {f1:.4f}\")\n",
    "print(f\"Training time: {gbt_time_quarterly:.2f} seconds\")\n",
    "\n",
    "# Check prediction distribution\n",
    "print(\"\\nQuarterly Prediction Distribution:\")\n",
    "pred_dist = gbt_predictions_quarterly.groupBy(\"prediction\").count().collect()\n",
    "for row in pred_dist:\n",
    "    class_name = \"Crisis Quarters\" if row['prediction'] == 1.0 else \"Normal Quarters\"\n",
    "    print(f\"  {class_name}: {row['count']:,} quarters\")\n",
    "\n",
    "# Confusion Matrix\n",
    "print(\"\\nQuarterly Confusion Matrix:\")\n",
    "confusion_matrix = gbt_predictions_quarterly.groupBy(\"is_crisis\", \"prediction\").count().collect()\n",
    "for row in confusion_matrix:\n",
    "    actual = \"Crisis Quarters\" if row['is_crisis'] == 1.0 else \"Normal Quarters\"\n",
    "    predicted = \"Crisis Quarters\" if row['prediction'] == 1.0 else \"Normal Quarters\"\n",
    "    print(f\"  {actual} → {predicted}: {row['count']:,} quarters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MODEL 5: NAIVE BAYES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Naive Bayes (Quarterly, Class Weighted)\n",
      "AUC: 0.7500\n",
      "Accuracy: 0.7692\n",
      "Precision: 0.8269\n",
      "Recall: 0.7692\n",
      "F1-Score: 0.7165\n",
      "Training time: 0.33 seconds\n",
      "\n",
      "Quarterly Prediction Distribution:\n",
      "  Crisis Quarters: 1 quarters\n",
      "  Normal Quarters: 12 quarters\n",
      "\n",
      "Quarterly Confusion Matrix:\n",
      "  Crisis Quarters → Crisis Quarters: 1 quarters\n",
      "  Crisis Quarters → Normal Quarters: 3 quarters\n",
      "  Normal Quarters → Normal Quarters: 9 quarters\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CELL 17: MODEL 5 - NAIVE BAYES (QUARTERLY)\n",
    "# ============================================================\n",
    "\n",
    "from pyspark.ml.classification import NaiveBayes\n",
    "\n",
    "# Select features suitable for Naive Bayes (positive values only)\n",
    "nb_feature_cols_quarterly = [\n",
    "    'num_routes', 'total_passengers', 'avg_passengers_per_route',\n",
    "    'avg_fare', 'avg_distance', 'fare_volatility', 'fare_range',\n",
    "    'passenger_efficiency', 'num_carriers', 'avg_market_share_large',\n",
    "    'Year', 'quarter'\n",
    "]\n",
    "\n",
    "# Vector Assembler for Naive Bayes features\n",
    "assembler_nb_quarterly = VectorAssembler(\n",
    "    inputCols=nb_feature_cols_quarterly,\n",
    "    outputCol=\"features\"\n",
    ")\n",
    "\n",
    "# Create pipeline with Naive Bayes for quarterly data\n",
    "nb_classifier_quarterly = NaiveBayes(\n",
    "    featuresCol=\"features\", \n",
    "    labelCol=\"is_crisis\",\n",
    "    weightCol=\"class_weight\",\n",
    "    smoothing=1.0\n",
    ")\n",
    "nb_pipeline_quarterly = Pipeline(stages=[assembler_nb_quarterly, nb_classifier_quarterly])\n",
    "\n",
    "# Train model with weighted quarterly data\n",
    "start_time = time.time()\n",
    "nb_model_quarterly = nb_pipeline_quarterly.fit(train_data_quarterly_weighted)\n",
    "nb_time_quarterly = time.time() - start_time\n",
    "\n",
    "# Predictions on quarterly test data\n",
    "nb_predictions_quarterly = nb_model_quarterly.transform(test_data_quarterly)\n",
    "\n",
    "# Evaluate with comprehensive metrics\n",
    "auc_evaluator = BinaryClassificationEvaluator(labelCol=\"is_crisis\", metricName=\"areaUnderROC\")\n",
    "accuracy_evaluator = MulticlassClassificationEvaluator(labelCol=\"is_crisis\", metricName=\"accuracy\")\n",
    "precision_evaluator = MulticlassClassificationEvaluator(labelCol=\"is_crisis\", metricName=\"weightedPrecision\")\n",
    "recall_evaluator = MulticlassClassificationEvaluator(labelCol=\"is_crisis\", metricName=\"weightedRecall\")\n",
    "f1_evaluator = MulticlassClassificationEvaluator(labelCol=\"is_crisis\", metricName=\"f1\")\n",
    "\n",
    "auc = auc_evaluator.evaluate(nb_predictions_quarterly)\n",
    "accuracy = accuracy_evaluator.evaluate(nb_predictions_quarterly)\n",
    "precision = precision_evaluator.evaluate(nb_predictions_quarterly)\n",
    "recall = recall_evaluator.evaluate(nb_predictions_quarterly)\n",
    "f1 = f1_evaluator.evaluate(nb_predictions_quarterly)\n",
    "\n",
    "# Results\n",
    "print(\"Model: Naive Bayes (Quarterly, Class Weighted)\")\n",
    "print(f\"AUC: {auc:.4f}\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1-Score: {f1:.4f}\")\n",
    "print(f\"Training time: {nb_time_quarterly:.2f} seconds\")\n",
    "\n",
    "# Check prediction distribution\n",
    "print(\"\\nQuarterly Prediction Distribution:\")\n",
    "pred_dist = nb_predictions_quarterly.groupBy(\"prediction\").count().collect()\n",
    "for row in pred_dist:\n",
    "    class_name = \"Crisis Quarters\" if row['prediction'] == 1.0 else \"Normal Quarters\"\n",
    "    print(f\"  {class_name}: {row['count']:,} quarters\")\n",
    "\n",
    "# Confusion Matrix\n",
    "print(\"\\nQuarterly Confusion Matrix:\")\n",
    "confusion_matrix = nb_predictions_quarterly.groupBy(\"is_crisis\", \"prediction\").count().collect()\n",
    "for row in confusion_matrix:\n",
    "    actual = \"Crisis Quarters\" if row['is_crisis'] == 1.0 else \"Normal Quarters\"\n",
    "    predicted = \"Crisis Quarters\" if row['prediction'] == 1.0 else \"Normal Quarters\"\n",
    "    print(f\"  {actual} → {predicted}: {row['count']:,} quarters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MODEL 6: SUPPORT VECTOR MACHINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Support Vector Machine (Quarterly, Class Weighted)\n",
      "AUC: 0.2500\n",
      "Accuracy: 0.7692\n",
      "Precision: 0.8269\n",
      "Recall: 0.7692\n",
      "F1-Score: 0.7165\n",
      "Training time: 26.92 seconds\n",
      "\n",
      "Quarterly Prediction Distribution:\n",
      "  Crisis Quarters: 1 quarters\n",
      "  Normal Quarters: 12 quarters\n",
      "\n",
      "Quarterly Confusion Matrix:\n",
      "  Crisis Quarters → Crisis Quarters: 1 quarters\n",
      "  Crisis Quarters → Normal Quarters: 3 quarters\n",
      "  Normal Quarters → Normal Quarters: 9 quarters\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CELL 18: MODEL 6 - SUPPORT VECTOR MACHINE (QUARTERLY)\n",
    "# ============================================================\n",
    "\n",
    "from pyspark.ml.classification import LinearSVC\n",
    "\n",
    "# Create pipeline with SVM for quarterly data\n",
    "svm_classifier_quarterly = LinearSVC(\n",
    "    featuresCol=\"scaled_features\", \n",
    "    labelCol=\"is_crisis\",\n",
    "    weightCol=\"class_weight\",\n",
    "    maxIter=200,\n",
    "    regParam=0.01,\n",
    "    threshold=0.5\n",
    ")\n",
    "svm_pipeline_quarterly = Pipeline(stages=[assembler_quarterly, scaler_quarterly, svm_classifier_quarterly])\n",
    "\n",
    "# Train model with weighted quarterly data\n",
    "start_time = time.time()\n",
    "svm_model_quarterly = svm_pipeline_quarterly.fit(train_data_quarterly_weighted)\n",
    "svm_time_quarterly = time.time() - start_time\n",
    "\n",
    "# Predictions on quarterly test data\n",
    "svm_predictions_quarterly = svm_model_quarterly.transform(test_data_quarterly)\n",
    "\n",
    "# Evaluate with comprehensive metrics\n",
    "auc_evaluator = BinaryClassificationEvaluator(labelCol=\"is_crisis\", metricName=\"areaUnderROC\")\n",
    "accuracy_evaluator = MulticlassClassificationEvaluator(labelCol=\"is_crisis\", metricName=\"accuracy\")\n",
    "precision_evaluator = MulticlassClassificationEvaluator(labelCol=\"is_crisis\", metricName=\"weightedPrecision\")\n",
    "recall_evaluator = MulticlassClassificationEvaluator(labelCol=\"is_crisis\", metricName=\"weightedRecall\")\n",
    "f1_evaluator = MulticlassClassificationEvaluator(labelCol=\"is_crisis\", metricName=\"f1\")\n",
    "\n",
    "auc = auc_evaluator.evaluate(svm_predictions_quarterly)\n",
    "accuracy = accuracy_evaluator.evaluate(svm_predictions_quarterly)\n",
    "precision = precision_evaluator.evaluate(svm_predictions_quarterly)\n",
    "recall = recall_evaluator.evaluate(svm_predictions_quarterly)\n",
    "f1 = f1_evaluator.evaluate(svm_predictions_quarterly)\n",
    "\n",
    "# Results\n",
    "print(\"Model: Support Vector Machine (Quarterly, Class Weighted)\")\n",
    "print(f\"AUC: {auc:.4f}\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1-Score: {f1:.4f}\")\n",
    "print(f\"Training time: {svm_time_quarterly:.2f} seconds\")\n",
    "\n",
    "# Check prediction distribution\n",
    "print(\"\\nQuarterly Prediction Distribution:\")\n",
    "pred_dist = svm_predictions_quarterly.groupBy(\"prediction\").count().collect()\n",
    "for row in pred_dist:\n",
    "    class_name = \"Crisis Quarters\" if row['prediction'] == 1.0 else \"Normal Quarters\"\n",
    "    print(f\"  {class_name}: {row['count']:,} quarters\")\n",
    "\n",
    "# Confusion Matrix\n",
    "print(\"\\nQuarterly Confusion Matrix:\")\n",
    "confusion_matrix = svm_predictions_quarterly.groupBy(\"is_crisis\", \"prediction\").count().collect()\n",
    "for row in confusion_matrix:\n",
    "    actual = \"Crisis Quarters\" if row['is_crisis'] == 1.0 else \"Normal Quarters\"\n",
    "    predicted = \"Crisis Quarters\" if row['prediction'] == 1.0 else \"Normal Quarters\"\n",
    "    print(f\"  {actual} → {predicted}: {row['count']:,} quarters\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
