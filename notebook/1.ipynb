{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9c41643",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark session created successfully!\n",
      "Loading data with multiLine option...\n",
      "Data loaded successfully!\n",
      "Dataset shape: (245,955 rows, 23 columns)\n",
      "Columns: ['tbl', 'Year', 'quarter', 'citymarketid_1', 'citymarketid_2', 'city1', 'city2', 'airportid_1', 'airportid_2', 'airport_1', 'airport_2', 'nsmiles', 'passengers', 'fare', 'carrier_lg', 'large_ms', 'fare_lg', 'carrier_low', 'lf_ms', 'fare_low', 'Geocoded_City1', 'Geocoded_City2', 'tbl1apk']\n",
      "\n",
      "First 5 rows:\n",
      "+-------+----+-------+--------------+--------------+------------------------------+-----------------------------+-----------+-----------+---------+---------+-------+----------+------+----------+--------+-------+-----------+------+--------+--------------+--------------+---------------------+\n",
      "|tbl    |Year|quarter|citymarketid_1|citymarketid_2|city1                         |city2                        |airportid_1|airportid_2|airport_1|airport_2|nsmiles|passengers|fare  |carrier_lg|large_ms|fare_lg|carrier_low|lf_ms |fare_low|Geocoded_City1|Geocoded_City2|tbl1apk              |\n",
      "+-------+----+-------+--------------+--------------+------------------------------+-----------------------------+-----------+-----------+---------+---------+-------+----------+------+----------+--------+-------+-----------+------+--------+--------------+--------------+---------------------+\n",
      "|Table1a|2021|3      |30135         |33195         |Allentown/Bethlehem/Easton, PA|Tampa, FL (Metropolitan Area)|10135      |14112      |ABE      |PIE      |970    |180       |81.43 |G4        |1.0     |81.43  |G4         |1.0   |81.43   |NULL          |NULL          |202131013514112ABEPIE|\n",
      "|Table1a|2021|3      |30135         |33195         |Allentown/Bethlehem/Easton, PA|Tampa, FL (Metropolitan Area)|10135      |15304      |ABE      |TPA      |970    |19        |208.93|DL        |0.4659  |219.98 |UA         |0.1193|154.11  |NULL          |NULL          |202131013515304ABETPA|\n",
      "|Table1a|2021|3      |30140         |30194         |Albuquerque, NM               |Dallas/Fort Worth, TX        |10140      |11259      |ABQ      |DAL      |580    |204       |184.56|WN        |0.9968  |184.44 |WN         |0.9968|184.44  |NULL          |NULL          |202131014011259ABQDAL|\n",
      "|Table1a|2021|3      |30140         |30194         |Albuquerque, NM               |Dallas/Fort Worth, TX        |10140      |11298      |ABQ      |DFW      |580    |264       |182.64|AA        |0.9774  |183.09 |AA         |0.9774|183.09  |NULL          |NULL          |202131014011298ABQDFW|\n",
      "|Table1a|2021|3      |30140         |30466         |Albuquerque, NM               |Phoenix, AZ                  |10140      |14107      |ABQ      |PHX      |328    |398       |177.11|WN        |0.6061  |184.49 |AA         |0.3939|165.77  |NULL          |NULL          |202131014014107ABQPHX|\n",
      "+-------+----+-------+--------------+--------------+------------------------------+-----------------------------+-----------+-----------+---------+---------+-------+----------+------+----------+--------+-------+-----------+------+--------+--------------+--------------+---------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "\n",
      "Data types:\n",
      "root\n",
      " |-- tbl: string (nullable = true)\n",
      " |-- Year: integer (nullable = true)\n",
      " |-- quarter: integer (nullable = true)\n",
      " |-- citymarketid_1: integer (nullable = true)\n",
      " |-- citymarketid_2: integer (nullable = true)\n",
      " |-- city1: string (nullable = true)\n",
      " |-- city2: string (nullable = true)\n",
      " |-- airportid_1: integer (nullable = true)\n",
      " |-- airportid_2: integer (nullable = true)\n",
      " |-- airport_1: string (nullable = true)\n",
      " |-- airport_2: string (nullable = true)\n",
      " |-- nsmiles: integer (nullable = true)\n",
      " |-- passengers: integer (nullable = true)\n",
      " |-- fare: double (nullable = true)\n",
      " |-- carrier_lg: string (nullable = true)\n",
      " |-- large_ms: double (nullable = true)\n",
      " |-- fare_lg: double (nullable = true)\n",
      " |-- carrier_low: string (nullable = true)\n",
      " |-- lf_ms: double (nullable = true)\n",
      " |-- fare_low: double (nullable = true)\n",
      " |-- Geocoded_City1: string (nullable = true)\n",
      " |-- Geocoded_City2: string (nullable = true)\n",
      " |-- tbl1apk: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# Create Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Aviation_Data_Analysis\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"false\") \\\n",
    "    .config(\"spark.driver.memory\", \"2g\") \\\n",
    "    .config(\"spark.executor.memory\", \"2g\") \\\n",
    "    .config(\"spark.sql.warehouse.dir\", \"/tmp/spark-warehouse\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Set log level to reduce warnings\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "print(\"Spark session created successfully!\")\n",
    "\n",
    "# Load data with multiLine option to handle data with line breaks\n",
    "file_path = \"/home/jovyan/data/US Airline Flight Routes and Fares 1993-2024.csv\"\n",
    "\n",
    "print(\"Loading data with multiLine option...\")\n",
    "df = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"multiLine\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .option(\"quote\", '\"') \\\n",
    "    .option(\"escape\", '\"') \\\n",
    "    .csv(file_path)\n",
    "\n",
    "print(f\"Data loaded successfully!\")\n",
    "print(f\"Dataset shape: ({df.count():,} rows, {len(df.columns)} columns)\")\n",
    "print(f\"Columns: {df.columns}\")\n",
    "\n",
    "# Show first few rows\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "df.show(5, truncate=False)\n",
    "\n",
    "# Show data types\n",
    "print(\"\\nData types:\")\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9fe426f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "MISSING VALUES ANALYSIS\n",
      "============================================================\n",
      "Missing values per column:\n",
      "+--------------+-------------+------------------+\n",
      "|        Column|Missing_Count|   Missing_Percent|\n",
      "+--------------+-------------+------------------+\n",
      "|Geocoded_City2|        39206|15.940314285133459|\n",
      "|Geocoded_City1|        39206|15.940314285133459|\n",
      "|   carrier_low|         1612|0.6554044439023399|\n",
      "|         lf_ms|         1612|0.6554044439023399|\n",
      "|      fare_low|         1612|0.6554044439023399|\n",
      "|    carrier_lg|         1540| 0.626130796283873|\n",
      "|      large_ms|         1540| 0.626130796283873|\n",
      "|       fare_lg|         1540| 0.626130796283873|\n",
      "|    passengers|            0|               0.0|\n",
      "|     airport_2|            0|               0.0|\n",
      "|          fare|            0|               0.0|\n",
      "|   airportid_1|            0|               0.0|\n",
      "|          Year|            0|               0.0|\n",
      "|           tbl|            0|               0.0|\n",
      "|citymarketid_2|            0|               0.0|\n",
      "|         city1|            0|               0.0|\n",
      "|       tbl1apk|            0|               0.0|\n",
      "|         city2|            0|               0.0|\n",
      "|   airportid_2|            0|               0.0|\n",
      "|       quarter|            0|               0.0|\n",
      "|       nsmiles|            0|               0.0|\n",
      "|citymarketid_1|            0|               0.0|\n",
      "|     airport_1|            0|               0.0|\n",
      "+--------------+-------------+------------------+\n",
      "\n",
      "\n",
      "SUMMARY:\n",
      "Total missing values: 87,868\n",
      "Total cells: 5,656,965\n",
      "Overall missing percentage: 1.55%\n",
      "\n",
      "Columns with missing values: 8\n",
      "Columns: ['carrier_lg', 'large_ms', 'fare_lg', 'carrier_low', 'lf_ms', 'fare_low', 'Geocoded_City1', 'Geocoded_City2']\n",
      "\n",
      "Checking for completely empty rows...\n",
      "Completely empty rows: 0\n",
      "Rows with all key columns NULL: 0\n"
     ]
    }
   ],
   "source": [
    "# Check missing values in the dataset\n",
    "print(\"=\" * 60)\n",
    "print(\"MISSING VALUES ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Import necessary functions\n",
    "from pyspark.sql.functions import col as spark_col, sum as spark_sum_func\n",
    "from pyspark.sql import Row\n",
    "import builtins\n",
    "\n",
    "# Count missing values for each column\n",
    "missing_data = []\n",
    "for column_name in df.columns:\n",
    "    null_count = df.filter(spark_col(column_name).isNull()).count()\n",
    "    total_count = df.count()\n",
    "    missing_percent = (null_count / total_count) * 100\n",
    "    missing_data.append((column_name, null_count, missing_percent))\n",
    "\n",
    "# Create DataFrame for better visualization\n",
    "missing_df = spark.createDataFrame(\n",
    "    [Row(Column=col_name, Missing_Count=count, Missing_Percent=percent) \n",
    "     for col_name, count, percent in missing_data]\n",
    ")\n",
    "\n",
    "print(\"Missing values per column:\")\n",
    "missing_df.orderBy(\"Missing_Count\", ascending=False).show(len(df.columns))\n",
    "\n",
    "# Summary statistics - use Python built-in sum()\n",
    "total_missing = builtins.sum([count for _, count, _ in missing_data])\n",
    "total_cells = df.count() * len(df.columns)\n",
    "overall_missing_percent = (total_missing / total_cells) * 100\n",
    "\n",
    "print(f\"\\nSUMMARY:\")\n",
    "print(f\"Total missing values: {total_missing:,}\")\n",
    "print(f\"Total cells: {total_cells:,}\")\n",
    "print(f\"Overall missing percentage: {overall_missing_percent:.2f}%\")\n",
    "\n",
    "# Columns with missing values\n",
    "columns_with_missing = [col_name for col_name, count, _ in missing_data if count > 0]\n",
    "print(f\"\\nColumns with missing values: {len(columns_with_missing)}\")\n",
    "print(f\"Columns: {columns_with_missing}\")\n",
    "\n",
    "# Check for completely empty rows - use reduce with + operator\n",
    "print(f\"\\nChecking for completely empty rows...\")\n",
    "from functools import reduce\n",
    "from operator import add\n",
    "\n",
    "# Create null count columns for all columns\n",
    "null_columns = [spark_col(column_name).isNull().cast(\"int\") for column_name in df.columns]\n",
    "# Sum all null counts using reduce\n",
    "total_nulls = reduce(add, null_columns)\n",
    "\n",
    "empty_rows = df.filter(total_nulls == len(df.columns)).count()\n",
    "print(f\"Completely empty rows: {empty_rows}\")\n",
    "\n",
    "# Check for rows with all NULL values in key columns\n",
    "key_columns = ['Year', 'quarter', 'city1', 'city2', 'fare', 'passengers']\n",
    "key_null_columns = [spark_col(c).isNull().cast(\"int\") for c in key_columns]\n",
    "key_total_nulls = reduce(add, key_null_columns)\n",
    "\n",
    "key_null_rows = df.filter(key_total_nulls == len(key_columns)).count()\n",
    "print(f\"Rows with all key columns NULL: {key_null_rows}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "483c09eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "MISSING VALUES TREATMENT\n",
      "============================================================\n",
      "Missing values in key columns:\n",
      "  nsmiles        :      0 ( 0.00%)\n",
      "  fare           :      0 ( 0.00%)\n",
      "  passengers     :      0 ( 0.00%)\n",
      "  carrier_lg     :  1,540 ( 0.63%)\n",
      "  carrier_low    :  1,612 ( 0.66%)\n",
      "  large_ms       :  1,540 ( 0.63%)\n",
      "  fare_lg        :  1,540 ( 0.63%)\n",
      "  lf_ms          :  1,612 ( 0.66%)\n",
      "  fare_low       :  1,612 ( 0.66%)\n",
      "\n",
      "MISSING VALUES TREATMENT STRATEGY:\n",
      "1. Geocoded_City1, Geocoded_City2: DROP (not needed for analysis)\n",
      "2. carrier_lg, large_ms, fare_lg: DROP rows (carrier info missing)\n",
      "3. carrier_low, lf_ms, fare_low: DROP rows (low-cost carrier info missing)\n",
      "4. nsmiles, fare, passengers: KEEP (core analysis variables)\n",
      "\n",
      "Creating clean dataset...\n",
      "Original dataset: 245,955 rows\n",
      "Clean dataset: 244,343 rows\n",
      "Rows removed: 1,612 (0.66%)\n",
      "\n",
      "Verifying clean dataset...\n",
      "  nsmiles        :      0 missing\n",
      "  fare           :      0 missing\n",
      "  passengers     :      0 missing\n",
      "  carrier_lg     :      0 missing\n",
      "  carrier_low    :      0 missing\n",
      "  large_ms       :      0 missing\n",
      "  fare_lg        :      0 missing\n",
      "  lf_ms          :      0 missing\n",
      "  fare_low       :      0 missing\n",
      "\n",
      "Sample of clean data:\n",
      "+-------+------+----------+----------+-----------+\n",
      "|nsmiles|  fare|passengers|carrier_lg|carrier_low|\n",
      "+-------+------+----------+----------+-----------+\n",
      "|    970| 81.43|       180|        G4|         G4|\n",
      "|    970|208.93|        19|        DL|         UA|\n",
      "|    580|184.56|       204|        WN|         WN|\n",
      "|    580|182.64|       264|        AA|         AA|\n",
      "|    328|177.11|       398|        WN|         AA|\n",
      "+-------+------+----------+----------+-----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "\n",
      "✅ Dataset cleaned and ready for analysis!\n"
     ]
    }
   ],
   "source": [
    "# Handle missing values for analysis\n",
    "print(\"=\" * 60)\n",
    "print(\"MISSING VALUES TREATMENT\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Import necessary functions\n",
    "from pyspark.sql.functions import col as spark_col, when, isnan, isnull\n",
    "\n",
    "# Check missing values in key columns for analysis\n",
    "key_columns = ['nsmiles', 'fare', 'passengers', 'carrier_lg', 'carrier_low', 'large_ms', 'fare_lg', 'lf_ms', 'fare_low']\n",
    "\n",
    "print(\"Missing values in key columns:\")\n",
    "for col_name in key_columns:\n",
    "    null_count = df.filter(spark_col(col_name).isNull()).count()\n",
    "    total_count = df.count()\n",
    "    missing_percent = (null_count / total_count) * 100\n",
    "    print(f\"  {col_name:15}: {null_count:6,} ({missing_percent:5.2f}%)\")\n",
    "\n",
    "# Strategy for handling missing values\n",
    "print(f\"\\nMISSING VALUES TREATMENT STRATEGY:\")\n",
    "print(f\"1. Geocoded_City1, Geocoded_City2: DROP (not needed for analysis)\")\n",
    "print(f\"2. carrier_lg, large_ms, fare_lg: DROP rows (carrier info missing)\")\n",
    "print(f\"3. carrier_low, lf_ms, fare_low: DROP rows (low-cost carrier info missing)\")\n",
    "print(f\"4. nsmiles, fare, passengers: KEEP (core analysis variables)\")\n",
    "\n",
    "# Create clean dataset for analysis\n",
    "print(f\"\\nCreating clean dataset...\")\n",
    "\n",
    "# Drop Geocoded columns and rows with missing carrier info\n",
    "df_clean = df.drop(\"Geocoded_City1\", \"Geocoded_City2\")\n",
    "\n",
    "# Drop rows where carrier information is missing\n",
    "df_clean = df_clean.filter(\n",
    "    spark_col(\"carrier_lg\").isNotNull() & \n",
    "    spark_col(\"large_ms\").isNotNull() & \n",
    "    spark_col(\"fare_lg\").isNotNull() &\n",
    "    spark_col(\"carrier_low\").isNotNull() & \n",
    "    spark_col(\"lf_ms\").isNotNull() & \n",
    "    spark_col(\"fare_low\").isNotNull()\n",
    ")\n",
    "\n",
    "# Ensure core variables are not null\n",
    "df_clean = df_clean.filter(\n",
    "    spark_col(\"nsmiles\").isNotNull() & \n",
    "    spark_col(\"fare\").isNotNull() & \n",
    "    spark_col(\"passengers\").isNotNull()\n",
    ")\n",
    "\n",
    "print(f\"Original dataset: {df.count():,} rows\")\n",
    "print(f\"Clean dataset: {df_clean.count():,} rows\")\n",
    "print(f\"Rows removed: {df.count() - df_clean.count():,} ({(df.count() - df_clean.count())/df.count()*100:.2f}%)\")\n",
    "\n",
    "# Verify no missing values in clean dataset\n",
    "print(f\"\\nVerifying clean dataset...\")\n",
    "for col_name in key_columns:\n",
    "    if col_name in df_clean.columns:\n",
    "        null_count = df_clean.filter(spark_col(col_name).isNull()).count()\n",
    "        print(f\"  {col_name:15}: {null_count:6,} missing\")\n",
    "\n",
    "# Show sample of clean data\n",
    "print(f\"\\nSample of clean data:\")\n",
    "df_clean.select(\"nsmiles\", \"fare\", \"passengers\", \"carrier_lg\", \"carrier_low\").show(5)\n",
    "\n",
    "# Update df to use clean dataset for further analysis\n",
    "df = df_clean\n",
    "print(f\"\\n✅ Dataset cleaned and ready for analysis!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "770c0a98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "CORRELATION ANALYSIS\n",
      "============================================================\n",
      "1. CORRELATION MATRIX\n",
      "------------------------------\n",
      "Correlation Matrix:\n",
      "Variables: ['nsmiles', 'fare', 'passengers', 'fare_lg', 'fare_low', 'large_ms', 'lf_ms']\n",
      "\n",
      "Correlation with Distance (nsmiles):\n",
      "  fare        :  0.5122\n",
      "  passengers  : -0.0791\n",
      "  fare_lg     :  0.4835\n",
      "  fare_low    :  0.4167\n",
      "  large_ms    : -0.4014\n",
      "  lf_ms       : -0.2530\n",
      "\n",
      "2. SPECIFIC CORRELATIONS\n",
      "------------------------------\n",
      "Distance vs Fare: 0.5122\n",
      "Distance vs Passengers: -0.0791\n",
      "Distance vs Fare_lg: 0.4835\n",
      "Distance vs Fare_low: 0.4167\n",
      "\n",
      "3. CORRELATION INTERPRETATION\n",
      "------------------------------\n",
      "Distance vs Fare: Moderate Positive\n",
      "Distance vs Passengers: Very Weak Negative\n",
      "Distance vs Fare_lg: Weak Positive\n",
      "Distance vs Fare_low: Weak Positive\n",
      "\n",
      "4. STATISTICAL SIGNIFICANCE\n",
      "------------------------------\n",
      "Sample size: 244,343 observations\n",
      "With this large sample size, even small correlations are statistically significant\n",
      "Focus on practical significance (effect size) rather than statistical significance\n"
     ]
    }
   ],
   "source": [
    "# CORRELATION ANALYSIS: Distance vs Fare & Passengers\n",
    "print(\"=\" * 60)\n",
    "print(\"CORRELATION ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Import necessary functions\n",
    "from pyspark.sql.functions import col as spark_col, corr, desc\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.stat import Correlation\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml import Pipeline\n",
    "import numpy as np\n",
    "\n",
    "# 1. CORRELATION MATRIX\n",
    "print(\"1. CORRELATION MATRIX\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# Select key variables for correlation\n",
    "key_vars = ['nsmiles', 'fare', 'passengers', 'fare_lg', 'fare_low', 'large_ms', 'lf_ms']\n",
    "correlation_data = df.select(*key_vars)\n",
    "\n",
    "# Calculate correlation matrix\n",
    "assembler = VectorAssembler(inputCols=key_vars, outputCol=\"features\")\n",
    "correlation_df = assembler.transform(correlation_data).select(\"features\")\n",
    "\n",
    "# Calculate correlation matrix\n",
    "correlation_matrix = Correlation.corr(correlation_df, \"features\").collect()[0][0]\n",
    "correlation_array = correlation_matrix.toArray()\n",
    "\n",
    "# Display correlation matrix\n",
    "print(\"Correlation Matrix:\")\n",
    "print(\"Variables:\", key_vars)\n",
    "print(\"\\nCorrelation with Distance (nsmiles):\")\n",
    "for i, var in enumerate(key_vars):\n",
    "    if var != 'nsmiles':\n",
    "        corr_value = correlation_array[0, i]  # nsmiles is first column (index 0)\n",
    "        print(f\"  {var:12}: {corr_value:7.4f}\")\n",
    "\n",
    "# 2. SPECIFIC CORRELATIONS\n",
    "print(f\"\\n2. SPECIFIC CORRELATIONS\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# Distance vs Fare\n",
    "distance_fare_corr = df.stat.corr(\"nsmiles\", \"fare\")\n",
    "print(f\"Distance vs Fare: {distance_fare_corr:.4f}\")\n",
    "\n",
    "# Distance vs Passengers  \n",
    "distance_passengers_corr = df.stat.corr(\"nsmiles\", \"passengers\")\n",
    "print(f\"Distance vs Passengers: {distance_passengers_corr:.4f}\")\n",
    "\n",
    "# Distance vs Fare_lg (large carrier fare)\n",
    "distance_fare_lg_corr = df.stat.corr(\"nsmiles\", \"fare_lg\")\n",
    "print(f\"Distance vs Fare_lg: {distance_fare_lg_corr:.4f}\")\n",
    "\n",
    "# Distance vs Fare_low (low-cost carrier fare)\n",
    "distance_fare_low_corr = df.stat.corr(\"nsmiles\", \"fare_low\")\n",
    "print(f\"Distance vs Fare_low: {distance_fare_low_corr:.4f}\")\n",
    "\n",
    "# 3. CORRELATION INTERPRETATION\n",
    "print(f\"\\n3. CORRELATION INTERPRETATION\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "def interpret_correlation(corr_value):\n",
    "    import builtins\n",
    "    abs_corr = builtins.abs(corr_value)\n",
    "    if abs_corr >= 0.7:\n",
    "        strength = \"Strong\"\n",
    "    elif abs_corr >= 0.5:\n",
    "        strength = \"Moderate\"\n",
    "    elif abs_corr >= 0.3:\n",
    "        strength = \"Weak\"\n",
    "    else:\n",
    "        strength = \"Very Weak\"\n",
    "    \n",
    "    direction = \"Positive\" if corr_value > 0 else \"Negative\"\n",
    "    return f\"{strength} {direction}\"\n",
    "\n",
    "print(f\"Distance vs Fare: {interpret_correlation(distance_fare_corr)}\")\n",
    "print(f\"Distance vs Passengers: {interpret_correlation(distance_passengers_corr)}\")\n",
    "print(f\"Distance vs Fare_lg: {interpret_correlation(distance_fare_lg_corr)}\")\n",
    "print(f\"Distance vs Fare_low: {interpret_correlation(distance_fare_low_corr)}\")\n",
    "\n",
    "# 4. STATISTICAL SIGNIFICANCE (Sample size is large, so correlations are likely significant)\n",
    "print(f\"\\n4. STATISTICAL SIGNIFICANCE\")\n",
    "print(\"-\" * 30)\n",
    "print(f\"Sample size: {df.count():,} observations\")\n",
    "print(\"With this large sample size, even small correlations are statistically significant\")\n",
    "print(\"Focus on practical significance (effect size) rather than statistical significance\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0040207a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "LINEAR REGRESSION ANALYSIS\n",
      "============================================================\n",
      "1. PREPARING DATA FOR REGRESSION\n",
      "------------------------------\n",
      "\n",
      "2. LINEAR REGRESSION MODELS\n",
      "------------------------------\n",
      "Model 1: Distance -> Fare\n",
      "Model 2: Distance -> Passengers\n",
      "\n",
      "3. HYPERPARAMETER TUNING\n",
      "------------------------------\n",
      "\n",
      "4. CROSS-VALIDATION\n",
      "------------------------------\n",
      "\n",
      "5. TRAINING MODELS\n",
      "------------------------------\n",
      "Training data: 195,235 rows\n",
      "Test data: 49,108 rows\n",
      "Training Distance -> Fare model...\n",
      "Training Distance -> Passengers model...\n",
      "\n",
      "6. MODEL EVALUATION\n",
      "------------------------------\n",
      "Distance -> Fare Model:\n",
      "  RMSE: 68.0079\n",
      "  R²: 0.2634\n",
      "  MAE: 50.8009\n",
      "\n",
      "Distance -> Passengers Model:\n",
      "  RMSE: 511.7212\n",
      "  R²: 0.0058\n",
      "  MAE: 314.5641\n",
      "\n",
      "7. BEST PARAMETERS\n",
      "------------------------------\n",
      "Best parameters for Distance -> Fare:\n",
      "  Regularization: 0.01\n",
      "  Elastic Net: 0.0\n",
      "  Max Iterations: 50\n",
      "  Tolerance: 1e-06\n",
      "\n",
      "Best parameters for Distance -> Passengers:\n",
      "  Regularization: 0.1\n",
      "  Elastic Net: 0.0\n",
      "  Max Iterations: 50\n",
      "  Tolerance: 1e-06\n",
      "\n",
      "8. MODEL COEFFICIENTS\n",
      "------------------------------\n",
      "Distance -> Fare equation:\n",
      "  Fare = 40.7462 * Distance + 218.5078\n",
      "\n",
      "Distance -> Passengers equation:\n",
      "  Passengers = -40.8250 * Distance + 301.2593\n",
      "\n",
      "9. INTERPRETATION\n",
      "------------------------------\n",
      "Distance -> Fare:\n",
      "  Coefficient: 40.7462 (per mile increase in fare)\n",
      "  R²: 0.2634 (26.3% of fare variance explained by distance)\n",
      "\n",
      "Distance -> Passengers:\n",
      "  Coefficient: -40.8250 (per mile increase in passengers)\n",
      "  R²: 0.0058 (0.6% of passenger variance explained by distance)\n",
      "\n",
      "✅ Linear Regression analysis completed!\n"
     ]
    }
   ],
   "source": [
    "# LINEAR REGRESSION with Hyperparameter Tuning\n",
    "print(\"=\" * 60)\n",
    "print(\"LINEAR REGRESSION ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Import necessary functions\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.functions import col as spark_col, rand\n",
    "\n",
    "# 1. PREPARE DATA FOR REGRESSION\n",
    "print(\"1. PREPARING DATA FOR REGRESSION\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# Create features vector\n",
    "feature_cols = ['nsmiles']\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
    "\n",
    "# Standardize features for better convergence\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaled_features\", withStd=True, withMean=True)\n",
    "\n",
    "# 2. LINEAR REGRESSION MODELS\n",
    "print(\"\\n2. LINEAR REGRESSION MODELS\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# Model 1: Distance -> Fare\n",
    "print(\"Model 1: Distance -> Fare\")\n",
    "lr_fare = LinearRegression(featuresCol=\"scaled_features\", labelCol=\"fare\", predictionCol=\"fare_pred\")\n",
    "\n",
    "# Model 2: Distance -> Passengers  \n",
    "print(\"Model 2: Distance -> Passengers\")\n",
    "lr_passengers = LinearRegression(featuresCol=\"scaled_features\", labelCol=\"passengers\", predictionCol=\"passengers_pred\")\n",
    "\n",
    "# 3. HYPERPARAMETER TUNING\n",
    "print(\"\\n3. HYPERPARAMETER TUNING\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# Create parameter grids for tuning\n",
    "param_grid_fare = ParamGridBuilder() \\\n",
    "    .addGrid(lr_fare.regParam, [0.0, 0.01, 0.1, 0.5, 1.0]) \\\n",
    "    .addGrid(lr_fare.elasticNetParam, [0.0, 0.25, 0.5, 0.75, 1.0]) \\\n",
    "    .addGrid(lr_fare.maxIter, [50, 100, 200]) \\\n",
    "    .addGrid(lr_fare.tol, [1e-6, 1e-4, 1e-2]) \\\n",
    "    .build()\n",
    "\n",
    "param_grid_passengers = ParamGridBuilder() \\\n",
    "    .addGrid(lr_passengers.regParam, [0.0, 0.01, 0.1, 0.5, 1.0]) \\\n",
    "    .addGrid(lr_passengers.elasticNetParam, [0.0, 0.25, 0.5, 0.75, 1.0]) \\\n",
    "    .addGrid(lr_passengers.maxIter, [50, 100, 200]) \\\n",
    "    .addGrid(lr_passengers.tol, [1e-6, 1e-4, 1e-2]) \\\n",
    "    .build()\n",
    "\n",
    "# Create pipelines\n",
    "pipeline_fare = Pipeline(stages=[assembler, scaler, lr_fare])\n",
    "pipeline_passengers = Pipeline(stages=[assembler, scaler, lr_passengers])\n",
    "\n",
    "# Create evaluators\n",
    "evaluator_fare = RegressionEvaluator(labelCol=\"fare\", predictionCol=\"fare_pred\", metricName=\"rmse\")\n",
    "evaluator_passengers = RegressionEvaluator(labelCol=\"passengers\", predictionCol=\"passengers_pred\", metricName=\"rmse\")\n",
    "\n",
    "# 4. CROSS-VALIDATION\n",
    "print(\"\\n4. CROSS-VALIDATION\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# Create cross-validators\n",
    "cv_fare = CrossValidator(\n",
    "    estimator=pipeline_fare,\n",
    "    estimatorParamMaps=param_grid_fare,\n",
    "    evaluator=evaluator_fare,\n",
    "    numFolds=3,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "cv_passengers = CrossValidator(\n",
    "    estimator=pipeline_passengers,\n",
    "    estimatorParamMaps=param_grid_passengers,\n",
    "    evaluator=evaluator_passengers,\n",
    "    numFolds=3,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# 5. TRAIN MODELS\n",
    "print(\"\\n5. TRAINING MODELS\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# Split data for training and testing\n",
    "train_data, test_data = df.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "print(f\"Training data: {train_data.count():,} rows\")\n",
    "print(f\"Test data: {test_data.count():,} rows\")\n",
    "\n",
    "# Train models with cross-validation\n",
    "print(\"Training Distance -> Fare model...\")\n",
    "cv_model_fare = cv_fare.fit(train_data)\n",
    "\n",
    "print(\"Training Distance -> Passengers model...\")\n",
    "cv_model_passengers = cv_passengers.fit(train_data)\n",
    "\n",
    "# 6. EVALUATE MODELS\n",
    "print(\"\\n6. MODEL EVALUATION\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# Make predictions\n",
    "predictions_fare = cv_model_fare.transform(test_data)\n",
    "predictions_passengers = cv_model_passengers.transform(test_data)\n",
    "\n",
    "# Evaluate models\n",
    "rmse_fare = evaluator_fare.evaluate(predictions_fare)\n",
    "r2_fare = RegressionEvaluator(labelCol=\"fare\", predictionCol=\"fare_pred\", metricName=\"r2\").evaluate(predictions_fare)\n",
    "mae_fare = RegressionEvaluator(labelCol=\"fare\", predictionCol=\"fare_pred\", metricName=\"mae\").evaluate(predictions_fare)\n",
    "\n",
    "rmse_passengers = evaluator_passengers.evaluate(predictions_passengers)\n",
    "r2_passengers = RegressionEvaluator(labelCol=\"passengers\", predictionCol=\"passengers_pred\", metricName=\"r2\").evaluate(predictions_passengers)\n",
    "mae_passengers = RegressionEvaluator(labelCol=\"passengers\", predictionCol=\"passengers_pred\", metricName=\"mae\").evaluate(predictions_passengers)\n",
    "\n",
    "# Display results\n",
    "print(\"Distance -> Fare Model:\")\n",
    "print(f\"  RMSE: {rmse_fare:.4f}\")\n",
    "print(f\"  R²: {r2_fare:.4f}\")\n",
    "print(f\"  MAE: {mae_fare:.4f}\")\n",
    "\n",
    "print(\"\\nDistance -> Passengers Model:\")\n",
    "print(f\"  RMSE: {rmse_passengers:.4f}\")\n",
    "print(f\"  R²: {r2_passengers:.4f}\")\n",
    "print(f\"  MAE: {mae_passengers:.4f}\")\n",
    "\n",
    "# 7. BEST PARAMETERS\n",
    "print(\"\\n7. BEST PARAMETERS\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "best_model_fare = cv_model_fare.bestModel\n",
    "best_model_passengers = cv_model_passengers.bestModel\n",
    "\n",
    "print(\"Best parameters for Distance -> Fare:\")\n",
    "print(f\"  Regularization: {best_model_fare.stages[-1].getRegParam()}\")\n",
    "print(f\"  Elastic Net: {best_model_fare.stages[-1].getElasticNetParam()}\")\n",
    "print(f\"  Max Iterations: {best_model_fare.stages[-1].getMaxIter()}\")\n",
    "print(f\"  Tolerance: {best_model_fare.stages[-1].getTol()}\")\n",
    "\n",
    "print(\"\\nBest parameters for Distance -> Passengers:\")\n",
    "print(f\"  Regularization: {best_model_passengers.stages[-1].getRegParam()}\")\n",
    "print(f\"  Elastic Net: {best_model_passengers.stages[-1].getElasticNetParam()}\")\n",
    "print(f\"  Max Iterations: {best_model_passengers.stages[-1].getMaxIter()}\")\n",
    "print(f\"  Tolerance: {best_model_passengers.stages[-1].getTol()}\")\n",
    "\n",
    "# 8. MODEL COEFFICIENTS\n",
    "print(\"\\n8. MODEL COEFFICIENTS\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# Get coefficients\n",
    "coef_fare = best_model_fare.stages[-1].coefficients[0]\n",
    "intercept_fare = best_model_fare.stages[-1].intercept\n",
    "\n",
    "coef_passengers = best_model_passengers.stages[-1].coefficients[0]\n",
    "intercept_passengers = best_model_passengers.stages[-1].intercept\n",
    "\n",
    "print(f\"Distance -> Fare equation:\")\n",
    "print(f\"  Fare = {coef_fare:.4f} * Distance + {intercept_fare:.4f}\")\n",
    "\n",
    "print(f\"\\nDistance -> Passengers equation:\")\n",
    "print(f\"  Passengers = {coef_passengers:.4f} * Distance + {intercept_passengers:.4f}\")\n",
    "\n",
    "# 9. INTERPRETATION\n",
    "print(\"\\n9. INTERPRETATION\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "print(f\"Distance -> Fare:\")\n",
    "print(f\"  Coefficient: {coef_fare:.4f} (per mile increase in fare)\")\n",
    "print(f\"  R²: {r2_fare:.4f} ({r2_fare*100:.1f}% of fare variance explained by distance)\")\n",
    "\n",
    "print(f\"\\nDistance -> Passengers:\")\n",
    "print(f\"  Coefficient: {coef_passengers:.4f} (per mile increase in passengers)\")\n",
    "print(f\"  R²: {r2_passengers:.4f} ({r2_passengers*100:.1f}% of passenger variance explained by distance)\")\n",
    "\n",
    "print(f\"\\n✅ Linear Regression analysis completed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c5fafa80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "POLYNOMIAL REGRESSION ANALYSIS\n",
      "============================================================\n",
      "1. PREPARING DATA FOR POLYNOMIAL REGRESSION\n",
      "------------------------------\n",
      "\n",
      "2. POLYNOMIAL REGRESSION MODELS\n",
      "------------------------------\n",
      "Model 1: Distance -> Fare (Polynomial)\n",
      "Model 2: Distance -> Passengers (Polynomial)\n",
      "\n",
      "3. HYPERPARAMETER TUNING\n",
      "------------------------------\n",
      "\n",
      "4. POLYNOMIAL DEGREE TUNING\n",
      "------------------------------\n",
      "Testing different polynomial degrees...\n",
      "\n",
      "Testing degree 2...\n",
      "  Training Distance -> Fare model (degree 2)...\n",
      "  Training Distance -> Passengers model (degree 2)...\n",
      "  Degree 2 - Fare RMSE: 67.9858\n",
      "  Degree 2 - Passengers RMSE: 511.7310\n",
      "\n",
      "Testing degree 3...\n",
      "  Training Distance -> Fare model (degree 3)...\n",
      "  Training Distance -> Passengers model (degree 3)...\n",
      "  Degree 3 - Fare RMSE: 67.9396\n",
      "  Degree 3 - Passengers RMSE: 511.3937\n",
      "\n",
      "Testing degree 4...\n",
      "  Training Distance -> Fare model (degree 4)...\n",
      "  Training Distance -> Passengers model (degree 4)...\n",
      "  Degree 4 - Fare RMSE: 67.9325\n",
      "  Degree 4 - Passengers RMSE: 511.2501\n",
      "\n",
      "Testing degree 5...\n",
      "  Training Distance -> Fare model (degree 5)...\n",
      "  Training Distance -> Passengers model (degree 5)...\n",
      "  Degree 5 - Fare RMSE: 67.9216\n",
      "  Degree 5 - Passengers RMSE: 511.2616\n",
      "\n",
      "5. FINAL POLYNOMIAL MODELS\n",
      "------------------------------\n",
      "Best polynomial degree for Distance -> Fare: 5\n",
      "Best polynomial degree for Distance -> Passengers: 4\n",
      "\n",
      "6. EVALUATING BEST POLYNOMIAL MODELS\n",
      "------------------------------\n",
      "Distance -> Fare Model (Polynomial):\n",
      "  RMSE: 67.9216\n",
      "  R²: 0.2653\n",
      "  MAE: 50.6912\n",
      "\n",
      "Distance -> Passengers Model (Polynomial):\n",
      "  RMSE: 511.2501\n",
      "  R²: 0.0077\n",
      "  MAE: 314.3652\n",
      "\n",
      "7. MODEL COMPARISON\n",
      "------------------------------\n",
      "Linear vs Polynomial Regression:\n",
      "Distance -> Fare:\n",
      "  Linear RMSE: 68.0079\n",
      "  Polynomial RMSE: 67.9216\n",
      "  Improvement: 0.13%\n",
      "\n",
      "Distance -> Passengers:\n",
      "  Linear RMSE: 511.7212\n",
      "  Polynomial RMSE: 511.2501\n",
      "  Improvement: 0.09%\n",
      "\n",
      "8. INTERPRETATION\n",
      "------------------------------\n",
      "Polynomial Regression Results:\n",
      "  Best degree for Fare: 5\n",
      "  Best degree for Passengers: 4\n",
      "\n",
      "Distance -> Fare (Polynomial):\n",
      "  R²: 0.2653 (26.5% of fare variance explained)\n",
      "  RMSE: 67.9216 (average prediction error)\n",
      "\n",
      "Distance -> Passengers (Polynomial):\n",
      "  R²: 0.0077 (0.8% of passenger variance explained)\n",
      "  RMSE: 511.2501 (average prediction error)\n",
      "\n",
      "✅ Polynomial Regression analysis completed!\n"
     ]
    }
   ],
   "source": [
    "# POLYNOMIAL REGRESSION with Hyperparameter Tuning\n",
    "print(\"=\" * 60)\n",
    "print(\"POLYNOMIAL REGRESSION ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Import necessary functions\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler, PolynomialExpansion\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.functions import col as spark_col, rand\n",
    "\n",
    "# 1. PREPARE DATA FOR POLYNOMIAL REGRESSION\n",
    "print(\"1. PREPARING DATA FOR POLYNOMIAL REGRESSION\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# Create features vector\n",
    "feature_cols = ['nsmiles']\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaled_features\", withStd=True, withMean=True)\n",
    "\n",
    "# 2. POLYNOMIAL REGRESSION MODELS\n",
    "print(\"\\n2. POLYNOMIAL REGRESSION MODELS\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# Model 1: Distance -> Fare (Polynomial)\n",
    "print(\"Model 1: Distance -> Fare (Polynomial)\")\n",
    "lr_fare_poly = LinearRegression(featuresCol=\"poly_features\", labelCol=\"fare\", predictionCol=\"fare_pred_poly\")\n",
    "\n",
    "# Model 2: Distance -> Passengers (Polynomial)\n",
    "print(\"Model 2: Distance -> Passengers (Polynomial)\")\n",
    "lr_passengers_poly = LinearRegression(featuresCol=\"poly_features\", labelCol=\"passengers\", predictionCol=\"passengers_pred_poly\")\n",
    "\n",
    "# 3. HYPERPARAMETER TUNING FOR POLYNOMIAL MODELS\n",
    "print(\"\\n3. HYPERPARAMETER TUNING\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# Create parameter grids for polynomial models\n",
    "param_grid_fare_poly = ParamGridBuilder() \\\n",
    "    .addGrid(lr_fare_poly.regParam, [0.0, 0.01, 0.1, 0.5, 1.0]) \\\n",
    "    .addGrid(lr_fare_poly.elasticNetParam, [0.0, 0.25, 0.5, 0.75, 1.0]) \\\n",
    "    .addGrid(lr_fare_poly.maxIter, [50, 100, 200]) \\\n",
    "    .addGrid(lr_fare_poly.tol, [1e-6, 1e-4, 1e-2]) \\\n",
    "    .build()\n",
    "\n",
    "param_grid_passengers_poly = ParamGridBuilder() \\\n",
    "    .addGrid(lr_passengers_poly.regParam, [0.0, 0.01, 0.1, 0.5, 1.0]) \\\n",
    "    .addGrid(lr_passengers_poly.elasticNetParam, [0.0, 0.25, 0.5, 0.75, 1.0]) \\\n",
    "    .addGrid(lr_passengers_poly.maxIter, [50, 100, 200]) \\\n",
    "    .addGrid(lr_passengers_poly.tol, [1e-6, 1e-4, 1e-2]) \\\n",
    "    .build()\n",
    "\n",
    "# 4. POLYNOMIAL DEGREE TUNING\n",
    "print(\"\\n4. POLYNOMIAL DEGREE TUNING\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# Test different polynomial degrees\n",
    "degrees = [2, 3, 4, 5]\n",
    "best_degree_fare = 2\n",
    "best_degree_passengers = 2\n",
    "best_score_fare = float('inf')\n",
    "best_score_passengers = float('inf')\n",
    "\n",
    "print(\"Testing different polynomial degrees...\")\n",
    "\n",
    "for degree in degrees:\n",
    "    print(f\"\\nTesting degree {degree}...\")\n",
    "    \n",
    "    # Create polynomial expansion\n",
    "    poly_expansion = PolynomialExpansion(degree=degree, inputCol=\"scaled_features\", outputCol=\"poly_features\")\n",
    "    \n",
    "    # Create pipelines\n",
    "    pipeline_fare_poly = Pipeline(stages=[assembler, scaler, poly_expansion, lr_fare_poly])\n",
    "    pipeline_passengers_poly = Pipeline(stages=[assembler, scaler, poly_expansion, lr_passengers_poly])\n",
    "    \n",
    "    # Create cross-validators\n",
    "    cv_fare_poly = CrossValidator(\n",
    "        estimator=pipeline_fare_poly,\n",
    "        estimatorParamMaps=param_grid_fare_poly,\n",
    "        evaluator=RegressionEvaluator(labelCol=\"fare\", predictionCol=\"fare_pred_poly\", metricName=\"rmse\"),\n",
    "        numFolds=3,\n",
    "        seed=42\n",
    "    )\n",
    "    \n",
    "    cv_passengers_poly = CrossValidator(\n",
    "        estimator=pipeline_passengers_poly,\n",
    "        estimatorParamMaps=param_grid_passengers_poly,\n",
    "        evaluator=RegressionEvaluator(labelCol=\"passengers\", predictionCol=\"passengers_pred_poly\", metricName=\"rmse\"),\n",
    "        numFolds=3,\n",
    "        seed=42\n",
    "    )\n",
    "    \n",
    "    # Train models\n",
    "    print(f\"  Training Distance -> Fare model (degree {degree})...\")\n",
    "    cv_model_fare_poly = cv_fare_poly.fit(train_data)\n",
    "    \n",
    "    print(f\"  Training Distance -> Passengers model (degree {degree})...\")\n",
    "    cv_model_passengers_poly = cv_passengers_poly.fit(train_data)\n",
    "    \n",
    "    # Evaluate models\n",
    "    predictions_fare_poly = cv_model_fare_poly.transform(test_data)\n",
    "    predictions_passengers_poly = cv_model_passengers_poly.transform(test_data)\n",
    "    \n",
    "    rmse_fare_poly = RegressionEvaluator(labelCol=\"fare\", predictionCol=\"fare_pred_poly\", metricName=\"rmse\").evaluate(predictions_fare_poly)\n",
    "    rmse_passengers_poly = RegressionEvaluator(labelCol=\"passengers\", predictionCol=\"passengers_pred_poly\", metricName=\"rmse\").evaluate(predictions_passengers_poly)\n",
    "    \n",
    "    print(f\"  Degree {degree} - Fare RMSE: {rmse_fare_poly:.4f}\")\n",
    "    print(f\"  Degree {degree} - Passengers RMSE: {rmse_passengers_poly:.4f}\")\n",
    "    \n",
    "    # Update best models\n",
    "    if rmse_fare_poly < best_score_fare:\n",
    "        best_score_fare = rmse_fare_poly\n",
    "        best_degree_fare = degree\n",
    "        best_model_fare_poly = cv_model_fare_poly\n",
    "    \n",
    "    if rmse_passengers_poly < best_score_passengers:\n",
    "        best_score_passengers = rmse_passengers_poly\n",
    "        best_degree_passengers = degree\n",
    "        best_model_passengers_poly = cv_model_passengers_poly\n",
    "\n",
    "# 5. FINAL POLYNOMIAL MODELS\n",
    "print(\"\\n5. FINAL POLYNOMIAL MODELS\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "print(f\"Best polynomial degree for Distance -> Fare: {best_degree_fare}\")\n",
    "print(f\"Best polynomial degree for Distance -> Passengers: {best_degree_passengers}\")\n",
    "\n",
    "# 6. EVALUATE BEST POLYNOMIAL MODELS\n",
    "print(\"\\n6. EVALUATING BEST POLYNOMIAL MODELS\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# Make predictions with best models\n",
    "predictions_fare_poly = best_model_fare_poly.transform(test_data)\n",
    "predictions_passengers_poly = best_model_passengers_poly.transform(test_data)\n",
    "\n",
    "# Evaluate models\n",
    "rmse_fare_poly = RegressionEvaluator(labelCol=\"fare\", predictionCol=\"fare_pred_poly\", metricName=\"rmse\").evaluate(predictions_fare_poly)\n",
    "r2_fare_poly = RegressionEvaluator(labelCol=\"fare\", predictionCol=\"fare_pred_poly\", metricName=\"r2\").evaluate(predictions_fare_poly)\n",
    "mae_fare_poly = RegressionEvaluator(labelCol=\"fare\", predictionCol=\"fare_pred_poly\", metricName=\"mae\").evaluate(predictions_fare_poly)\n",
    "\n",
    "rmse_passengers_poly = RegressionEvaluator(labelCol=\"passengers\", predictionCol=\"passengers_pred_poly\", metricName=\"rmse\").evaluate(predictions_passengers_poly)\n",
    "r2_passengers_poly = RegressionEvaluator(labelCol=\"passengers\", predictionCol=\"passengers_pred_poly\", metricName=\"r2\").evaluate(predictions_passengers_poly)\n",
    "mae_passengers_poly = RegressionEvaluator(labelCol=\"passengers\", predictionCol=\"passengers_pred_poly\", metricName=\"mae\").evaluate(predictions_passengers_poly)\n",
    "\n",
    "# Display results\n",
    "print(\"Distance -> Fare Model (Polynomial):\")\n",
    "print(f\"  RMSE: {rmse_fare_poly:.4f}\")\n",
    "print(f\"  R²: {r2_fare_poly:.4f}\")\n",
    "print(f\"  MAE: {mae_fare_poly:.4f}\")\n",
    "\n",
    "print(\"\\nDistance -> Passengers Model (Polynomial):\")\n",
    "print(f\"  RMSE: {rmse_passengers_poly:.4f}\")\n",
    "print(f\"  R²: {r2_passengers_poly:.4f}\")\n",
    "print(f\"  MAE: {mae_passengers_poly:.4f}\")\n",
    "\n",
    "# 7. MODEL COMPARISON\n",
    "print(\"\\n7. MODEL COMPARISON\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "print(\"Linear vs Polynomial Regression:\")\n",
    "print(f\"Distance -> Fare:\")\n",
    "print(f\"  Linear RMSE: {rmse_fare:.4f}\")\n",
    "print(f\"  Polynomial RMSE: {rmse_fare_poly:.4f}\")\n",
    "print(f\"  Improvement: {((rmse_fare - rmse_fare_poly) / rmse_fare * 100):.2f}%\")\n",
    "\n",
    "print(f\"\\nDistance -> Passengers:\")\n",
    "print(f\"  Linear RMSE: {rmse_passengers:.4f}\")\n",
    "print(f\"  Polynomial RMSE: {rmse_passengers_poly:.4f}\")\n",
    "print(f\"  Improvement: {((rmse_passengers - rmse_passengers_poly) / rmse_passengers * 100):.2f}%\")\n",
    "\n",
    "# 8. INTERPRETATION\n",
    "print(\"\\n8. INTERPRETATION\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "print(f\"Polynomial Regression Results:\")\n",
    "print(f\"  Best degree for Fare: {best_degree_fare}\")\n",
    "print(f\"  Best degree for Passengers: {best_degree_passengers}\")\n",
    "\n",
    "print(f\"\\nDistance -> Fare (Polynomial):\")\n",
    "print(f\"  R²: {r2_fare_poly:.4f} ({r2_fare_poly*100:.1f}% of fare variance explained)\")\n",
    "print(f\"  RMSE: {rmse_fare_poly:.4f} (average prediction error)\")\n",
    "\n",
    "print(f\"\\nDistance -> Passengers (Polynomial):\")\n",
    "print(f\"  R²: {r2_passengers_poly:.4f} ({r2_passengers_poly*100:.1f}% of passenger variance explained)\")\n",
    "print(f\"  RMSE: {rmse_passengers_poly:.4f} (average prediction error)\")\n",
    "\n",
    "print(f\"\\n✅ Polynomial Regression analysis completed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "18f158d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "FINAL SUMMARY AND CONCLUSIONS\n",
      "============================================================\n",
      "1. DATASET OVERVIEW\n",
      "------------------------------\n",
      "Total observations: 244,343\n",
      "Time period: 1993-2024\n",
      "Variables analyzed: Distance (nsmiles), Fare, Passengers\n",
      "Data quality: Clean dataset with no missing values in key variables\n",
      "\n",
      "2. KEY FINDINGS\n",
      "------------------------------\n",
      "CORRELATION ANALYSIS:\n",
      "  Distance vs Fare: 0.5122 (Moderate Positive)\n",
      "  Distance vs Passengers: -0.0791 (Very Weak Negative)\n",
      "  Distance vs Fare_lg: 0.4835 (Moderate Positive)\n",
      "  Distance vs Fare_low: 0.4167 (Moderate Positive)\n",
      "\n",
      "LINEAR REGRESSION RESULTS:\n",
      "  Distance -> Fare: R² = 0.2634 (26.3% variance explained)\n",
      "  Distance -> Passengers: R² = 0.0058 (0.6% variance explained)\n",
      "\n",
      "POLYNOMIAL REGRESSION RESULTS:\n",
      "  Distance -> Fare: R² = 0.2653 (26.5% variance explained)\n",
      "  Distance -> Passengers: R² = 0.0077 (0.8% variance explained)\n",
      "\n",
      "3. BUSINESS INSIGHTS\n",
      "------------------------------\n",
      "DISTANCE IMPACT ON FARE:\n",
      "  • Strong positive correlation (r = 0.512)\n",
      "  • Linear model explains 26.3% of fare variance\n",
      "  • Polynomial model explains 26.5% of fare variance\n",
      "  • Business implication: Distance is a key pricing factor\n",
      "\n",
      "DISTANCE IMPACT ON PASSENGERS:\n",
      "  • Very weak negative correlation (r = -0.079)\n",
      "  • Linear model explains 0.6% of passenger variance\n",
      "  • Polynomial model explains 0.8% of passenger variance\n",
      "  • Business implication: Distance has minimal impact on passenger demand\n",
      "\n",
      "4. MODEL COMPARISON\n",
      "------------------------------\n",
      "FARE PREDICTION:\n",
      "  Linear RMSE: 68.0079\n",
      "  Polynomial RMSE: 67.9216\n",
      "  Polynomial improvement: 0.13%\n",
      "\n",
      "PASSENGER PREDICTION:\n",
      "  Linear RMSE: 511.7212\n",
      "  Polynomial RMSE: 511.2501\n",
      "  Polynomial improvement: 0.09%\n",
      "\n",
      "5. RECOMMENDATIONS\n",
      "------------------------------\n",
      "FOR AIRLINE PRICING:\n",
      "  • Distance is a strong predictor of fare (R² = 0.265)\n",
      "  • Use distance-based pricing models for fare optimization\n",
      "  • Consider polynomial relationships for complex pricing strategies\n",
      "\n",
      "FOR ROUTE PLANNING:\n",
      "  • Distance has minimal impact on passenger demand (R² = 0.008)\n",
      "  • Focus on other factors (frequency, timing, competition) for passenger growth\n",
      "  • Distance-based passenger prediction is not reliable\n",
      "\n",
      "FOR MODEL SELECTION:\n",
      "  • Use polynomial regression for fare prediction (better R²)\n",
      "  • Use linear regression for passenger prediction (simpler, similar performance)\n",
      "  • Consider additional features beyond distance for better predictions\n",
      "\n",
      "6. LIMITATIONS\n",
      "------------------------------\n",
      "  • Single variable analysis (distance only)\n",
      "  • Missing other important factors (competition, seasonality, etc.)\n",
      "  • Low R² for passenger prediction suggests other factors are more important\n",
      "  • Correlation does not imply causation\n",
      "\n",
      "7. NEXT STEPS\n",
      "------------------------------\n",
      "  • Add more features (competition, seasonality, route characteristics)\n",
      "  • Implement ensemble methods for better predictions\n",
      "  • Analyze temporal trends and seasonality\n",
      "  • Consider market-specific factors\n",
      "\n",
      "✅ Analysis completed successfully!\n",
      "📊 Dataset: 244,343 observations\n",
      "🔍 Methods: Correlation, Linear Regression, Polynomial Regression\n",
      "🎯 Focus: Distance impact on Fare & Passengers\n",
      "📈 Results: Distance strongly affects fare, weakly affects passengers\n"
     ]
    }
   ],
   "source": [
    "# FINAL SUMMARY AND CONCLUSIONS\n",
    "print(\"=\" * 60)\n",
    "print(\"FINAL SUMMARY AND CONCLUSIONS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 1. DATASET OVERVIEW\n",
    "print(\"1. DATASET OVERVIEW\")\n",
    "print(\"-\" * 30)\n",
    "print(f\"Total observations: {df.count():,}\")\n",
    "print(f\"Time period: 1993-2024\")\n",
    "print(f\"Variables analyzed: Distance (nsmiles), Fare, Passengers\")\n",
    "print(f\"Data quality: Clean dataset with no missing values in key variables\")\n",
    "\n",
    "# 2. KEY FINDINGS\n",
    "print(\"\\n2. KEY FINDINGS\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "print(\"CORRELATION ANALYSIS:\")\n",
    "print(f\"  Distance vs Fare: {distance_fare_corr:.4f} (Moderate Positive)\")\n",
    "print(f\"  Distance vs Passengers: {distance_passengers_corr:.4f} (Very Weak Negative)\")\n",
    "print(f\"  Distance vs Fare_lg: {distance_fare_lg_corr:.4f} (Moderate Positive)\")\n",
    "print(f\"  Distance vs Fare_low: {distance_fare_low_corr:.4f} (Moderate Positive)\")\n",
    "\n",
    "print(\"\\nLINEAR REGRESSION RESULTS:\")\n",
    "print(f\"  Distance -> Fare: R² = {r2_fare:.4f} ({r2_fare*100:.1f}% variance explained)\")\n",
    "print(f\"  Distance -> Passengers: R² = {r2_passengers:.4f} ({r2_passengers*100:.1f}% variance explained)\")\n",
    "\n",
    "print(\"\\nPOLYNOMIAL REGRESSION RESULTS:\")\n",
    "print(f\"  Distance -> Fare: R² = {r2_fare_poly:.4f} ({r2_fare_poly*100:.1f}% variance explained)\")\n",
    "print(f\"  Distance -> Passengers: R² = {r2_passengers_poly:.4f} ({r2_passengers_poly*100:.1f}% variance explained)\")\n",
    "\n",
    "# 3. BUSINESS INSIGHTS\n",
    "print(\"\\n3. BUSINESS INSIGHTS\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "print(\"DISTANCE IMPACT ON FARE:\")\n",
    "print(f\"  • Strong positive correlation (r = {distance_fare_corr:.3f})\")\n",
    "print(f\"  • Linear model explains {r2_fare*100:.1f}% of fare variance\")\n",
    "print(f\"  • Polynomial model explains {r2_fare_poly*100:.1f}% of fare variance\")\n",
    "print(f\"  • Business implication: Distance is a key pricing factor\")\n",
    "\n",
    "print(\"\\nDISTANCE IMPACT ON PASSENGERS:\")\n",
    "print(f\"  • Very weak negative correlation (r = {distance_passengers_corr:.3f})\")\n",
    "print(f\"  • Linear model explains {r2_passengers*100:.1f}% of passenger variance\")\n",
    "print(f\"  • Polynomial model explains {r2_passengers_poly*100:.1f}% of passenger variance\")\n",
    "print(f\"  • Business implication: Distance has minimal impact on passenger demand\")\n",
    "\n",
    "# 4. MODEL COMPARISON\n",
    "print(\"\\n4. MODEL COMPARISON\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "print(\"FARE PREDICTION:\")\n",
    "print(f\"  Linear RMSE: {rmse_fare:.4f}\")\n",
    "print(f\"  Polynomial RMSE: {rmse_fare_poly:.4f}\")\n",
    "if rmse_fare_poly < rmse_fare:\n",
    "    improvement = ((rmse_fare - rmse_fare_poly) / rmse_fare * 100)\n",
    "    print(f\"  Polynomial improvement: {improvement:.2f}%\")\n",
    "else:\n",
    "    print(f\"  Linear model performs better\")\n",
    "\n",
    "print(\"\\nPASSENGER PREDICTION:\")\n",
    "print(f\"  Linear RMSE: {rmse_passengers:.4f}\")\n",
    "print(f\"  Polynomial RMSE: {rmse_passengers_poly:.4f}\")\n",
    "if rmse_passengers_poly < rmse_passengers:\n",
    "    improvement = ((rmse_passengers - rmse_passengers_poly) / rmse_passengers * 100)\n",
    "    print(f\"  Polynomial improvement: {improvement:.2f}%\")\n",
    "else:\n",
    "    print(f\"  Linear model performs better\")\n",
    "\n",
    "# 5. RECOMMENDATIONS\n",
    "print(\"\\n5. RECOMMENDATIONS\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "print(\"FOR AIRLINE PRICING:\")\n",
    "print(\"  • Distance is a strong predictor of fare (R² = {:.3f})\".format(r2_fare_poly))\n",
    "print(\"  • Use distance-based pricing models for fare optimization\")\n",
    "print(\"  • Consider polynomial relationships for complex pricing strategies\")\n",
    "\n",
    "print(\"\\nFOR ROUTE PLANNING:\")\n",
    "print(\"  • Distance has minimal impact on passenger demand (R² = {:.3f})\".format(r2_passengers_poly))\n",
    "print(\"  • Focus on other factors (frequency, timing, competition) for passenger growth\")\n",
    "print(\"  • Distance-based passenger prediction is not reliable\")\n",
    "\n",
    "print(\"\\nFOR MODEL SELECTION:\")\n",
    "print(\"  • Use polynomial regression for fare prediction (better R²)\")\n",
    "print(\"  • Use linear regression for passenger prediction (simpler, similar performance)\")\n",
    "print(\"  • Consider additional features beyond distance for better predictions\")\n",
    "\n",
    "# 6. LIMITATIONS\n",
    "print(\"\\n6. LIMITATIONS\")\n",
    "print(\"-\" * 30)\n",
    "print(\"  • Single variable analysis (distance only)\")\n",
    "print(\"  • Missing other important factors (competition, seasonality, etc.)\")\n",
    "print(\"  • Low R² for passenger prediction suggests other factors are more important\")\n",
    "print(\"  • Correlation does not imply causation\")\n",
    "\n",
    "# 7. NEXT STEPS\n",
    "print(\"\\n7. NEXT STEPS\")\n",
    "print(\"-\" * 30)\n",
    "print(\"  • Add more features (competition, seasonality, route characteristics)\")\n",
    "print(\"  • Implement ensemble methods for better predictions\")\n",
    "print(\"  • Analyze temporal trends and seasonality\")\n",
    "print(\"  • Consider market-specific factors\")\n",
    "\n",
    "print(f\"\\n✅ Analysis completed successfully!\")\n",
    "print(f\"📊 Dataset: {df.count():,} observations\")\n",
    "print(f\"🔍 Methods: Correlation, Linear Regression, Polynomial Regression\")\n",
    "print(f\"🎯 Focus: Distance impact on Fare & Passengers\")\n",
    "print(f\"📈 Results: Distance strongly affects fare, weakly affects passengers\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "41cd90d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "FEATURE ENGINEERING - MULTI-FEATURE MODEL\n",
      "============================================================\n",
      "1. FEATURE OVERVIEW\n",
      "------------------------------\n",
      "Numeric features: ['nsmiles', 'passengers', 'quarter', 'Year', 'large_ms', 'lf_ms']\n",
      "Categorical features: ['carrier_lg']\n",
      "Target variables: ['fare', 'fare_lg', 'fare_low']\n",
      "\n",
      "2. DATA TYPES VERIFICATION\n",
      "------------------------------\n",
      "root\n",
      " |-- nsmiles: integer (nullable = true)\n",
      " |-- passengers: integer (nullable = true)\n",
      " |-- quarter: integer (nullable = true)\n",
      " |-- Year: integer (nullable = true)\n",
      " |-- large_ms: double (nullable = true)\n",
      " |-- lf_ms: double (nullable = true)\n",
      " |-- carrier_lg: string (nullable = true)\n",
      " |-- fare: double (nullable = true)\n",
      " |-- fare_lg: double (nullable = true)\n",
      " |-- fare_low: double (nullable = true)\n",
      "\n",
      "\n",
      "3. CATEGORICAL FEATURES ANALYSIS\n",
      "------------------------------\n",
      "carrier_lg: 67 unique values\n",
      "+----------+-----+\n",
      "|carrier_lg|count|\n",
      "+----------+-----+\n",
      "|        WN|58999|\n",
      "|        AA|45818|\n",
      "|        DL|34081|\n",
      "|        UA|29802|\n",
      "|        US|25835|\n",
      "|        CO|14397|\n",
      "|        NW| 7169|\n",
      "|        B6| 6137|\n",
      "|        AS| 4121|\n",
      "|        HP| 3721|\n",
      "+----------+-----+\n",
      "only showing top 10 rows\n",
      "\n",
      "\n",
      "4. NUMERIC FEATURES STATISTICS\n",
      "------------------------------\n",
      "+-------+------------------+-----------------+------------------+-----------------+-------------------+-------------------+\n",
      "|summary|           nsmiles|       passengers|           quarter|             Year|           large_ms|              lf_ms|\n",
      "+-------+------------------+-----------------+------------------+-----------------+-------------------+-------------------+\n",
      "|  count|            244343|           244343|            244343|           244343|             244343|             244343|\n",
      "|   mean|1189.4210556471844|301.4425582071105|2.4790929144685956|2008.577507847575| 0.6654360480963714|0.45043751243130975|\n",
      "| stddev| 702.8892334240609|512.4984522750754|  1.12208154280452|8.688731844202687|0.22441010659554572| 0.3326690289129114|\n",
      "|    min|               109|                0|                 1|             1993|                0.1|               0.01|\n",
      "|    max|              2724|             8301|                 4|             2024|                1.0|                1.0|\n",
      "+-------+------------------+-----------------+------------------+-----------------+-------------------+-------------------+\n",
      "\n",
      "✅ Feature engineering preparation completed!\n"
     ]
    }
   ],
   "source": [
    "# FEATURE ENGINEERING - Multi-feature Model\n",
    "print(\"=\" * 60)\n",
    "print(\"FEATURE ENGINEERING - MULTI-FEATURE MODEL\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Import necessary functions\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler, StringIndexer, OneHotEncoder\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.functions import col as spark_col\n",
    "\n",
    "print(\"1. FEATURE OVERVIEW\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# Define features by type\n",
    "numeric_features = ['nsmiles', 'passengers', 'quarter', 'Year', 'large_ms', 'lf_ms']\n",
    "categorical_features = ['carrier_lg']  # carrier_low nếu cần\n",
    "target_features = ['fare', 'fare_lg', 'fare_low']\n",
    "\n",
    "print(f\"Numeric features: {numeric_features}\")\n",
    "print(f\"Categorical features: {categorical_features}\")\n",
    "print(f\"Target variables: {target_features}\")\n",
    "\n",
    "# Check data types\n",
    "print(\"\\n2. DATA TYPES VERIFICATION\")\n",
    "print(\"-\" * 30)\n",
    "df.select(numeric_features + categorical_features + target_features).printSchema()\n",
    "\n",
    "# Check unique values in categorical features\n",
    "print(\"\\n3. CATEGORICAL FEATURES ANALYSIS\")\n",
    "print(\"-\" * 30)\n",
    "for cat_feature in categorical_features:\n",
    "    unique_count = df.select(cat_feature).distinct().count()\n",
    "    print(f\"{cat_feature}: {unique_count} unique values\")\n",
    "    df.groupBy(cat_feature).count().orderBy(\"count\", ascending=False).show(10)\n",
    "\n",
    "# Check numeric features statistics\n",
    "print(\"\\n4. NUMERIC FEATURES STATISTICS\")\n",
    "print(\"-\" * 30)\n",
    "df.select(numeric_features).describe().show()\n",
    "\n",
    "print(\"✅ Feature engineering preparation completed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f561d5c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "CORRELATION ANALYSIS - MULTI-VARIABLE\n",
      "============================================================\n",
      "1. CORRELATION MATRIX - ALL NUMERIC FEATURES\n",
      "------------------------------\n",
      "\n",
      "Correlation with FARE:\n",
      "  nsmiles        :  0.5122\n",
      "  passengers     : -0.1744\n",
      "  quarter        : -0.0268\n",
      "  Year           :  0.1639\n",
      "  large_ms       : -0.1827\n",
      "  lf_ms          : -0.1888\n",
      "  fare_lg        :  0.9562\n",
      "  fare_low       :  0.8747\n",
      "\n",
      "2. CORRELATION WITH FARE_LG:\n",
      "------------------------------\n",
      "  nsmiles        :  0.4835\n",
      "  passengers     : -0.1270\n",
      "  quarter        : -0.0244\n",
      "  Year           :  0.1545\n",
      "  large_ms       : -0.1788\n",
      "  lf_ms          : -0.2356\n",
      "  fare           :  0.9562\n",
      "  fare_low       :  0.8281\n",
      "\n",
      "3. CORRELATION WITH FARE_LOW:\n",
      "------------------------------\n",
      "  nsmiles        :  0.4167\n",
      "  passengers     : -0.2053\n",
      "  quarter        : -0.0159\n",
      "  Year           :  0.1743\n",
      "  large_ms       : -0.0821\n",
      "  lf_ms          :  0.0573\n",
      "  fare           :  0.8747\n",
      "  fare_lg        :  0.8281\n",
      "\n",
      "4. STRONG PREDICTORS FOR FARE:\n",
      "------------------------------\n",
      "  fare_lg        :  0.9562\n",
      "  fare_low       :  0.8747\n",
      "  nsmiles        :  0.5122\n",
      "\n",
      "✅ Found 3 strong predictors for fare!\n"
     ]
    }
   ],
   "source": [
    "# CORRELATION ANALYSIS - Multi-variable\n",
    "print(\"=\" * 60)\n",
    "print(\"CORRELATION ANALYSIS - MULTI-VARIABLE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "import builtins\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.stat import Correlation\n",
    "\n",
    "# 1. CORRELATION MATRIX FOR ALL NUMERIC FEATURES\n",
    "print(\"1. CORRELATION MATRIX - ALL NUMERIC FEATURES\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# Select numeric features including targets\n",
    "all_numeric = numeric_features + target_features\n",
    "correlation_data = df.select(all_numeric)\n",
    "\n",
    "# Calculate correlation matrix\n",
    "assembler_corr = VectorAssembler(inputCols=all_numeric, outputCol=\"features\")\n",
    "corr_df = assembler_corr.transform(correlation_data).select(\"features\")\n",
    "\n",
    "correlation_matrix = Correlation.corr(corr_df, \"features\").collect()[0][0]\n",
    "correlation_array = correlation_matrix.toArray()\n",
    "\n",
    "# Display correlation with Fare (target)\n",
    "print(\"\\nCorrelation with FARE:\")\n",
    "fare_idx = all_numeric.index('fare')\n",
    "for i, var in enumerate(all_numeric):\n",
    "    if var != 'fare':\n",
    "        corr_value = correlation_array[fare_idx, i]\n",
    "        print(f\"  {var:15}: {corr_value:7.4f}\")\n",
    "\n",
    "# 2. CORRELATION WITH FARE_LG\n",
    "print(\"\\n2. CORRELATION WITH FARE_LG:\")\n",
    "print(\"-\" * 30)\n",
    "fare_lg_idx = all_numeric.index('fare_lg')\n",
    "for i, var in enumerate(all_numeric):\n",
    "    if var != 'fare_lg':\n",
    "        corr_value = correlation_array[fare_lg_idx, i]\n",
    "        print(f\"  {var:15}: {corr_value:7.4f}\")\n",
    "\n",
    "# 3. CORRELATION WITH FARE_LOW\n",
    "print(\"\\n3. CORRELATION WITH FARE_LOW:\")\n",
    "print(\"-\" * 30)\n",
    "fare_low_idx = all_numeric.index('fare_low')\n",
    "for i, var in enumerate(all_numeric):\n",
    "    if var != 'fare_low':\n",
    "        corr_value = correlation_array[fare_low_idx, i]\n",
    "        print(f\"  {var:15}: {corr_value:7.4f}\")\n",
    "\n",
    "# 4. IDENTIFY STRONG PREDICTORS\n",
    "print(\"\\n4. STRONG PREDICTORS FOR FARE:\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "strong_predictors = []\n",
    "for i, var in enumerate(all_numeric):\n",
    "    if var != 'fare':\n",
    "        corr_value = correlation_array[fare_idx, i]\n",
    "        if builtins.abs(corr_value) > 0.3:\n",
    "            strong_predictors.append((var, corr_value))\n",
    "\n",
    "strong_predictors.sort(key=lambda x: builtins.abs(x[1]), reverse=True)\n",
    "for var, corr_val in strong_predictors:\n",
    "    print(f\"  {var:15}: {corr_val:7.4f}\")\n",
    "\n",
    "print(f\"\\n✅ Found {len(strong_predictors)} strong predictors for fare!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "15edfbf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "LINEAR REGRESSION - MULTI-FEATURE MODEL\n",
      "============================================================\n",
      "1. FEATURE PREPARATION\n",
      "------------------------------\n",
      "Input features: ['nsmiles', 'passengers', 'quarter', 'Year', 'large_ms', 'lf_ms', 'carrier_lg_idx']\n",
      "Total features: 7\n",
      "\n",
      "2. CREATE MODELS\n",
      "------------------------------\n",
      "\n",
      "3. HYPERPARAMETER TUNING\n",
      "------------------------------\n",
      "Parameter grid size: 24\n",
      "\n",
      "4. CREATE PIPELINES\n",
      "------------------------------\n",
      "\n",
      "5. CROSS-VALIDATION SETUP\n",
      "------------------------------\n",
      "Cross-validator created with 3-fold CV\n",
      "\n",
      "6. TRAINING MULTI-FEATURE MODEL\n",
      "------------------------------\n",
      "Training data: 195,235 rows\n",
      "Test data: 49,108 rows\n",
      "Training multi-feature model...\n",
      "\n",
      "7. MODEL EVALUATION\n",
      "------------------------------\n",
      "Multi-feature Fare Prediction Model:\n",
      "  RMSE: 64.8726\n",
      "  R²: 0.3298\n",
      "  MAE: 47.0739\n",
      "\n",
      "8. BEST PARAMETERS\n",
      "------------------------------\n",
      "Best parameters:\n",
      "  Regularization: 0.01\n",
      "  Elastic Net: 0.5\n",
      "  Max Iterations: 100\n",
      "\n",
      "9. FEATURE IMPORTANCE\n",
      "------------------------------\n",
      "Features ranked by absolute coefficient value:\n",
      "  nsmiles        :    37.9124\n",
      "  Year           :    12.7432\n",
      "  passengers     :   -12.7268\n",
      "  lf_ms          :    -9.9639\n",
      "  carrier_lg_idx :    -6.4577\n",
      "  large_ms       :     3.6032\n",
      "  quarter        :    -2.0645\n",
      "\n",
      "✅ Multi-feature Linear Regression completed!\n"
     ]
    }
   ],
   "source": [
    "# LINEAR REGRESSION - MULTI-FEATURE MODEL\n",
    "print(\"=\" * 60)\n",
    "print(\"LINEAR REGRESSION - MULTI-FEATURE MODEL\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler, StringIndexer\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# 1. FEATURE PREPARATION\n",
    "print(\"1. FEATURE PREPARATION\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# Encode categorical features\n",
    "carrier_indexer = StringIndexer(inputCol=\"carrier_lg\", outputCol=\"carrier_lg_idx\", handleInvalid=\"keep\")\n",
    "\n",
    "# Define input features for model\n",
    "input_features = ['nsmiles', 'passengers', 'quarter', 'Year', 'large_ms', 'lf_ms', 'carrier_lg_idx']\n",
    "\n",
    "# Assemble features\n",
    "assembler_multi = VectorAssembler(inputCols=input_features, outputCol=\"features\")\n",
    "\n",
    "# Scale features\n",
    "scaler_multi = StandardScaler(inputCol=\"features\", outputCol=\"scaled_features\", withStd=True, withMean=True)\n",
    "\n",
    "print(f\"Input features: {input_features}\")\n",
    "print(f\"Total features: {len(input_features)}\")\n",
    "\n",
    "# 2. CREATE MODELS\n",
    "print(\"\\n2. CREATE MODELS\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# Model 1: Predict Fare\n",
    "lr_fare_multi = LinearRegression(\n",
    "    featuresCol=\"scaled_features\", \n",
    "    labelCol=\"fare\", \n",
    "    predictionCol=\"fare_pred_multi\"\n",
    ")\n",
    "\n",
    "# Model 2: Predict Fare_lg\n",
    "lr_fare_lg_multi = LinearRegression(\n",
    "    featuresCol=\"scaled_features\", \n",
    "    labelCol=\"fare_lg\", \n",
    "    predictionCol=\"fare_lg_pred_multi\"\n",
    ")\n",
    "\n",
    "# Model 3: Predict Fare_low\n",
    "lr_fare_low_multi = LinearRegression(\n",
    "    featuresCol=\"scaled_features\", \n",
    "    labelCol=\"fare_low\", \n",
    "    predictionCol=\"fare_low_pred_multi\"\n",
    ")\n",
    "\n",
    "# 3. HYPERPARAMETER TUNING\n",
    "print(\"\\n3. HYPERPARAMETER TUNING\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# Create parameter grid\n",
    "param_grid_multi = ParamGridBuilder() \\\n",
    "    .addGrid(lr_fare_multi.regParam, [0.0, 0.01, 0.1, 1.0]) \\\n",
    "    .addGrid(lr_fare_multi.elasticNetParam, [0.0, 0.5, 1.0]) \\\n",
    "    .addGrid(lr_fare_multi.maxIter, [100, 200]) \\\n",
    "    .build()\n",
    "\n",
    "print(f\"Parameter grid size: {len(param_grid_multi)}\")\n",
    "\n",
    "# 4. CREATE PIPELINES\n",
    "print(\"\\n4. CREATE PIPELINES\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# Pipeline for Fare prediction\n",
    "pipeline_fare_multi = Pipeline(stages=[\n",
    "    carrier_indexer,\n",
    "    assembler_multi,\n",
    "    scaler_multi,\n",
    "    lr_fare_multi\n",
    "])\n",
    "\n",
    "# 5. CROSS-VALIDATION\n",
    "print(\"\\n5. CROSS-VALIDATION SETUP\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "evaluator_multi = RegressionEvaluator(\n",
    "    labelCol=\"fare\", \n",
    "    predictionCol=\"fare_pred_multi\", \n",
    "    metricName=\"rmse\"\n",
    ")\n",
    "\n",
    "cv_multi = CrossValidator(\n",
    "    estimator=pipeline_fare_multi,\n",
    "    estimatorParamMaps=param_grid_multi,\n",
    "    evaluator=evaluator_multi,\n",
    "    numFolds=3,\n",
    "    seed=42,\n",
    "    parallelism=2\n",
    ")\n",
    "\n",
    "print(\"Cross-validator created with 3-fold CV\")\n",
    "\n",
    "# 6. TRAIN MODEL\n",
    "print(\"\\n6. TRAINING MULTI-FEATURE MODEL\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# Split data\n",
    "train_multi, test_multi = df.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "print(f\"Training data: {train_multi.count():,} rows\")\n",
    "print(f\"Test data: {test_multi.count():,} rows\")\n",
    "\n",
    "print(\"Training multi-feature model...\")\n",
    "cv_model_multi = cv_multi.fit(train_multi)\n",
    "\n",
    "# 7. EVALUATE MODEL\n",
    "print(\"\\n7. MODEL EVALUATION\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# Make predictions\n",
    "predictions_multi = cv_model_multi.transform(test_multi)\n",
    "\n",
    "# Evaluate\n",
    "rmse_multi = evaluator_multi.evaluate(predictions_multi)\n",
    "r2_multi = RegressionEvaluator(labelCol=\"fare\", predictionCol=\"fare_pred_multi\", metricName=\"r2\").evaluate(predictions_multi)\n",
    "mae_multi = RegressionEvaluator(labelCol=\"fare\", predictionCol=\"fare_pred_multi\", metricName=\"mae\").evaluate(predictions_multi)\n",
    "\n",
    "print(\"Multi-feature Fare Prediction Model:\")\n",
    "print(f\"  RMSE: {rmse_multi:.4f}\")\n",
    "print(f\"  R²: {r2_multi:.4f}\")\n",
    "print(f\"  MAE: {mae_multi:.4f}\")\n",
    "\n",
    "# 8. BEST PARAMETERS\n",
    "print(\"\\n8. BEST PARAMETERS\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "best_model_multi = cv_model_multi.bestModel\n",
    "lr_stage = best_model_multi.stages[-1]\n",
    "\n",
    "print(\"Best parameters:\")\n",
    "print(f\"  Regularization: {lr_stage.getRegParam()}\")\n",
    "print(f\"  Elastic Net: {lr_stage.getElasticNetParam()}\")\n",
    "print(f\"  Max Iterations: {lr_stage.getMaxIter()}\")\n",
    "\n",
    "# 9. FEATURE IMPORTANCE (via coefficients)\n",
    "print(\"\\n9. FEATURE IMPORTANCE\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "coefficients = lr_stage.coefficients.toArray()\n",
    "feature_importance = list(zip(input_features, coefficients))\n",
    "feature_importance.sort(key=lambda x: builtins.abs(x[1]), reverse=True)\n",
    "\n",
    "print(\"Features ranked by absolute coefficient value:\")\n",
    "for feature, coef in feature_importance:\n",
    "    print(f\"  {feature:15}: {coef:10.4f}\")\n",
    "\n",
    "print(f\"\\n✅ Multi-feature Linear Regression completed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "04d01bb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "MODEL COMPARISON: SINGLE vs MULTI-FEATURE\n",
      "============================================================\n",
      "1. PERFORMANCE COMPARISON\n",
      "------------------------------\n",
      "FARE PREDICTION MODELS:\n",
      "Model                           R²       RMSE        MAE   Features\n",
      "-----------------------------------------------------------------\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'r2_fare' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m65\u001b[39m)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Single-feature model (from previous cells)\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSingle-feature (distance)\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m25\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mr2_fare\u001b[49m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m8.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrmse_fare\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m10.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmae_fare\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m10.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m>10\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMulti-feature (all vars)\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m25\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mr2_multi\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m8.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrmse_multi\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m10.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmae_multi\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m10.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m7\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m>10\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Calculate improvements\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'r2_fare' is not defined"
     ]
    }
   ],
   "source": [
    "# MODEL COMPARISON: Single vs Multi-Feature\n",
    "print(\"=\" * 60)\n",
    "print(\"MODEL COMPARISON: SINGLE vs MULTI-FEATURE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 1. COMPARISON TABLE\n",
    "print(\"1. PERFORMANCE COMPARISON\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "print(\"FARE PREDICTION MODELS:\")\n",
    "print(f\"{'Model':25} {'R²':>8} {'RMSE':>10} {'MAE':>10} {'Features':>10}\")\n",
    "print(\"-\" * 65)\n",
    "\n",
    "# Single-feature model (from previous cells)\n",
    "print(f\"{'Single-feature (distance)':25} {r2_fare:8.4f} {rmse_fare:10.4f} {mae_fare:10.4f} {'1':>10}\")\n",
    "print(f\"{'Multi-feature (all vars)':25} {r2_multi:8.4f} {rmse_multi:10.4f} {mae_multi:10.4f} {'7':>10}\")\n",
    "\n",
    "# Calculate improvements\n",
    "r2_improvement = ((r2_multi - r2_fare) / r2_fare) * 100\n",
    "rmse_improvement = ((rmse_fare - rmse_multi) / rmse_fare) * 100\n",
    "mae_improvement = ((mae_fare - mae_multi) / mae_fare) * 100\n",
    "\n",
    "print(\"-\" * 65)\n",
    "print(f\"{'IMPROVEMENT':25} {r2_improvement:8.1f}% {rmse_improvement:10.1f}% {mae_improvement:10.1f}% {'+6':>10}\")\n",
    "\n",
    "# 2. DETAILED ANALYSIS\n",
    "print(\"\\n2. DETAILED ANALYSIS\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "print(\"R² IMPROVEMENT:\")\n",
    "print(f\"  Single-feature R²: {r2_fare:.4f} ({r2_fare*100:.1f}%)\")\n",
    "print(f\"  Multi-feature R²: {r2_multi:.4f} ({r2_multi*100:.1f}%)\")\n",
    "print(f\"  Improvement: {r2_improvement:.1f}% more variance explained\")\n",
    "\n",
    "print(\"\\nRMSE IMPROVEMENT:\")\n",
    "print(f\"  Single-feature RMSE: {rmse_fare:.4f}\")\n",
    "print(f\"  Multi-feature RMSE: {rmse_multi:.4f}\")\n",
    "print(f\"  Improvement: {rmse_improvement:.1f}% reduction in error\")\n",
    "\n",
    "print(\"\\nMAE IMPROVEMENT:\")\n",
    "print(f\"  Single-feature MAE: {mae_fare:.4f}\")\n",
    "print(f\"  Multi-feature MAE: {mae_multi:.4f}\")\n",
    "print(f\"  Improvement: {mae_improvement:.1f}% reduction in error\")\n",
    "\n",
    "# 3. FEATURE IMPORTANCE INSIGHTS\n",
    "print(\"\\n3. FEATURE IMPORTANCE INSIGHTS\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "print(\"Top 3 most important features:\")\n",
    "for i, (feature, coef) in enumerate(feature_importance[:3]):\n",
    "    print(f\"  {i+1}. {feature:15}: {coef:8.4f}\")\n",
    "\n",
    "print(\"\\nFeature interpretation:\")\n",
    "for feature, coef in feature_importance:\n",
    "    if feature == 'nsmiles':\n",
    "        print(f\"  Distance (nsmiles): {coef:8.4f} - {coef:.2f} dollars per mile\")\n",
    "    elif feature == 'passengers':\n",
    "        print(f\"  Passengers: {coef:8.4f} - {coef:.4f} dollars per passenger\")\n",
    "    elif feature == 'quarter':\n",
    "        print(f\"  Quarter: {coef:8.4f} - seasonal effect\")\n",
    "    elif feature == 'Year':\n",
    "        print(f\"  Year: {coef:8.4f} - temporal trend\")\n",
    "    elif feature == 'large_ms':\n",
    "        print(f\"  Large carrier market share: {coef:8.4f}\")\n",
    "    elif feature == 'lf_ms':\n",
    "        print(f\"  Low-cost carrier market share: {coef:8.4f}\")\n",
    "    elif feature == 'carrier_lg_idx':\n",
    "        print(f\"  Carrier type: {coef:8.4f} - carrier-specific effect\")\n",
    "\n",
    "# 4. BUSINESS IMPLICATIONS\n",
    "print(\"\\n4. BUSINESS IMPLICATIONS\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "print(\"SINGLE-FEATURE MODEL:\")\n",
    "print(f\"  • Simple distance-based pricing\")\n",
    "print(f\"  • Explains {r2_fare*100:.1f}% of fare variance\")\n",
    "print(f\"  • Average error: ±${rmse_fare:.0f}\")\n",
    "print(f\"  • Good for basic distance pricing\")\n",
    "\n",
    "print(\"\\nMULTI-FEATURE MODEL:\")\n",
    "print(f\"  • Comprehensive pricing model\")\n",
    "print(f\"  • Explains {r2_multi*100:.1f}% of fare variance\")\n",
    "print(f\"  • Average error: ±${rmse_multi:.0f}\")\n",
    "print(f\"  • Better for complex pricing strategies\")\n",
    "\n",
    "# 5. RECOMMENDATIONS\n",
    "print(\"\\n5. RECOMMENDATIONS\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "if r2_improvement > 20:\n",
    "    print(\"✅ STRONG RECOMMENDATION: Use multi-feature model\")\n",
    "    print(\"  • Significant improvement in accuracy\")\n",
    "    print(\"  • Worth the additional complexity\")\n",
    "elif r2_improvement > 10:\n",
    "    print(\"✅ MODERATE RECOMMENDATION: Consider multi-feature model\")\n",
    "    print(\"  • Noticeable improvement in accuracy\")\n",
    "    print(\"  • Balance complexity vs. accuracy\")\n",
    "else:\n",
    "    print(\"⚠️  WEAK RECOMMENDATION: Single-feature model may suffice\")\n",
    "    print(\"  • Small improvement in accuracy\")\n",
    "    print(\"  • Consider if complexity is worth it\")\n",
    "\n",
    "print(f\"\\n✅ Model comparison completed!\")\n",
    "print(f\"📊 Multi-feature model shows {r2_improvement:.1f}% improvement in R²\")\n",
    "print(f\"🎯 Use multi-feature model for production pricing systems\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86691f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FINAL SUMMARY - Multi-Feature Analysis\n",
    "print(\"=\" * 60)\n",
    "print(\"FINAL SUMMARY - MULTI-FEATURE ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 1. ANALYSIS OVERVIEW\n",
    "print(\"1. ANALYSIS OVERVIEW\")\n",
    "print(\"-\" * 30)\n",
    "print(f\"Dataset: {df.count():,} observations\")\n",
    "print(f\"Time period: 1993-2024\")\n",
    "print(f\"Features analyzed: {len(input_features)} variables\")\n",
    "print(f\"Target variable: Fare prediction\")\n",
    "\n",
    "# 2. FEATURE SUMMARY\n",
    "print(\"\\n2. FEATURE SUMMARY\")\n",
    "print(\"-\" * 30)\n",
    "print(\"Input features used:\")\n",
    "for i, feature in enumerate(input_features, 1):\n",
    "    print(f\"  {i}. {feature}\")\n",
    "\n",
    "print(f\"\\nFeature types:\")\n",
    "print(f\"  • Numeric features: {len([f for f in input_features if f not in ['carrier_lg_idx']])}\")\n",
    "print(f\"  • Categorical features: 1 (carrier_lg)\")\n",
    "\n",
    "# 3. MODEL PERFORMANCE SUMMARY\n",
    "print(\"\\n3. MODEL PERFORMANCE SUMMARY\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "print(\"SINGLE-FEATURE MODEL (Distance only):\")\n",
    "print(f\"  • Features: 1 (nsmiles)\")\n",
    "print(f\"  • R²: {r2_fare:.4f} ({r2_fare*100:.1f}%)\")\n",
    "print(f\"  • RMSE: {rmse_fare:.4f}\")\n",
    "print(f\"  • MAE: {mae_fare:.4f}\")\n",
    "\n",
    "print(\"\\nMULTI-FEATURE MODEL (All variables):\")\n",
    "print(f\"  • Features: {len(input_features)}\")\n",
    "print(f\"  • R²: {r2_multi:.4f} ({r2_multi*100:.1f}%)\")\n",
    "print(f\"  • RMSE: {rmse_multi:.4f}\")\n",
    "print(f\"  • MAE: {mae_multi:.4f}\")\n",
    "\n",
    "# 4. KEY FINDINGS\n",
    "print(\"\\n4. KEY FINDINGS\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "print(\"IMPROVEMENT ACHIEVED:\")\n",
    "print(f\"  • R² improvement: {r2_improvement:.1f}%\")\n",
    "print(f\"  • RMSE reduction: {rmse_improvement:.1f}%\")\n",
    "print(f\"  • MAE reduction: {mae_improvement:.1f}%\")\n",
    "\n",
    "print(\"\\nMOST IMPORTANT FEATURES:\")\n",
    "for i, (feature, coef) in enumerate(feature_importance[:3], 1):\n",
    "    print(f\"  {i}. {feature}: coefficient = {coef:.4f}\")\n",
    "\n",
    "# 5. BUSINESS INSIGHTS\n",
    "print(\"\\n5. BUSINESS INSIGHTS\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "print(\"PRICING INSIGHTS:\")\n",
    "print(f\"  • Distance remains important: {feature_importance[0][1]:.2f} dollars per mile\")\n",
    "print(f\"  • Passenger volume impact: {feature_importance[1][1]:.4f} dollars per passenger\")\n",
    "print(f\"  • Seasonal effects: Quarter coefficient = {feature_importance[2][1]:.4f}\")\n",
    "\n",
    "print(\"\\nCOMPETITIVE INSIGHTS:\")\n",
    "for feature, coef in feature_importance:\n",
    "    if 'ms' in feature:\n",
    "        print(f\"  • {feature}: {coef:.4f} (market share effect)\")\n",
    "\n",
    "# 6. RECOMMENDATIONS\n",
    "print(\"\\n6. RECOMMENDATIONS\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "print(\"FOR AIRLINE PRICING STRATEGY:\")\n",
    "print(f\"  ✅ Use multi-feature model for production pricing\")\n",
    "print(f\"  ✅ Distance-based pricing with {len(input_features)} variables\")\n",
    "print(f\"  ✅ Consider seasonal and competitive factors\")\n",
    "\n",
    "print(\"\\nFOR MODEL DEPLOYMENT:\")\n",
    "if r2_improvement > 15:\n",
    "    print(f\"  ✅ STRONG CASE: {r2_improvement:.1f}% improvement justifies complexity\")\n",
    "elif r2_improvement > 5:\n",
    "    print(f\"  ⚠️  MODERATE CASE: {r2_improvement:.1f}% improvement - evaluate trade-offs\")\n",
    "else:\n",
    "    print(f\"  ❌ WEAK CASE: {r2_improvement:.1f}% improvement may not justify complexity\")\n",
    "\n",
    "# 7. NEXT STEPS\n",
    "print(\"\\n7. NEXT STEPS\")\n",
    "print(\"-\" * 30)\n",
    "print(\"  • Deploy multi-feature model for fare prediction\")\n",
    "print(\"  • Monitor model performance in production\")\n",
    "print(\"  • Consider additional features (competition, seasonality)\")\n",
    "print(\"  • Implement ensemble methods for better accuracy\")\n",
    "print(\"  • Add real-time model retraining\")\n",
    "\n",
    "print(f\"\\n🎯 CONCLUSION:\")\n",
    "print(f\"Multi-feature model with {len(input_features)} variables shows {r2_improvement:.1f}% improvement\")\n",
    "print(f\"over single-feature model, making it suitable for production pricing systems.\")\n",
    "print(f\"\\n✅ Multi-feature analysis completed successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f82a47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "RANDOM FOREST MODEL - DISTANCE, QUARTER, YEAR\n",
      "============================================================\n",
      "1. FEATURE PREPARATION\n",
      "------------------------------\n",
      "Selected features: ['nsmiles', 'quarter', 'Year']\n",
      "\n",
      "2. CREATE RANDOM FOREST MODEL\n",
      "------------------------------\n",
      "Random Forest parameters:\n",
      "  Number of Trees: 100\n",
      "  Max Depth: 10\n",
      "  Max Bins: 32\n",
      "\n",
      "3. HYPERPARAMETER TUNING\n",
      "------------------------------\n",
      "Parameter grid size: 108\n",
      "\n",
      "4. CREATE PIPELINE\n",
      "------------------------------\n",
      "\n",
      "5. CROSS-VALIDATION SETUP\n",
      "------------------------------\n",
      "Cross-validator created with 3-fold CV\n",
      "\n",
      "6. TRAINING RANDOM FOREST MODEL\n",
      "------------------------------\n",
      "Training data: 195,235 rows\n",
      "Test data: 49,108 rows\n",
      "Training Random Forest model...\n",
      "\n",
      "7. MODEL EVALUATION\n",
      "------------------------------\n",
      "Random Forest Fare Prediction Model:\n",
      "  RMSE: 63.5017\n",
      "  R²: 0.3578\n",
      "  MAE: 46.1637\n",
      "\n",
      "8. BEST PARAMETERS\n",
      "------------------------------\n",
      "Best Random Forest parameters:\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'int' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 124\u001b[0m\n\u001b[1;32m    121\u001b[0m rf_stage \u001b[38;5;241m=\u001b[39m best_rf_model\u001b[38;5;241m.\u001b[39mstages[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest Random Forest parameters:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 124\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  Number of Trees: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mrf_stage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetNumTrees\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  Max Depth: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrf_stage\u001b[38;5;241m.\u001b[39mgetMaxDepth()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  Max Bins: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrf_stage\u001b[38;5;241m.\u001b[39mgetMaxBins()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: 'int' object is not callable"
     ]
    }
   ],
   "source": [
    "# RANDOM FOREST MODEL - Distance, Quarter, Year Features\n",
    "print(\"=\" * 60)\n",
    "print(\"RANDOM FOREST MODEL - DISTANCE, QUARTER, YEAR\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# 1. FEATURE PREPARATION\n",
    "print(\"1. FEATURE PREPARATION\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# Chọn features: khoảng cách, quý, năm\n",
    "rf_features = ['nsmiles', 'quarter', 'Year']\n",
    "print(f\"Selected features: {rf_features}\")\n",
    "\n",
    "# Assemble features\n",
    "rf_assembler = VectorAssembler(inputCols=rf_features, outputCol=\"rf_features\")\n",
    "\n",
    "# Scale features\n",
    "rf_scaler = StandardScaler(inputCol=\"rf_features\", outputCol=\"rf_scaled_features\", withStd=True, withMean=True)\n",
    "\n",
    "# 2. CREATE RANDOM FOREST MODEL\n",
    "print(\"\\n2. CREATE RANDOM FOREST MODEL\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "rf = RandomForestRegressor(\n",
    "    featuresCol=\"rf_scaled_features\",\n",
    "    labelCol=\"fare\",\n",
    "    predictionCol=\"fare_pred_rf\",\n",
    "    numTrees=100,\n",
    "    maxDepth=10,\n",
    "    maxBins=32,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "print(\"Random Forest parameters:\")\n",
    "print(f\"  Number of Trees: {rf.getNumTrees()}\")\n",
    "print(f\"  Max Depth: {rf.getMaxDepth()}\")\n",
    "print(f\"  Max Bins: {rf.getMaxBins()}\")\n",
    "\n",
    "# 3. HYPERPARAMETER TUNING\n",
    "print(\"\\n3. HYPERPARAMETER TUNING\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# Create parameter grid for Random Forest\n",
    "rf_param_grid = ParamGridBuilder() \\\n",
    "    .addGrid(rf.numTrees, [50, 100, 200]) \\\n",
    "    .addGrid(rf.maxDepth, [5, 10, 15, 20]) \\\n",
    "    .addGrid(rf.maxBins, [16, 32, 64]) \\\n",
    "    .addGrid(rf.minInstancesPerNode, [1, 5, 10]) \\\n",
    "    .build()\n",
    "\n",
    "print(f\"Parameter grid size: {len(rf_param_grid)}\")\n",
    "\n",
    "# 4. CREATE PIPELINE\n",
    "print(\"\\n4. CREATE PIPELINE\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "rf_pipeline = Pipeline(stages=[rf_assembler, rf_scaler, rf])\n",
    "\n",
    "# 5. CROSS-VALIDATION\n",
    "print(\"\\n5. CROSS-VALIDATION SETUP\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "rf_evaluator = RegressionEvaluator(\n",
    "    labelCol=\"fare\",\n",
    "    predictionCol=\"fare_pred_rf\",\n",
    "    metricName=\"rmse\"\n",
    ")\n",
    "\n",
    "rf_cv = CrossValidator(\n",
    "    estimator=rf_pipeline,\n",
    "    estimatorParamMaps=rf_param_grid,\n",
    "    evaluator=rf_evaluator,\n",
    "    numFolds=3,\n",
    "    seed=42,\n",
    "    parallelism=2\n",
    ")\n",
    "\n",
    "print(\"Cross-validator created with 3-fold CV\")\n",
    "\n",
    "# 6. TRAIN MODEL\n",
    "print(\"\\n6. TRAINING RANDOM FOREST MODEL\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# Split data\n",
    "train_rf, test_rf = df.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "print(f\"Training data: {train_rf.count():,} rows\")\n",
    "print(f\"Test data: {test_rf.count():,} rows\")\n",
    "\n",
    "print(\"Training Random Forest model...\")\n",
    "rf_cv_model = rf_cv.fit(train_rf)\n",
    "\n",
    "# 7. EVALUATE MODEL\n",
    "print(\"\\n7. MODEL EVALUATION\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# Make predictions\n",
    "rf_predictions = rf_cv_model.transform(test_rf)\n",
    "\n",
    "# Evaluate\n",
    "rf_rmse = rf_evaluator.evaluate(rf_predictions)\n",
    "rf_r2 = RegressionEvaluator(labelCol=\"fare\", predictionCol=\"fare_pred_rf\", metricName=\"r2\").evaluate(rf_predictions)\n",
    "rf_mae = RegressionEvaluator(labelCol=\"fare\", predictionCol=\"fare_pred_rf\", metricName=\"mae\").evaluate(rf_predictions)\n",
    "\n",
    "print(\"Random Forest Fare Prediction Model:\")\n",
    "print(f\"  RMSE: {rf_rmse:.4f}\")\n",
    "print(f\"  R²: {rf_r2:.4f}\")\n",
    "print(f\"  MAE: {rf_mae:.4f}\")\n",
    "\n",
    "# 8. BEST PARAMETERS\n",
    "print(\"\\n8. BEST PARAMETERS\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "best_rf_model = rf_cv_model.bestModel\n",
    "rf_stage = best_rf_model.stages[-1]\n",
    "\n",
    "print(\"Best Random Forest parameters:\")\n",
    "print(f\"  Number of Trees: {rf_stage.numTrees}\")\n",
    "print(f\"  Max Depth: {rf_stage.maxDepth}\")\n",
    "print(f\"  Max Bins: {rf_stage.maxBins}\")\n",
    "print(f\"  Min Instances Per Node: {rf_stage.minInstancesPerNode}\")\n",
    "\n",
    "# 9. FEATURE IMPORTANCE\n",
    "print(\"\\n9. FEATURE IMPORTANCE\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "feature_importance = rf_stage.featureImportances.toArray()\n",
    "rf_feature_importance = list(zip(rf_features, feature_importance))\n",
    "rf_feature_importance.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(\"Features ranked by importance:\")\n",
    "for feature, importance in rf_feature_importance:\n",
    "    print(f\"  {feature:15}: {importance:8.4f} ({importance*100:.2f}%)\")\n",
    "\n",
    "# 10. MODEL COMPARISON\n",
    "print(\"\\n10. MODEL COMPARISON\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "print(\"Linear Regression vs Random Forest:\")\n",
    "print(f\"{'Model':25} {'R²':>8} {'RMSE':>10} {'MAE':>10}\")\n",
    "print(\"-\" * 55)\n",
    "\n",
    "# So sánh với Linear Regression (single feature)\n",
    "print(f\"{'Linear (distance only)':25} {r2_fare:8.4f} {rmse_fare:10.4f} {mae_fare:10.4f}\")\n",
    "print(f\"{'Random Forest (3 features)':25} {rf_r2:8.4f} {rf_rmse:10.4f} {rf_mae:10.4f}\")\n",
    "\n",
    "# Calculate improvements\n",
    "r2_improvement = ((rf_r2 - r2_fare) / r2_fare) * 100\n",
    "rmse_improvement = ((rmse_fare - rf_rmse) / rmse_fare) * 100\n",
    "mae_improvement = ((mae_fare - rf_mae) / mae_fare) * 100\n",
    "\n",
    "print(\"-\" * 55)\n",
    "print(f\"{'IMPROVEMENT':25} {r2_improvement:8.1f}% {rmse_improvement:10.1f}% {mae_improvement:10.1f}%\")\n",
    "\n",
    "# 11. BUSINESS INSIGHTS\n",
    "print(\"\\n11. BUSINESS INSIGHTS\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "print(\"RANDOM FOREST INSIGHTS:\")\n",
    "print(f\"  • R²: {rf_r2:.4f} ({rf_r2*100:.1f}% of fare variance explained)\")\n",
    "print(f\"  • Most important feature: {rf_feature_importance[0][0]} ({rf_feature_importance[0][1]*100:.1f}%)\")\n",
    "print(f\"  • Second most important: {rf_feature_importance[1][0]} ({rf_feature_importance[1][1]*100:.1f}%)\")\n",
    "print(f\"  • Third most important: {rf_feature_importance[2][0]} ({rf_feature_importance[2][1]*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\n✅ Random Forest model completed!\")\n",
    "print(f\"🎯 Features: {rf_features}\")\n",
    "print(f\"📊 Performance: R² = {rf_r2:.4f}, RMSE = {rf_rmse:.4f}\")\n",
    "print(f\"🔍 Most important: {rf_feature_importance[0][0]} ({rf_feature_importance[0][1]*100:.1f}%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e1e617",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
