{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark session initialized successfully.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CELL 1: IMPORT LIBRARIES AND INITIALIZE SPARK\n",
    "# ============================================================\n",
    "\n",
    "# Import PySpark libraries\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import VectorAssembler, StringIndexer, StandardScaler\n",
    "from pyspark.ml.classification import LogisticRegression, RandomForestClassifier, GBTClassifier, DecisionTreeClassifier, NaiveBayes, MultilayerPerceptronClassifier, LinearSVC\n",
    "from pyspark.ml.regression import LinearRegression, RandomForestRegressor, GBTRegressor, DecisionTreeRegressor\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator, RegressionEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, TrainValidationSplit, CrossValidator\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"AviationTrendAnalysis\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"Spark session initialized successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully: 245,955 rows, 23 columns\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CELL 2: LOAD DATA FROM CSV FILE\n",
    "# ============================================================\n",
    "\n",
    "# Load aviation data from CSV file\n",
    "file_path = \"../data/US Airline Flight Routes and Fares 1993-2024.csv\"\n",
    "\n",
    "df = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"multiLine\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .option(\"quote\", '\"') \\\n",
    "    .option(\"escape\", '\"') \\\n",
    "    .csv(file_path)\n",
    "\n",
    "print(f\"Data loaded successfully: {df.count():,} rows, {len(df.columns)} columns\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data cleaning completed.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CELL 3: DATA CLEANING - WHITESPACE HANDLING\n",
    "# ============================================================\n",
    "\n",
    "# Import functions for data cleaning\n",
    "from pyspark.sql.functions import col, trim, regexp_replace\n",
    "\n",
    "# Identify string columns\n",
    "string_columns = [field.name for field in df.schema.fields if field.dataType.typeName() == 'string']\n",
    "\n",
    "# Remove leading/trailing whitespace\n",
    "for col_name in string_columns:\n",
    "    df = df.withColumn(col_name, trim(col(col_name)))\n",
    "\n",
    "# Normalize whitespace (replace multiple spaces with single space)\n",
    "for col_name in string_columns:\n",
    "    df = df.withColumn(col_name, regexp_replace(col(col_name), \"\\\\s+\", \" \"))\n",
    "\n",
    "print(\"Data cleaning completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data type conversion completed: 13 columns converted to double.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CELL 4: DATA TYPE CONVERSION\n",
    "# ============================================================\n",
    "\n",
    "# Define numeric columns to convert\n",
    "numeric_columns = ['Year', 'quarter', 'citymarketid_1', 'citymarketid_2', \n",
    "                   'airportid_1', 'airportid_2', 'nsmiles', 'passengers', \n",
    "                   'fare', 'large_ms', 'fare_lg', 'lf_ms', 'fare_low']\n",
    "\n",
    "# Convert numeric columns to double\n",
    "converted_count = 0\n",
    "for col_name in numeric_columns:\n",
    "    if col_name in df.columns:\n",
    "        df = df.withColumn(col_name, col(col_name).cast(\"double\"))\n",
    "        converted_count += 1\n",
    "\n",
    "print(f\"Data type conversion completed: {converted_count} columns converted to double.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values analysis completed: 87,868 total missing values in 8 columns.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CELL 5: MISSING VALUES CHECK\n",
    "# ============================================================\n",
    "\n",
    "# Import functions for missing value analysis\n",
    "from pyspark.sql.functions import isnan, isnull\n",
    "\n",
    "# Analyze missing values\n",
    "missing_summary = []\n",
    "total_missing = 0\n",
    "\n",
    "for col_name in df.columns:\n",
    "    # Count null values\n",
    "    null_count = df.filter(col(col_name).isNull()).count()\n",
    "    \n",
    "    # Count NaN values (for numeric columns only)\n",
    "    nan_count = 0\n",
    "    if col_name in numeric_columns:\n",
    "        nan_count = df.filter(isnan(col(col_name))).count()\n",
    "    \n",
    "    # Total missing values\n",
    "    col_missing = null_count + nan_count\n",
    "    total_missing += col_missing\n",
    "    \n",
    "    if col_missing > 0:\n",
    "        missing_summary.append((col_name, col_missing, null_count, nan_count))\n",
    "\n",
    "print(f\"Missing values analysis completed: {total_missing:,} total missing values in {len(missing_summary)} columns.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values handling completed: 1,612 rows removed (0.66%)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CELL 6: MISSING VALUES HANDLING (EXCLUDING GEOCODED COLUMNS)\n",
    "# ============================================================\n",
    "\n",
    "# Define columns to exclude from missing value removal\n",
    "exclude_columns = ['Geocoded_City1', 'Geocoded_City2']\n",
    "\n",
    "# Create condition to check missing values (excluding excluded columns)\n",
    "missing_condition = None\n",
    "checked_columns = []\n",
    "\n",
    "for col_name in df.columns:\n",
    "    if col_name not in exclude_columns:\n",
    "        checked_columns.append(col_name)\n",
    "        if missing_condition is None:\n",
    "            missing_condition = col(col_name).isNull() | isnan(col(col_name))\n",
    "        else:\n",
    "            missing_condition = missing_condition | col(col_name).isNull() | isnan(col(col_name))\n",
    "\n",
    "# Remove rows with missing values in important columns\n",
    "df_clean = df.filter(~missing_condition)\n",
    "\n",
    "# Calculate statistics\n",
    "original_count = df.count()\n",
    "clean_count = df_clean.count()\n",
    "removed_count = original_count - clean_count\n",
    "removed_percentage = (removed_count / original_count) * 100\n",
    "\n",
    "print(f\"Missing values handling completed: {removed_count:,} rows removed ({removed_percentage:.2f}%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data verification completed: 244,343 rows, 23 columns\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CELL 7: DATA CLEANLINESS VERIFICATION\n",
    "# ============================================================\n",
    "\n",
    "# Check missing values in important columns\n",
    "important_columns = [col for col in df_clean.columns if col not in exclude_columns]\n",
    "all_clean = True\n",
    "\n",
    "for col_name in important_columns:\n",
    "    null_count = df_clean.filter(col(col_name).isNull()).count()\n",
    "    nan_count = df_clean.filter(isnan(col(col_name))).count()\n",
    "    total_missing = null_count + nan_count\n",
    "    \n",
    "    if total_missing > 0:\n",
    "        all_clean = False\n",
    "\n",
    "# Cache data for performance\n",
    "df_clean.cache()\n",
    "df_clean.count()  # Trigger caching\n",
    "\n",
    "print(f\"Data verification completed: {df_clean.count():,} rows, {len(df_clean.columns)} columns\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quarterly data preparation completed: 118 quarters aggregated.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CELL 8: PREPARE QUARTERLY DATA FOR MACHINE LEARNING\n",
    "# ============================================================\n",
    "\n",
    "# Import ML libraries\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import Window\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "from pyspark.ml.classification import LogisticRegression, RandomForestClassifier, GBTClassifier\n",
    "from pyspark.ml.regression import LinearRegression, RandomForestRegressor, GBTRegressor\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, RegressionEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml import Pipeline\n",
    "import time\n",
    "\n",
    "# Aggregate data by Year-Quarter\n",
    "df_quarterly = df_clean.groupBy('Year', 'quarter').agg(\n",
    "    # Volume metrics\n",
    "    count('*').alias('num_routes'),\n",
    "    sum('passengers').alias('total_passengers'),\n",
    "    avg('passengers').alias('avg_passengers_per_route'),\n",
    "\n",
    "    # Price metrics\n",
    "    avg('fare').alias('avg_fare'),\n",
    "    stddev('fare').alias('fare_std'),\n",
    "    min('fare').alias('fare_min'),\n",
    "    max('fare').alias('fare_max'),\n",
    "\n",
    "    # Distance metrics\n",
    "    avg('nsmiles').alias('avg_distance'),\n",
    "    stddev('nsmiles').alias('distance_std'),\n",
    "\n",
    "    # Competition metrics\n",
    "    countDistinct('carrier_lg').alias('num_carriers'),\n",
    "    avg('large_ms').alias('avg_market_share_large'),\n",
    "    avg('lf_ms').alias('avg_market_share_lowcost')\n",
    ").orderBy('Year', 'quarter')\n",
    "\n",
    "# Create time_period identifier\n",
    "df_quarterly = df_quarterly.withColumn('time_period',\n",
    "    concat(col('Year').cast('string'), lit('-Q'), col('quarter').cast('string'))\n",
    ")\n",
    "\n",
    "# Create labels (COVID = crisis)\n",
    "df_quarterly = df_quarterly.withColumn('is_crisis',\n",
    "    when((col('Year') >= 2020) & (col('Year') <= 2021), 1.0)\n",
    "    .otherwise(0.0)\n",
    ")\n",
    "\n",
    "print(f\"Quarterly data preparation completed: {df_quarterly.count()} quarters aggregated.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature engineering completed.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CELL 9: FEATURE ENGINEERING FOR QUARTERLY DATA\n",
    "# ============================================================\n",
    "\n",
    "# Window specs for time-based calculations\n",
    "window_qoq = Window.orderBy('Year', 'quarter')\n",
    "\n",
    "# Calculate QoQ (Quarter-over-Quarter) changes\n",
    "change_cols = ['num_routes', 'total_passengers', 'avg_fare', 'avg_distance']\n",
    "\n",
    "for col_name in change_cols:\n",
    "    # Get previous quarter value\n",
    "    df_quarterly = df_quarterly.withColumn(\n",
    "        f'{col_name}_prev_q',\n",
    "        lag(col(col_name), 1).over(window_qoq)\n",
    "    )\n",
    "\n",
    "    # Calculate % change\n",
    "    df_quarterly = df_quarterly.withColumn(\n",
    "        f'{col_name}_change_qoq',\n",
    "        when(col(f'{col_name}_prev_q').isNotNull() & (col(f'{col_name}_prev_q') != 0),\n",
    "             (col(col_name) - col(f'{col_name}_prev_q')) / col(f'{col_name}_prev_q'))\n",
    "        .otherwise(0.0)\n",
    "    )\n",
    "\n",
    "    # Drop temp column\n",
    "    df_quarterly = df_quarterly.drop(f'{col_name}_prev_q')\n",
    "\n",
    "# Calculate YoY (Year-over-Year) changes\n",
    "yoy_cols = ['num_routes', 'total_passengers', 'avg_fare']\n",
    "\n",
    "for col_name in yoy_cols:\n",
    "    df_quarterly = df_quarterly.withColumn(\n",
    "        f'{col_name}_prev_year',\n",
    "        lag(col(col_name), 4).over(window_qoq)\n",
    "    )\n",
    "\n",
    "    df_quarterly = df_quarterly.withColumn(\n",
    "        f'{col_name}_change_yoy',\n",
    "        when(col(f'{col_name}_prev_year').isNotNull() & (col(f'{col_name}_prev_year') != 0),\n",
    "             (col(col_name) - col(f'{col_name}_prev_year')) / col(f'{col_name}_prev_year'))\n",
    "        .otherwise(0.0)\n",
    "    )\n",
    "\n",
    "    df_quarterly = df_quarterly.drop(f'{col_name}_prev_year')\n",
    "\n",
    "# Create derived features\n",
    "# Volatility metrics\n",
    "df_quarterly = df_quarterly.withColumn('fare_volatility',\n",
    "    when(col('avg_fare') != 0, col('fare_std') / col('avg_fare')).otherwise(0.0)\n",
    ")\n",
    "\n",
    "df_quarterly = df_quarterly.withColumn('distance_volatility',\n",
    "    when(col('avg_distance') != 0, col('distance_std') / col('avg_distance')).otherwise(0.0)\n",
    ")\n",
    "\n",
    "# Range metrics\n",
    "df_quarterly = df_quarterly.withColumn('fare_range',\n",
    "    col('fare_max') - col('fare_min')\n",
    ")\n",
    "\n",
    "# Passenger efficiency\n",
    "df_quarterly = df_quarterly.withColumn('passenger_efficiency',\n",
    "    when(col('num_routes') != 0, col('total_passengers') / col('num_routes')).otherwise(0.0)\n",
    ")\n",
    "\n",
    "# Handle missing values\n",
    "df_quarterly = df_quarterly.fillna(0.0)\n",
    "\n",
    "print(\"Feature engineering completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quarterly data preparation completed: 20 features\n",
      "Training quarters: 105\n",
      "Test quarters: 13\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CELL 10: SELECT FEATURES AND PREPARE TRAINING DATA\n",
    "# ============================================================\n",
    "\n",
    "# Select final features for quarterly ML\n",
    "feature_cols_quarterly = [\n",
    "    # Core metrics (8)\n",
    "    'num_routes', 'total_passengers', 'avg_passengers_per_route',\n",
    "    'avg_fare', 'avg_distance', 'fare_volatility', 'fare_range',\n",
    "    'passenger_efficiency',\n",
    "    \n",
    "    # Market metrics (3)\n",
    "    'num_carriers', 'avg_market_share_large', 'avg_market_share_lowcost',\n",
    "    \n",
    "    # QoQ changes (4)\n",
    "    'num_routes_change_qoq', 'total_passengers_change_qoq', \n",
    "    'avg_fare_change_qoq', 'avg_distance_change_qoq',\n",
    "    \n",
    "    # YoY changes (3)\n",
    "    'num_routes_change_yoy', 'total_passengers_change_yoy', \n",
    "    'avg_fare_change_yoy',\n",
    "    \n",
    "    # Time features (2)\n",
    "    'Year', 'quarter'\n",
    "]\n",
    "\n",
    "# Vector Assembler for quarterly features\n",
    "assembler_quarterly = VectorAssembler(\n",
    "    inputCols=feature_cols_quarterly,\n",
    "    outputCol=\"features\"\n",
    ")\n",
    "\n",
    "# Standard Scaler for quarterly features\n",
    "scaler_quarterly = StandardScaler(\n",
    "    inputCol=\"features\",\n",
    "    outputCol=\"scaled_features\",\n",
    "    withStd=True,\n",
    "    withMean=True\n",
    ")\n",
    "\n",
    "# Temporal split for quarterly data\n",
    "train_data_quarterly = df_quarterly.filter(col('Year') <= 2020)  # Pre-COVID + 2020\n",
    "test_data_quarterly = df_quarterly.filter(col('Year') > 2020)    # 2021+ (Post-COVID)\n",
    "\n",
    "# Cache data for performance\n",
    "train_data_quarterly.cache()\n",
    "test_data_quarterly.cache()\n",
    "\n",
    "print(f\"Quarterly data preparation completed: {len(feature_cols_quarterly)} features\")\n",
    "print(f\"Training quarters: {train_data_quarterly.count():,}\")\n",
    "print(f\"Test quarters: {test_data_quarterly.count():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QUARTERLY DATASET OVERVIEW:\n",
      "  Total quarters: 118\n",
      "  Training quarters: 105\n",
      "  Test quarters: 13\n",
      "\n",
      "QUARTERLY CLASS DISTRIBUTION:\n",
      "  - Normal Quarters: 110 quarters (93.2%)\n",
      "  - COVID Crisis Quarters: 8 quarters (6.8%)\n",
      "\n",
      "Quarterly imbalance ratio: 13.8:1 (Normal:Crisis)\n",
      "SEVERE CLASS IMBALANCE (>10:1) - Quarterly Level\n",
      "\n",
      "Quarterly analysis completed.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CELL 11: CLASS IMBALANCE ANALYSIS FOR QUARTERLY DATA\n",
    "# ============================================================\n",
    "\n",
    "print(\"QUARTERLY DATASET OVERVIEW:\")\n",
    "print(f\"  Total quarters: {df_quarterly.count():,}\")\n",
    "print(f\"  Training quarters: {train_data_quarterly.count():,}\")\n",
    "print(f\"  Test quarters: {test_data_quarterly.count():,}\")\n",
    "\n",
    "# Class distribution analysis for quarterly data\n",
    "quarterly_distribution = df_quarterly.groupBy('is_crisis').count().collect()\n",
    "total_quarters = df_quarterly.count()\n",
    "\n",
    "print(\"\\nQUARTERLY CLASS DISTRIBUTION:\")\n",
    "for row in quarterly_distribution:\n",
    "    class_label = \"COVID Crisis Quarters\" if row['is_crisis'] == 1.0 else \"Normal Quarters\"\n",
    "    count = row['count']\n",
    "    percentage = (count / total_quarters) * 100\n",
    "    print(f\"  - {class_label}: {count} quarters ({percentage:.1f}%)\")\n",
    "\n",
    "# Calculate imbalance ratio for quarterly data\n",
    "crisis_quarters = next(row['count'] for row in quarterly_distribution if row['is_crisis'] == 1.0)\n",
    "normal_quarters = next(row['count'] for row in quarterly_distribution if row['is_crisis'] == 0.0)\n",
    "imbalance_ratio_quarterly = normal_quarters / crisis_quarters\n",
    "\n",
    "print(f\"\\nQuarterly imbalance ratio: {imbalance_ratio_quarterly:.1f}:1 (Normal:Crisis)\")\n",
    "\n",
    "if imbalance_ratio_quarterly > 10:\n",
    "    print(\"SEVERE CLASS IMBALANCE (>10:1) - Quarterly Level\")\n",
    "elif imbalance_ratio_quarterly > 5:\n",
    "    print(\"SIGNIFICANT CLASS IMBALANCE (>5:1) - Quarterly Level\")\n",
    "else:\n",
    "    print(\"Class balance is acceptable - Quarterly Level\")\n",
    "\n",
    "print(\"\\nQuarterly analysis completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quarterly class weights - Crisis: 7.38, Normal: 0.54\n",
      "Weighted quarterly dataset created: 105 quarters\n",
      "\n",
      "Quarterly class weight distribution:\n",
      "+---------+------------------+-----+\n",
      "|is_crisis|      class_weight|count|\n",
      "+---------+------------------+-----+\n",
      "|      0.0|0.5363636363636364|  101|\n",
      "|      1.0|             7.375|    4|\n",
      "+---------+------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CELL 12: CLASS WEIGHTING FOR QUARTERLY DATA\n",
    "# ============================================================\n",
    "\n",
    "from pyspark.sql.functions import when, col\n",
    "\n",
    "# Calculate class weights for quarterly data\n",
    "crisis_quarters_count = crisis_quarters\n",
    "normal_quarters_count = normal_quarters\n",
    "total_quarters_count = crisis_quarters_count + normal_quarters_count\n",
    "\n",
    "# Calculate weights (inverse frequency)\n",
    "weight_crisis_quarters = total_quarters_count / (2 * crisis_quarters_count)\n",
    "weight_normal_quarters = total_quarters_count / (2 * normal_quarters_count)\n",
    "\n",
    "print(f\"Quarterly class weights - Crisis: {weight_crisis_quarters:.2f}, Normal: {weight_normal_quarters:.2f}\")\n",
    "\n",
    "# Create weighted dataset for quarterly training\n",
    "train_data_quarterly_weighted = train_data_quarterly.withColumn(\n",
    "    \"class_weight\",\n",
    "    when(col(\"is_crisis\") == 1.0, weight_crisis_quarters).otherwise(weight_normal_quarters)\n",
    ")\n",
    "\n",
    "# Verify the weighting\n",
    "print(f\"Weighted quarterly dataset created: {train_data_quarterly_weighted.count():,} quarters\")\n",
    "\n",
    "# Check the class_weight distribution\n",
    "print(\"\\nQuarterly class weight distribution:\")\n",
    "train_data_quarterly_weighted.groupBy(\"is_crisis\", \"class_weight\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MODEL 1: LOGISTIC REGRESSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Logistic Regression (Quarterly, Class Weighted)\n",
      "AUC: 0.2778\n",
      "Accuracy: 0.7692\n",
      "Precision: 0.8269\n",
      "Recall: 0.7692\n",
      "F1-Score: 0.7165\n",
      "Training time: 2.60 seconds\n",
      "\n",
      "Quarterly Prediction Distribution:\n",
      "  Crisis Quarters: 1 quarters\n",
      "  Normal Quarters: 12 quarters\n",
      "\n",
      "Quarterly Confusion Matrix:\n",
      "  Crisis Quarters → Crisis Quarters: 1 quarters\n",
      "  Crisis Quarters → Normal Quarters: 3 quarters\n",
      "  Normal Quarters → Normal Quarters: 9 quarters\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CELL 13: MODEL 1 - LOGISTIC REGRESSION (QUARTERLY)\n",
    "# ============================================================\n",
    "\n",
    "# Create pipeline with class weighting for quarterly data\n",
    "logistic_reg_quarterly = LogisticRegression(\n",
    "    featuresCol=\"scaled_features\", \n",
    "    labelCol=\"is_crisis\",\n",
    "    weightCol=\"class_weight\",\n",
    "    maxIter=100,\n",
    "    regParam=0.01\n",
    ")\n",
    "logistic_pipeline_quarterly = Pipeline(stages=[assembler_quarterly, scaler_quarterly, logistic_reg_quarterly])\n",
    "\n",
    "# Train model with weighted quarterly data\n",
    "start_time = time.time()\n",
    "logistic_model_quarterly = logistic_pipeline_quarterly.fit(train_data_quarterly_weighted)\n",
    "logistic_time_quarterly = time.time() - start_time\n",
    "\n",
    "# Predictions on quarterly test data\n",
    "logistic_predictions_quarterly = logistic_model_quarterly.transform(test_data_quarterly)\n",
    "\n",
    "# Evaluate with comprehensive metrics\n",
    "auc_evaluator = BinaryClassificationEvaluator(labelCol=\"is_crisis\", metricName=\"areaUnderROC\")\n",
    "accuracy_evaluator = MulticlassClassificationEvaluator(labelCol=\"is_crisis\", metricName=\"accuracy\")\n",
    "precision_evaluator = MulticlassClassificationEvaluator(labelCol=\"is_crisis\", metricName=\"weightedPrecision\")\n",
    "recall_evaluator = MulticlassClassificationEvaluator(labelCol=\"is_crisis\", metricName=\"weightedRecall\")\n",
    "f1_evaluator = MulticlassClassificationEvaluator(labelCol=\"is_crisis\", metricName=\"f1\")\n",
    "\n",
    "auc = auc_evaluator.evaluate(logistic_predictions_quarterly)\n",
    "accuracy = accuracy_evaluator.evaluate(logistic_predictions_quarterly)\n",
    "precision = precision_evaluator.evaluate(logistic_predictions_quarterly)\n",
    "recall = recall_evaluator.evaluate(logistic_predictions_quarterly)\n",
    "f1 = f1_evaluator.evaluate(logistic_predictions_quarterly)\n",
    "\n",
    "# Results\n",
    "print(\"Model: Logistic Regression (Quarterly, Class Weighted)\")\n",
    "print(f\"AUC: {auc:.4f}\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1-Score: {f1:.4f}\")\n",
    "print(f\"Training time: {logistic_time_quarterly:.2f} seconds\")\n",
    "\n",
    "# Check prediction distribution\n",
    "print(\"\\nQuarterly Prediction Distribution:\")\n",
    "pred_dist = logistic_predictions_quarterly.groupBy(\"prediction\").count().collect()\n",
    "for row in pred_dist:\n",
    "    class_name = \"Crisis Quarters\" if row['prediction'] == 1.0 else \"Normal Quarters\"\n",
    "    print(f\"  {class_name}: {row['count']:,} quarters\")\n",
    "\n",
    "# Confusion Matrix\n",
    "print(\"\\nQuarterly Confusion Matrix:\")\n",
    "confusion_matrix = logistic_predictions_quarterly.groupBy(\"is_crisis\", \"prediction\").count().collect()\n",
    "for row in confusion_matrix:\n",
    "    actual = \"Crisis Quarters\" if row['is_crisis'] == 1.0 else \"Normal Quarters\"\n",
    "    predicted = \"Crisis Quarters\" if row['prediction'] == 1.0 else \"Normal Quarters\"\n",
    "    print(f\"  {actual} → {predicted}: {row['count']:,} quarters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MODEL 2: DECISION TREE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Decision Tree (Quarterly, Class Weighted)\n",
      "AUC: 0.6250\n",
      "Accuracy: 0.7692\n",
      "Precision: 0.8269\n",
      "Recall: 0.7692\n",
      "F1-Score: 0.7165\n",
      "Training time: 0.71 seconds\n",
      "\n",
      "Quarterly Prediction Distribution:\n",
      "  Crisis Quarters: 1 quarters\n",
      "  Normal Quarters: 12 quarters\n",
      "\n",
      "Quarterly Confusion Matrix:\n",
      "  Crisis Quarters → Crisis Quarters: 1 quarters\n",
      "  Crisis Quarters → Normal Quarters: 3 quarters\n",
      "  Normal Quarters → Normal Quarters: 9 quarters\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CELL 14: MODEL 2 - DECISION TREE (QUARTERLY)\n",
    "# ============================================================\n",
    "\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "\n",
    "# Create pipeline with Decision Tree for quarterly data\n",
    "dt_classifier_quarterly = DecisionTreeClassifier(\n",
    "    featuresCol=\"features\", \n",
    "    labelCol=\"is_crisis\",\n",
    "    weightCol=\"class_weight\",\n",
    "    maxDepth=15,\n",
    "    maxBins=64,\n",
    "    minInstancesPerNode=5\n",
    ")\n",
    "dt_pipeline_quarterly = Pipeline(stages=[assembler_quarterly, dt_classifier_quarterly])\n",
    "\n",
    "# Train model with weighted quarterly data\n",
    "start_time = time.time()\n",
    "dt_model_quarterly = dt_pipeline_quarterly.fit(train_data_quarterly_weighted)\n",
    "dt_time_quarterly = time.time() - start_time\n",
    "\n",
    "# Predictions on quarterly test data\n",
    "dt_predictions_quarterly = dt_model_quarterly.transform(test_data_quarterly)\n",
    "\n",
    "# Evaluate with comprehensive metrics\n",
    "auc_evaluator = BinaryClassificationEvaluator(labelCol=\"is_crisis\", metricName=\"areaUnderROC\")\n",
    "accuracy_evaluator = MulticlassClassificationEvaluator(labelCol=\"is_crisis\", metricName=\"accuracy\")\n",
    "precision_evaluator = MulticlassClassificationEvaluator(labelCol=\"is_crisis\", metricName=\"weightedPrecision\")\n",
    "recall_evaluator = MulticlassClassificationEvaluator(labelCol=\"is_crisis\", metricName=\"weightedRecall\")\n",
    "f1_evaluator = MulticlassClassificationEvaluator(labelCol=\"is_crisis\", metricName=\"f1\")\n",
    "\n",
    "auc = auc_evaluator.evaluate(dt_predictions_quarterly)\n",
    "accuracy = accuracy_evaluator.evaluate(dt_predictions_quarterly)\n",
    "precision = precision_evaluator.evaluate(dt_predictions_quarterly)\n",
    "recall = recall_evaluator.evaluate(dt_predictions_quarterly)\n",
    "f1 = f1_evaluator.evaluate(dt_predictions_quarterly)\n",
    "\n",
    "# Results\n",
    "print(\"Model: Decision Tree (Quarterly, Class Weighted)\")\n",
    "print(f\"AUC: {auc:.4f}\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1-Score: {f1:.4f}\")\n",
    "print(f\"Training time: {dt_time_quarterly:.2f} seconds\")\n",
    "\n",
    "# Check prediction distribution\n",
    "print(\"\\nQuarterly Prediction Distribution:\")\n",
    "pred_dist = dt_predictions_quarterly.groupBy(\"prediction\").count().collect()\n",
    "for row in pred_dist:\n",
    "    class_name = \"Crisis Quarters\" if row['prediction'] == 1.0 else \"Normal Quarters\"\n",
    "    print(f\"  {class_name}: {row['count']:,} quarters\")\n",
    "\n",
    "# Confusion Matrix\n",
    "print(\"\\nQuarterly Confusion Matrix:\")\n",
    "confusion_matrix = dt_predictions_quarterly.groupBy(\"is_crisis\", \"prediction\").count().collect()\n",
    "for row in confusion_matrix:\n",
    "    actual = \"Crisis Quarters\" if row['is_crisis'] == 1.0 else \"Normal Quarters\"\n",
    "    predicted = \"Crisis Quarters\" if row['prediction'] == 1.0 else \"Normal Quarters\"\n",
    "    print(f\"  {actual} → {predicted}: {row['count']:,} quarters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MODEL 3: RANDOM FOREST\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Random Forest (Quarterly, Class Weighted)\n",
      "AUC: 0.7500\n",
      "Accuracy: 0.7692\n",
      "Precision: 0.8269\n",
      "Recall: 0.7692\n",
      "F1-Score: 0.7165\n",
      "Training time: 0.92 seconds\n",
      "\n",
      "Quarterly Prediction Distribution:\n",
      "  Crisis Quarters: 1 quarters\n",
      "  Normal Quarters: 12 quarters\n",
      "\n",
      "Quarterly Confusion Matrix:\n",
      "  Crisis Quarters → Crisis Quarters: 1 quarters\n",
      "  Crisis Quarters → Normal Quarters: 3 quarters\n",
      "  Normal Quarters → Normal Quarters: 9 quarters\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CELL 15: MODEL 3 - RANDOM FOREST (QUARTERLY)\n",
    "# ============================================================\n",
    "\n",
    "# Create pipeline with Random Forest for quarterly data\n",
    "rf_classifier_quarterly = RandomForestClassifier(\n",
    "    featuresCol=\"features\", \n",
    "    labelCol=\"is_crisis\",\n",
    "    weightCol=\"class_weight\",\n",
    "    numTrees=200,\n",
    "    maxDepth=15,\n",
    "    maxBins=64,\n",
    "    minInstancesPerNode=5,\n",
    "    seed=42\n",
    ")\n",
    "rf_pipeline_quarterly = Pipeline(stages=[assembler_quarterly, rf_classifier_quarterly])\n",
    "\n",
    "# Train model with weighted quarterly data\n",
    "start_time = time.time()\n",
    "rf_model_quarterly = rf_pipeline_quarterly.fit(train_data_quarterly_weighted)\n",
    "rf_time_quarterly = time.time() - start_time\n",
    "\n",
    "# Predictions on quarterly test data\n",
    "rf_predictions_quarterly = rf_model_quarterly.transform(test_data_quarterly)\n",
    "\n",
    "# Evaluate with comprehensive metrics\n",
    "auc_evaluator = BinaryClassificationEvaluator(labelCol=\"is_crisis\", metricName=\"areaUnderROC\")\n",
    "accuracy_evaluator = MulticlassClassificationEvaluator(labelCol=\"is_crisis\", metricName=\"accuracy\")\n",
    "precision_evaluator = MulticlassClassificationEvaluator(labelCol=\"is_crisis\", metricName=\"weightedPrecision\")\n",
    "recall_evaluator = MulticlassClassificationEvaluator(labelCol=\"is_crisis\", metricName=\"weightedRecall\")\n",
    "f1_evaluator = MulticlassClassificationEvaluator(labelCol=\"is_crisis\", metricName=\"f1\")\n",
    "\n",
    "auc = auc_evaluator.evaluate(rf_predictions_quarterly)\n",
    "accuracy = accuracy_evaluator.evaluate(rf_predictions_quarterly)\n",
    "precision = precision_evaluator.evaluate(rf_predictions_quarterly)\n",
    "recall = recall_evaluator.evaluate(rf_predictions_quarterly)\n",
    "f1 = f1_evaluator.evaluate(rf_predictions_quarterly)\n",
    "\n",
    "# Results\n",
    "print(\"Model: Random Forest (Quarterly, Class Weighted)\")\n",
    "print(f\"AUC: {auc:.4f}\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1-Score: {f1:.4f}\")\n",
    "print(f\"Training time: {rf_time_quarterly:.2f} seconds\")\n",
    "\n",
    "# Check prediction distribution\n",
    "print(\"\\nQuarterly Prediction Distribution:\")\n",
    "pred_dist = rf_predictions_quarterly.groupBy(\"prediction\").count().collect()\n",
    "for row in pred_dist:\n",
    "    class_name = \"Crisis Quarters\" if row['prediction'] == 1.0 else \"Normal Quarters\"\n",
    "    print(f\"  {class_name}: {row['count']:,} quarters\")\n",
    "\n",
    "# Confusion Matrix\n",
    "print(\"\\nQuarterly Confusion Matrix:\")\n",
    "confusion_matrix = rf_predictions_quarterly.groupBy(\"is_crisis\", \"prediction\").count().collect()\n",
    "for row in confusion_matrix:\n",
    "    actual = \"Crisis Quarters\" if row['is_crisis'] == 1.0 else \"Normal Quarters\"\n",
    "    predicted = \"Crisis Quarters\" if row['prediction'] == 1.0 else \"Normal Quarters\"\n",
    "    print(f\"  {actual} → {predicted}: {row['count']:,} quarters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MODEL 4: GRADIENT BOOSTING TREES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Gradient Boosting Trees (Quarterly, Class Weighted)\n",
      "AUC: 0.6944\n",
      "Accuracy: 0.7692\n",
      "Precision: 0.8269\n",
      "Recall: 0.7692\n",
      "F1-Score: 0.7165\n",
      "Training time: 29.63 seconds\n",
      "\n",
      "Quarterly Prediction Distribution:\n",
      "  Crisis Quarters: 1 quarters\n",
      "  Normal Quarters: 12 quarters\n",
      "\n",
      "Quarterly Confusion Matrix:\n",
      "  Crisis Quarters → Crisis Quarters: 1 quarters\n",
      "  Crisis Quarters → Normal Quarters: 3 quarters\n",
      "  Normal Quarters → Normal Quarters: 9 quarters\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CELL 16: MODEL 4 - GRADIENT BOOSTING TREES (QUARTERLY)\n",
    "# ============================================================\n",
    "\n",
    "# Create pipeline with Gradient Boosting for quarterly data\n",
    "gbt_classifier_quarterly = GBTClassifier(\n",
    "    featuresCol=\"features\", \n",
    "    labelCol=\"is_crisis\",\n",
    "    weightCol=\"class_weight\",\n",
    "    maxIter=100,\n",
    "    maxDepth=8,\n",
    "    maxBins=64,\n",
    "    minInstancesPerNode=5,\n",
    "    seed=42\n",
    ")\n",
    "gbt_pipeline_quarterly = Pipeline(stages=[assembler_quarterly, gbt_classifier_quarterly])\n",
    "\n",
    "# Train model with weighted quarterly data\n",
    "start_time = time.time()\n",
    "gbt_model_quarterly = gbt_pipeline_quarterly.fit(train_data_quarterly_weighted)\n",
    "gbt_time_quarterly = time.time() - start_time\n",
    "\n",
    "# Predictions on quarterly test data\n",
    "gbt_predictions_quarterly = gbt_model_quarterly.transform(test_data_quarterly)\n",
    "\n",
    "# Evaluate with comprehensive metrics\n",
    "auc_evaluator = BinaryClassificationEvaluator(labelCol=\"is_crisis\", metricName=\"areaUnderROC\")\n",
    "accuracy_evaluator = MulticlassClassificationEvaluator(labelCol=\"is_crisis\", metricName=\"accuracy\")\n",
    "precision_evaluator = MulticlassClassificationEvaluator(labelCol=\"is_crisis\", metricName=\"weightedPrecision\")\n",
    "recall_evaluator = MulticlassClassificationEvaluator(labelCol=\"is_crisis\", metricName=\"weightedRecall\")\n",
    "f1_evaluator = MulticlassClassificationEvaluator(labelCol=\"is_crisis\", metricName=\"f1\")\n",
    "\n",
    "auc = auc_evaluator.evaluate(gbt_predictions_quarterly)\n",
    "accuracy = accuracy_evaluator.evaluate(gbt_predictions_quarterly)\n",
    "precision = precision_evaluator.evaluate(gbt_predictions_quarterly)\n",
    "recall = recall_evaluator.evaluate(gbt_predictions_quarterly)\n",
    "f1 = f1_evaluator.evaluate(gbt_predictions_quarterly)\n",
    "\n",
    "# Results\n",
    "print(\"Model: Gradient Boosting Trees (Quarterly, Class Weighted)\")\n",
    "print(f\"AUC: {auc:.4f}\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1-Score: {f1:.4f}\")\n",
    "print(f\"Training time: {gbt_time_quarterly:.2f} seconds\")\n",
    "\n",
    "# Check prediction distribution\n",
    "print(\"\\nQuarterly Prediction Distribution:\")\n",
    "pred_dist = gbt_predictions_quarterly.groupBy(\"prediction\").count().collect()\n",
    "for row in pred_dist:\n",
    "    class_name = \"Crisis Quarters\" if row['prediction'] == 1.0 else \"Normal Quarters\"\n",
    "    print(f\"  {class_name}: {row['count']:,} quarters\")\n",
    "\n",
    "# Confusion Matrix\n",
    "print(\"\\nQuarterly Confusion Matrix:\")\n",
    "confusion_matrix = gbt_predictions_quarterly.groupBy(\"is_crisis\", \"prediction\").count().collect()\n",
    "for row in confusion_matrix:\n",
    "    actual = \"Crisis Quarters\" if row['is_crisis'] == 1.0 else \"Normal Quarters\"\n",
    "    predicted = \"Crisis Quarters\" if row['prediction'] == 1.0 else \"Normal Quarters\"\n",
    "    print(f\"  {actual} → {predicted}: {row['count']:,} quarters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MODEL 5: NAIVE BAYES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Naive Bayes (Quarterly, Class Weighted)\n",
      "AUC: 0.7500\n",
      "Accuracy: 0.7692\n",
      "Precision: 0.8269\n",
      "Recall: 0.7692\n",
      "F1-Score: 0.7165\n",
      "Training time: 0.28 seconds\n",
      "\n",
      "Quarterly Prediction Distribution:\n",
      "  Crisis Quarters: 1 quarters\n",
      "  Normal Quarters: 12 quarters\n",
      "\n",
      "Quarterly Confusion Matrix:\n",
      "  Crisis Quarters → Crisis Quarters: 1 quarters\n",
      "  Crisis Quarters → Normal Quarters: 3 quarters\n",
      "  Normal Quarters → Normal Quarters: 9 quarters\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CELL 17: MODEL 5 - NAIVE BAYES (QUARTERLY)\n",
    "# ============================================================\n",
    "\n",
    "from pyspark.ml.classification import NaiveBayes\n",
    "\n",
    "# Select features suitable for Naive Bayes (positive values only)\n",
    "nb_feature_cols_quarterly = [\n",
    "    'num_routes', 'total_passengers', 'avg_passengers_per_route',\n",
    "    'avg_fare', 'avg_distance', 'fare_volatility', 'fare_range',\n",
    "    'passenger_efficiency', 'num_carriers', 'avg_market_share_large',\n",
    "    'Year', 'quarter'\n",
    "]\n",
    "\n",
    "# Vector Assembler for Naive Bayes features\n",
    "assembler_nb_quarterly = VectorAssembler(\n",
    "    inputCols=nb_feature_cols_quarterly,\n",
    "    outputCol=\"features\"\n",
    ")\n",
    "\n",
    "# Create pipeline with Naive Bayes for quarterly data\n",
    "nb_classifier_quarterly = NaiveBayes(\n",
    "    featuresCol=\"features\", \n",
    "    labelCol=\"is_crisis\",\n",
    "    weightCol=\"class_weight\",\n",
    "    smoothing=1.0\n",
    ")\n",
    "nb_pipeline_quarterly = Pipeline(stages=[assembler_nb_quarterly, nb_classifier_quarterly])\n",
    "\n",
    "# Train model with weighted quarterly data\n",
    "start_time = time.time()\n",
    "nb_model_quarterly = nb_pipeline_quarterly.fit(train_data_quarterly_weighted)\n",
    "nb_time_quarterly = time.time() - start_time\n",
    "\n",
    "# Predictions on quarterly test data\n",
    "nb_predictions_quarterly = nb_model_quarterly.transform(test_data_quarterly)\n",
    "\n",
    "# Evaluate with comprehensive metrics\n",
    "auc_evaluator = BinaryClassificationEvaluator(labelCol=\"is_crisis\", metricName=\"areaUnderROC\")\n",
    "accuracy_evaluator = MulticlassClassificationEvaluator(labelCol=\"is_crisis\", metricName=\"accuracy\")\n",
    "precision_evaluator = MulticlassClassificationEvaluator(labelCol=\"is_crisis\", metricName=\"weightedPrecision\")\n",
    "recall_evaluator = MulticlassClassificationEvaluator(labelCol=\"is_crisis\", metricName=\"weightedRecall\")\n",
    "f1_evaluator = MulticlassClassificationEvaluator(labelCol=\"is_crisis\", metricName=\"f1\")\n",
    "\n",
    "auc = auc_evaluator.evaluate(nb_predictions_quarterly)\n",
    "accuracy = accuracy_evaluator.evaluate(nb_predictions_quarterly)\n",
    "precision = precision_evaluator.evaluate(nb_predictions_quarterly)\n",
    "recall = recall_evaluator.evaluate(nb_predictions_quarterly)\n",
    "f1 = f1_evaluator.evaluate(nb_predictions_quarterly)\n",
    "\n",
    "# Results\n",
    "print(\"Model: Naive Bayes (Quarterly, Class Weighted)\")\n",
    "print(f\"AUC: {auc:.4f}\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1-Score: {f1:.4f}\")\n",
    "print(f\"Training time: {nb_time_quarterly:.2f} seconds\")\n",
    "\n",
    "# Check prediction distribution\n",
    "print(\"\\nQuarterly Prediction Distribution:\")\n",
    "pred_dist = nb_predictions_quarterly.groupBy(\"prediction\").count().collect()\n",
    "for row in pred_dist:\n",
    "    class_name = \"Crisis Quarters\" if row['prediction'] == 1.0 else \"Normal Quarters\"\n",
    "    print(f\"  {class_name}: {row['count']:,} quarters\")\n",
    "\n",
    "# Confusion Matrix\n",
    "print(\"\\nQuarterly Confusion Matrix:\")\n",
    "confusion_matrix = nb_predictions_quarterly.groupBy(\"is_crisis\", \"prediction\").count().collect()\n",
    "for row in confusion_matrix:\n",
    "    actual = \"Crisis Quarters\" if row['is_crisis'] == 1.0 else \"Normal Quarters\"\n",
    "    predicted = \"Crisis Quarters\" if row['prediction'] == 1.0 else \"Normal Quarters\"\n",
    "    print(f\"  {actual} → {predicted}: {row['count']:,} quarters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MODEL 6: SUPPORT VECTOR MACHINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 18: MODEL 6 - SUPPORT VECTOR MACHINE (QUARTERLY)\n",
    "# ============================================================\n",
    "\n",
    "from pyspark.ml.classification import LinearSVC\n",
    "\n",
    "# Create pipeline with SVM for quarterly data\n",
    "svm_classifier_quarterly = LinearSVC(\n",
    "    featuresCol=\"scaled_features\", \n",
    "    labelCol=\"is_crisis\",\n",
    "    weightCol=\"class_weight\",\n",
    "    maxIter=200,\n",
    "    regParam=0.01,\n",
    "    threshold=0.5\n",
    ")\n",
    "svm_pipeline_quarterly = Pipeline(stages=[assembler_quarterly, scaler_quarterly, svm_classifier_quarterly])\n",
    "\n",
    "# Train model with weighted quarterly data\n",
    "start_time = time.time()\n",
    "svm_model_quarterly = svm_pipeline_quarterly.fit(train_data_quarterly_weighted)\n",
    "svm_time_quarterly = time.time() - start_time\n",
    "\n",
    "# Predictions on quarterly test data\n",
    "svm_predictions_quarterly = svm_model_quarterly.transform(test_data_quarterly)\n",
    "\n",
    "# Evaluate with comprehensive metrics\n",
    "auc_evaluator = BinaryClassificationEvaluator(labelCol=\"is_crisis\", metricName=\"areaUnderROC\")\n",
    "accuracy_evaluator = MulticlassClassificationEvaluator(labelCol=\"is_crisis\", metricName=\"accuracy\")\n",
    "precision_evaluator = MulticlassClassificationEvaluator(labelCol=\"is_crisis\", metricName=\"weightedPrecision\")\n",
    "recall_evaluator = MulticlassClassificationEvaluator(labelCol=\"is_crisis\", metricName=\"weightedRecall\")\n",
    "f1_evaluator = MulticlassClassificationEvaluator(labelCol=\"is_crisis\", metricName=\"f1\")\n",
    "\n",
    "auc = auc_evaluator.evaluate(svm_predictions_quarterly)\n",
    "accuracy = accuracy_evaluator.evaluate(svm_predictions_quarterly)\n",
    "precision = precision_evaluator.evaluate(svm_predictions_quarterly)\n",
    "recall = recall_evaluator.evaluate(svm_predictions_quarterly)\n",
    "f1 = f1_evaluator.evaluate(svm_predictions_quarterly)\n",
    "\n",
    "# Results\n",
    "print(\"Model: Support Vector Machine (Quarterly, Class Weighted)\")\n",
    "print(f\"AUC: {auc:.4f}\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1-Score: {f1:.4f}\")\n",
    "print(f\"Training time: {svm_time_quarterly:.2f} seconds\")\n",
    "\n",
    "# Check prediction distribution\n",
    "print(\"\\nQuarterly Prediction Distribution:\")\n",
    "pred_dist = svm_predictions_quarterly.groupBy(\"prediction\").count().collect()\n",
    "for row in pred_dist:\n",
    "    class_name = \"Crisis Quarters\" if row['prediction'] == 1.0 else \"Normal Quarters\"\n",
    "    print(f\"  {class_name}: {row['count']:,} quarters\")\n",
    "\n",
    "# Confusion Matrix\n",
    "print(\"\\nQuarterly Confusion Matrix:\")\n",
    "confusion_matrix = svm_predictions_quarterly.groupBy(\"is_crisis\", \"prediction\").count().collect()\n",
    "for row in confusion_matrix:\n",
    "    actual = \"Crisis Quarters\" if row['is_crisis'] == 1.0 else \"Normal Quarters\"\n",
    "    predicted = \"Crisis Quarters\" if row['prediction'] == 1.0 else \"Normal Quarters\"\n",
    "    print(f\"  {actual} → {predicted}: {row['count']:,} quarters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 19: CONFUSION MATRIX VISUALIZATION\n",
    "# ============================================================\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, auc\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Set up the plotting style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Function to create confusion matrix plot\n",
    "def plot_confusion_matrix(y_true, y_pred, model_name, ax):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax,\n",
    "                xticklabels=['Normal', 'Crisis'], \n",
    "                yticklabels=['Normal', 'Crisis'])\n",
    "    ax.set_title(f'{model_name}\\nConfusion Matrix')\n",
    "    ax.set_xlabel('Predicted')\n",
    "    ax.set_ylabel('Actual')\n",
    "\n",
    "# Function to get predictions and true labels from Spark DataFrame\n",
    "def get_predictions_data(predictions_df):\n",
    "    # Convert Spark DataFrame to Pandas for plotting\n",
    "    predictions_pd = predictions_df.select('is_crisis', 'prediction').toPandas()\n",
    "    return predictions_pd['is_crisis'].values, predictions_pd['prediction'].values\n",
    "\n",
    "# Get predictions for all models\n",
    "models_data = {\n",
    "    'Logistic Regression': (logistic_predictions_quarterly, 'is_crisis', 'prediction'),\n",
    "    'Decision Tree': (dt_predictions_quarterly, 'is_crisis', 'prediction'),\n",
    "    'Random Forest': (rf_predictions_quarterly, 'is_crisis', 'prediction'),\n",
    "    'Gradient Boosting': (gbt_predictions_quarterly, 'is_crisis', 'prediction'),\n",
    "    'Naive Bayes': (nb_predictions_quarterly, 'is_crisis', 'prediction'),\n",
    "    'Support Vector Machine': (svm_predictions_quarterly, 'is_crisis', 'prediction')\n",
    "}\n",
    "\n",
    "# Create confusion matrix plots\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "fig.suptitle('Confusion Matrices for All Models (Quarterly Data)', fontsize=16, fontweight='bold')\n",
    "\n",
    "for i, (model_name, (predictions_df, true_col, pred_col)) in enumerate(models_data.items()):\n",
    "    row = i // 3\n",
    "    col = i % 3\n",
    "    \n",
    "    # Get true and predicted values\n",
    "    y_true, y_pred = get_predictions_data(predictions_df)\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    plot_confusion_matrix(y_true, y_pred, model_name, axes[row, col])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Confusion matrices created successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 20: ROC CURVES AND AUC VISUALIZATION\n",
    "# ============================================================\n",
    "\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Function to get probability scores for ROC curve\n",
    "def get_probability_scores(predictions_df):\n",
    "    # Convert Spark DataFrame to Pandas\n",
    "    predictions_pd = predictions_df.select('is_crisis', 'probability').toPandas()\n",
    "    \n",
    "    # Extract probability scores for positive class (crisis)\n",
    "    y_true = predictions_pd['is_crisis'].values\n",
    "    y_scores = predictions_pd['probability'].apply(lambda x: x[1]).values  # Probability of crisis class\n",
    "    \n",
    "    return y_true, y_scores\n",
    "\n",
    "# Function to plot ROC curve\n",
    "def plot_roc_curve(y_true, y_scores, model_name, ax, color):\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_scores)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    ax.plot(fpr, tpr, color=color, lw=2, \n",
    "            label=f'{model_name} (AUC = {roc_auc:.3f})')\n",
    "    return roc_auc\n",
    "\n",
    "# Create ROC curves plot\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 8))\n",
    "\n",
    "# Colors for different models\n",
    "colors = ['blue', 'red', 'green', 'orange', 'purple', 'brown']\n",
    "\n",
    "# Plot ROC curves for all models\n",
    "auc_scores = {}\n",
    "for i, (model_name, predictions_df) in enumerate([\n",
    "    ('Logistic Regression', logistic_predictions_quarterly),\n",
    "    ('Decision Tree', dt_predictions_quarterly),\n",
    "    ('Random Forest', rf_predictions_quarterly),\n",
    "    ('Gradient Boosting', gbt_predictions_quarterly),\n",
    "    ('Naive Bayes', nb_predictions_quarterly),\n",
    "    ('Support Vector Machine', svm_predictions_quarterly)\n",
    "]):\n",
    "    try:\n",
    "        y_true, y_scores = get_probability_scores(predictions_df)\n",
    "        roc_auc = plot_roc_curve(y_true, y_scores, model_name, ax, colors[i])\n",
    "        auc_scores[model_name] = roc_auc\n",
    "    except Exception as e:\n",
    "        print(f\"Error plotting {model_name}: {e}\")\n",
    "        # For models without probability scores, use prediction as score\n",
    "        y_true, y_pred = get_predictions_data(predictions_df)\n",
    "        y_scores = y_pred.astype(float)  # Use predictions as scores\n",
    "        roc_auc = plot_roc_curve(y_true, y_scores, model_name, ax, colors[i])\n",
    "        auc_scores[model_name] = roc_auc\n",
    "\n",
    "# Plot diagonal line (random classifier)\n",
    "ax.plot([0, 1], [0, 1], 'k--', lw=2, label='Random Classifier (AUC = 0.500)')\n",
    "\n",
    "# Customize the plot\n",
    "ax.set_xlim([0.0, 1.0])\n",
    "ax.set_ylim([0.0, 1.05])\n",
    "ax.set_xlabel('False Positive Rate (1 - Specificity)', fontsize=12)\n",
    "ax.set_ylabel('True Positive Rate (Sensitivity)', fontsize=12)\n",
    "ax.set_title('ROC Curves Comparison - Aviation Crisis Prediction Models', fontsize=14, fontweight='bold')\n",
    "ax.legend(loc=\"lower right\", fontsize=10)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print AUC scores\n",
    "print(\"\\nAUC Scores Summary:\")\n",
    "print(\"=\" * 50)\n",
    "for model_name, auc_score in sorted(auc_scores.items(), key=lambda x: x[1], reverse=True):\n",
    "    print(f\"{model_name:25s}: {auc_score:.4f}\")\n",
    "\n",
    "print(\"\\nROC curves created successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 21: COMPREHENSIVE MODEL PERFORMANCE COMPARISON\n",
    "# ============================================================\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Function to calculate all metrics\n",
    "def calculate_metrics(y_true, y_pred):\n",
    "    from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "    \n",
    "    # Calculate confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred, zero_division=0)\n",
    "    recall = recall_score(y_true, y_pred, zero_division=0)\n",
    "    f1 = f1_score(y_true, y_pred, zero_division=0)\n",
    "    \n",
    "    # Calculate specificity\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "    \n",
    "    return {\n",
    "        'Accuracy': accuracy,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1-Score': f1,\n",
    "        'Specificity': specificity,\n",
    "        'True Positives': tp,\n",
    "        'True Negatives': tn,\n",
    "        'False Positives': fp,\n",
    "        'False Negatives': fn\n",
    "    }\n",
    "\n",
    "# Collect metrics for all models\n",
    "all_metrics = {}\n",
    "\n",
    "for model_name, predictions_df in [\n",
    "    ('Logistic Regression', logistic_predictions_quarterly),\n",
    "    ('Decision Tree', dt_predictions_quarterly),\n",
    "    ('Random Forest', rf_predictions_quarterly),\n",
    "    ('Gradient Boosting', gbt_predictions_quarterly),\n",
    "    ('Naive Bayes', nb_predictions_quarterly),\n",
    "    ('Support Vector Machine', svm_predictions_quarterly)\n",
    "]:\n",
    "    y_true, y_pred = get_predictions_data(predictions_df)\n",
    "    metrics = calculate_metrics(y_true, y_pred)\n",
    "    all_metrics[model_name] = metrics\n",
    "\n",
    "# Create DataFrame for comparison\n",
    "metrics_df = pd.DataFrame(all_metrics).T\n",
    "\n",
    "# Display the comprehensive metrics table\n",
    "print(\"COMPREHENSIVE MODEL PERFORMANCE COMPARISON\")\n",
    "print(\"=\" * 80)\n",
    "print(metrics_df.round(4))\n",
    "\n",
    "# Create performance comparison charts\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "fig.suptitle('Model Performance Comparison - Aviation Crisis Prediction', fontsize=16, fontweight='bold')\n",
    "\n",
    "# 1. Main Metrics Comparison\n",
    "main_metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score']\n",
    "metrics_df[main_metrics].plot(kind='bar', ax=axes[0,0], width=0.8)\n",
    "axes[0,0].set_title('Main Performance Metrics', fontweight='bold')\n",
    "axes[0,0].set_ylabel('Score')\n",
    "axes[0,0].set_xlabel('Models')\n",
    "axes[0,0].legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "axes[0,0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# 2. Confusion Matrix Elements\n",
    "confusion_metrics = ['True Positives', 'True Negatives', 'False Positives', 'False Negatives']\n",
    "metrics_df[confusion_metrics].plot(kind='bar', ax=axes[0,1], width=0.8)\n",
    "axes[0,1].set_title('Confusion Matrix Elements', fontweight='bold')\n",
    "axes[0,1].set_ylabel('Count')\n",
    "axes[0,1].set_xlabel('Models')\n",
    "axes[0,1].legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "axes[0,1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# 3. Precision vs Recall Scatter\n",
    "axes[1,0].scatter(metrics_df['Precision'], metrics_df['Recall'], \n",
    "                 s=100, alpha=0.7, c=range(len(metrics_df)), cmap='viridis')\n",
    "for i, model in enumerate(metrics_df.index):\n",
    "    axes[1,0].annotate(model, (metrics_df['Precision'][i], metrics_df['Recall'][i]), \n",
    "                      xytext=(5, 5), textcoords='offset points', fontsize=8)\n",
    "axes[1,0].set_xlabel('Precision')\n",
    "axes[1,0].set_ylabel('Recall')\n",
    "axes[1,0].set_title('Precision vs Recall Trade-off', fontweight='bold')\n",
    "axes[1,0].grid(True, alpha=0.3)\n",
    "\n",
    "# 4. F1-Score Ranking\n",
    "f1_scores = metrics_df['F1-Score'].sort_values(ascending=True)\n",
    "f1_scores.plot(kind='barh', ax=axes[1,1], color='skyblue')\n",
    "axes[1,1].set_title('F1-Score Ranking', fontweight='bold')\n",
    "axes[1,1].set_xlabel('F1-Score')\n",
    "axes[1,1].set_ylabel('Models')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print best performing model\n",
    "best_f1_model = metrics_df['F1-Score'].idxmax()\n",
    "best_f1_score = metrics_df['F1-Score'].max()\n",
    "\n",
    "print(f\"\\nBest Performing Model (by F1-Score): {best_f1_model}\")\n",
    "print(f\"F1-Score: {best_f1_score:.4f}\")\n",
    "print(f\"Accuracy: {metrics_df.loc[best_f1_model, 'Accuracy']:.4f}\")\n",
    "print(f\"Precision: {metrics_df.loc[best_f1_model, 'Precision']:.4f}\")\n",
    "print(f\"Recall: {metrics_df.loc[best_f1_model, 'Recall']:.4f}\")\n",
    "\n",
    "print(\"\\nComprehensive model comparison completed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 22: DETAILED CONFUSION MATRIX ANALYSIS\n",
    "# ============================================================\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Function to create detailed confusion matrix with percentages\n",
    "def create_detailed_confusion_matrix(y_true, y_pred, model_name, ax):\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    \n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    # Calculate percentages\n",
    "    cm_percent = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] * 100\n",
    "    \n",
    "    # Create annotations with both counts and percentages\n",
    "    annotations = []\n",
    "    for i in range(cm.shape[0]):\n",
    "        row = []\n",
    "        for j in range(cm.shape[1]):\n",
    "            row.append(f'{cm[i,j]}\\n({cm_percent[i,j]:.1f}%)')\n",
    "        annotations.append(row)\n",
    "    \n",
    "    # Plot heatmap\n",
    "    sns.heatmap(cm, annot=annotations, fmt='', cmap='Blues', ax=ax,\n",
    "                xticklabels=['Normal', 'Crisis'], \n",
    "                yticklabels=['Normal', 'Crisis'],\n",
    "                cbar_kws={'label': 'Count'})\n",
    "    \n",
    "    ax.set_title(f'{model_name}\\nConfusion Matrix (Count & Percentage)', fontweight='bold')\n",
    "    ax.set_xlabel('Predicted Label')\n",
    "    ax.set_ylabel('True Label')\n",
    "    \n",
    "    # Add performance metrics as text\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    \n",
    "    metrics_text = f'Accuracy: {accuracy:.3f}\\nPrecision: {precision:.3f}\\nRecall: {recall:.3f}\\nF1: {f1:.3f}'\n",
    "    ax.text(0.02, 0.98, metrics_text, transform=ax.transAxes, \n",
    "            verticalalignment='top', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))\n",
    "\n",
    "# Create detailed confusion matrices for top 3 models\n",
    "top_models = ['Random Forest', 'Naive Bayes', 'Gradient Boosting']\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "fig.suptitle('Detailed Confusion Matrix Analysis - Top 3 Models', fontsize=16, fontweight='bold')\n",
    "\n",
    "for i, model_name in enumerate(top_models):\n",
    "    if model_name == 'Random Forest':\n",
    "        predictions_df = rf_predictions_quarterly\n",
    "    elif model_name == 'Naive Bayes':\n",
    "        predictions_df = nb_predictions_quarterly\n",
    "    elif model_name == 'Gradient Boosting':\n",
    "        predictions_df = gbt_predictions_quarterly\n",
    "    \n",
    "    y_true, y_pred = get_predictions_data(predictions_df)\n",
    "    create_detailed_confusion_matrix(y_true, y_pred, model_name, axes[i])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print detailed classification reports\n",
    "print(\"\\nDETAILED CLASSIFICATION REPORTS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for model_name in top_models:\n",
    "    if model_name == 'Random Forest':\n",
    "        predictions_df = rf_predictions_quarterly\n",
    "    elif model_name == 'Naive Bayes':\n",
    "        predictions_df = nb_predictions_quarterly\n",
    "    elif model_name == 'Gradient Boosting':\n",
    "        predictions_df = gbt_predictions_quarterly\n",
    "    \n",
    "    y_true, y_pred = get_predictions_data(predictions_df)\n",
    "    \n",
    "    print(f\"\\n{model_name.upper()}:\")\n",
    "    print(\"-\" * 40)\n",
    "    print(classification_report(y_true, y_pred, \n",
    "                              target_names=['Normal', 'Crisis'],\n",
    "                              digits=4))\n",
    "\n",
    "print(\"\\nDetailed confusion matrix analysis completed!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
