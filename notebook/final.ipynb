{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5235d910",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from: ./data/US Airline Flight Routes and Fares 1993-2024.csv\n",
      "Data loaded successfully!\n",
      "Dataset shape: (245,955 rows, 23 columns)\n",
      "Columns: ['tbl', 'Year', 'quarter', 'citymarketid_1', 'citymarketid_2', 'city1', 'city2', 'airportid_1', 'airportid_2', 'airport_1', 'airport_2', 'nsmiles', 'passengers', 'fare', 'carrier_lg', 'large_ms', 'fare_lg', 'carrier_low', 'lf_ms', 'fare_low', 'Geocoded_City1', 'Geocoded_City2', 'tbl1apk']\n",
      "\n",
      "First 5 rows:\n",
      "+-------+----+-------+--------------+--------------+------------------------------+-----------------------------+-----------+-----------+---------+---------+-------+----------+------+----------+--------+-------+-----------+------+--------+--------------+--------------+---------------------+\n",
      "|tbl    |Year|quarter|citymarketid_1|citymarketid_2|city1                         |city2                        |airportid_1|airportid_2|airport_1|airport_2|nsmiles|passengers|fare  |carrier_lg|large_ms|fare_lg|carrier_low|lf_ms |fare_low|Geocoded_City1|Geocoded_City2|tbl1apk              |\n",
      "+-------+----+-------+--------------+--------------+------------------------------+-----------------------------+-----------+-----------+---------+---------+-------+----------+------+----------+--------+-------+-----------+------+--------+--------------+--------------+---------------------+\n",
      "|Table1a|2021|3      |30135         |33195         |Allentown/Bethlehem/Easton, PA|Tampa, FL (Metropolitan Area)|10135      |14112      |ABE      |PIE      |970    |180       |81.43 |G4        |1.0     |81.43  |G4         |1.0   |81.43   |NULL          |NULL          |202131013514112ABEPIE|\n",
      "|Table1a|2021|3      |30135         |33195         |Allentown/Bethlehem/Easton, PA|Tampa, FL (Metropolitan Area)|10135      |15304      |ABE      |TPA      |970    |19        |208.93|DL        |0.4659  |219.98 |UA         |0.1193|154.11  |NULL          |NULL          |202131013515304ABETPA|\n",
      "|Table1a|2021|3      |30140         |30194         |Albuquerque, NM               |Dallas/Fort Worth, TX        |10140      |11259      |ABQ      |DAL      |580    |204       |184.56|WN        |0.9968  |184.44 |WN         |0.9968|184.44  |NULL          |NULL          |202131014011259ABQDAL|\n",
      "|Table1a|2021|3      |30140         |30194         |Albuquerque, NM               |Dallas/Fort Worth, TX        |10140      |11298      |ABQ      |DFW      |580    |264       |182.64|AA        |0.9774  |183.09 |AA         |0.9774|183.09  |NULL          |NULL          |202131014011298ABQDFW|\n",
      "|Table1a|2021|3      |30140         |30466         |Albuquerque, NM               |Phoenix, AZ                  |10140      |14107      |ABQ      |PHX      |328    |398       |177.11|WN        |0.6061  |184.49 |AA         |0.3939|165.77  |NULL          |NULL          |202131014014107ABQPHX|\n",
      "+-------+----+-------+--------------+--------------+------------------------------+-----------------------------+-----------+-----------+---------+---------+-------+----------+------+----------+--------+-------+-----------+------+--------+--------------+--------------+---------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "\n",
      "Data types:\n",
      "root\n",
      " |-- tbl: string (nullable = true)\n",
      " |-- Year: integer (nullable = true)\n",
      " |-- quarter: integer (nullable = true)\n",
      " |-- citymarketid_1: integer (nullable = true)\n",
      " |-- citymarketid_2: integer (nullable = true)\n",
      " |-- city1: string (nullable = true)\n",
      " |-- city2: string (nullable = true)\n",
      " |-- airportid_1: integer (nullable = true)\n",
      " |-- airportid_2: integer (nullable = true)\n",
      " |-- airport_1: string (nullable = true)\n",
      " |-- airport_2: string (nullable = true)\n",
      " |-- nsmiles: integer (nullable = true)\n",
      " |-- passengers: integer (nullable = true)\n",
      " |-- fare: double (nullable = true)\n",
      " |-- carrier_lg: string (nullable = true)\n",
      " |-- large_ms: double (nullable = true)\n",
      " |-- fare_lg: double (nullable = true)\n",
      " |-- carrier_low: string (nullable = true)\n",
      " |-- lf_ms: double (nullable = true)\n",
      " |-- fare_low: double (nullable = true)\n",
      " |-- Geocoded_City1: string (nullable = true)\n",
      " |-- Geocoded_City2: string (nullable = true)\n",
      " |-- tbl1apk: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Load Data from CSV file\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"AviationTrendAnalysis\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Set file path\n",
    "file_path = \"./data/US Airline Flight Routes and Fares 1993-2024.csv\"\n",
    "\n",
    "print(f\"Loading data from: {file_path}\")\n",
    "\n",
    "# Load CSV data with proper options\n",
    "df = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"multiLine\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .option(\"quote\", '\"') \\\n",
    "    .option(\"escape\", '\"') \\\n",
    "    .csv(file_path)\n",
    "\n",
    "# Show basic information about the dataset\n",
    "print(f\"Data loaded successfully!\")\n",
    "print(f\"Dataset shape: ({df.count():,} rows, {len(df.columns)} columns)\")\n",
    "print(f\"Columns: {df.columns}\")\n",
    "\n",
    "# Show first few rows\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "df.show(5, truncate=False)\n",
    "\n",
    "# Show data types\n",
    "print(\"\\nData types:\")\n",
    "df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "97e7083b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DATA CLEANING PROCESS ===\n",
      "Original dataset: 245,955 rows\n",
      "\n",
      "1. Cleaning whitespaces...\n",
      "2. Standardizing string values...\n",
      "3. Converting numeric columns...\n",
      "\n",
      "=== DATA QUALITY BEFORE CLEANING ===\n",
      "Missing values per column:\n",
      "  carrier_lg: 1,540 missing values\n",
      "  large_ms: 1,540 missing values\n",
      "  fare_lg: 1,540 missing values\n",
      "  carrier_low: 1,612 missing values\n",
      "  lf_ms: 1,612 missing values\n",
      "  fare_low: 1,612 missing values\n",
      "  Geocoded_City1: 39,206 missing values\n",
      "  Geocoded_City2: 39,206 missing values\n",
      "\n",
      "Cleaned dataset: 245,955 rows\n",
      "\n",
      "Sample of cleaned data:\n",
      "+-------+------+-------+--------------+--------------+------------------------------+-----------------------------+-----------+-----------+---------+---------+-------+----------+------+----------+--------+-------+-----------+------+--------+--------------+--------------+---------------------+\n",
      "|tbl    |Year  |quarter|citymarketid_1|citymarketid_2|city1                         |city2                        |airportid_1|airportid_2|airport_1|airport_2|nsmiles|passengers|fare  |carrier_lg|large_ms|fare_lg|carrier_low|lf_ms |fare_low|Geocoded_City1|Geocoded_City2|tbl1apk              |\n",
      "+-------+------+-------+--------------+--------------+------------------------------+-----------------------------+-----------+-----------+---------+---------+-------+----------+------+----------+--------+-------+-----------+------+--------+--------------+--------------+---------------------+\n",
      "|Table1a|2021.0|3.0    |30135.0       |33195.0       |Allentown/Bethlehem/Easton, PA|Tampa, FL (Metropolitan Area)|10135.0    |14112.0    |ABE      |PIE      |970.0  |180.0     |81.43 |G4        |1.0     |81.43  |G4         |1.0   |81.43   |NULL          |NULL          |202131013514112ABEPIE|\n",
      "|Table1a|2021.0|3.0    |30135.0       |33195.0       |Allentown/Bethlehem/Easton, PA|Tampa, FL (Metropolitan Area)|10135.0    |15304.0    |ABE      |TPA      |970.0  |19.0      |208.93|DL        |0.4659  |219.98 |UA         |0.1193|154.11  |NULL          |NULL          |202131013515304ABETPA|\n",
      "|Table1a|2021.0|3.0    |30140.0       |30194.0       |Albuquerque, NM               |Dallas/Fort Worth, TX        |10140.0    |11259.0    |ABQ      |DAL      |580.0  |204.0     |184.56|WN        |0.9968  |184.44 |WN         |0.9968|184.44  |NULL          |NULL          |202131014011259ABQDAL|\n",
      "|Table1a|2021.0|3.0    |30140.0       |30194.0       |Albuquerque, NM               |Dallas/Fort Worth, TX        |10140.0    |11298.0    |ABQ      |DFW      |580.0  |264.0     |182.64|AA        |0.9774  |183.09 |AA         |0.9774|183.09  |NULL          |NULL          |202131014011298ABQDFW|\n",
      "|Table1a|2021.0|3.0    |30140.0       |30466.0       |Albuquerque, NM               |Phoenix, AZ                  |10140.0    |14107.0    |ABQ      |PHX      |328.0  |398.0     |177.11|WN        |0.6061  |184.49 |AA         |0.3939|165.77  |NULL          |NULL          |202131014014107ABQPHX|\n",
      "+-------+------+-------+--------------+--------------+------------------------------+-----------------------------+-----------+-----------+---------+---------+-------+----------+------+----------+--------+-------+-----------+------+--------+--------------+--------------+---------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Data Cleaning\n",
    "from pyspark.sql.functions import col, trim, regexp_replace, when, isnan, isnull\n",
    "\n",
    "print(\"=== DATA CLEANING PROCESS ===\")\n",
    "print(f\"Original dataset: {df.count():,} rows\")\n",
    "\n",
    "# 1. Remove leading/trailing whitespaces from string columns\n",
    "print(\"\\n1. Cleaning whitespaces...\")\n",
    "string_columns = [field.name for field in df.schema.fields if field.dataType.typeName() == 'string']\n",
    "\n",
    "for col_name in string_columns:\n",
    "    df = df.withColumn(col_name, trim(col(col_name)))\n",
    "\n",
    "# 2. Clean and standardize string values\n",
    "print(\"2. Standardizing string values...\")\n",
    "# Remove extra spaces and standardize\n",
    "for col_name in string_columns:\n",
    "    df = df.withColumn(col_name, regexp_replace(col(col_name), \"\\\\s+\", \" \"))\n",
    "\n",
    "# 3. Handle numeric columns - convert to proper types\n",
    "print(\"3. Converting numeric columns...\")\n",
    "numeric_columns = ['Year', 'quarter', 'citymarketid_1', 'citymarketid_2', \n",
    "                   'airportid_1', 'airportid_2', 'nsmiles', 'passengers', \n",
    "                   'fare', 'large_ms', 'fare_lg', 'lf_ms', 'fare_low']\n",
    "\n",
    "for col_name in numeric_columns:\n",
    "    if col_name in df.columns:\n",
    "        # Convert to double for numeric columns\n",
    "        df = df.withColumn(col_name, col(col_name).cast(\"double\"))\n",
    "\n",
    "# 4. Show data quality metrics before cleaning\n",
    "print(\"\\n=== DATA QUALITY BEFORE CLEANING ===\")\n",
    "print(\"Missing values per column:\")\n",
    "for col_name in df.columns:\n",
    "    null_count = df.filter(col(col_name).isNull()).count()\n",
    "    nan_count = df.filter(isnan(col(col_name))).count() if col_name in numeric_columns else 0\n",
    "    total_missing = null_count + nan_count\n",
    "    if total_missing > 0:\n",
    "        print(f\"  {col_name}: {total_missing:,} missing values\")\n",
    "\n",
    "# 5. Show sample of cleaned data\n",
    "print(f\"\\nCleaned dataset: {df.count():,} rows\")\n",
    "print(\"\\nSample of cleaned data:\")\n",
    "df.show(5, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bf717f4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== REMOVING MISSING VALUES ===\n",
      "Dataset before removing missing values: 245,955 rows\n",
      "\n",
      "Missing values analysis:\n",
      "  carrier_lg: 1,540 missing values\n",
      "  large_ms: 1,540 missing values\n",
      "  fare_lg: 1,540 missing values\n",
      "  carrier_low: 1,612 missing values\n",
      "  lf_ms: 1,612 missing values\n",
      "  fare_low: 1,612 missing values\n",
      "  Geocoded_City1: 39,206 missing values\n",
      "  Geocoded_City2: 39,206 missing values\n",
      "\n",
      "Excluding columns from missing value removal: ['Geocoded_City1', 'Geocoded_City2']\n",
      "\n",
      "Removing rows with missing values (excluding ['Geocoded_City1', 'Geocoded_City2'])...\n",
      "Dataset after removing missing values: 244,343 rows\n",
      "Removed 1,612 rows (0.66%)\n",
      "\n",
      "=== VERIFICATION: Missing values after cleaning ===\n",
      "✓ No missing values found in important columns!\n",
      "\n",
      "Status of excluded columns:\n",
      "  Geocoded_City1: 39,154 missing values (kept as-is)\n",
      "  Geocoded_City2: 39,154 missing values (kept as-is)\n",
      "\n",
      "=== FINAL DATASET STATISTICS ===\n",
      "Total rows: 244,343\n",
      "Total columns: 23\n",
      "Columns: ['tbl', 'Year', 'quarter', 'citymarketid_1', 'citymarketid_2', 'city1', 'city2', 'airportid_1', 'airportid_2', 'airport_1', 'airport_2', 'nsmiles', 'passengers', 'fare', 'carrier_lg', 'large_ms', 'fare_lg', 'carrier_low', 'lf_ms', 'fare_low', 'Geocoded_City1', 'Geocoded_City2', 'tbl1apk']\n",
      "\n",
      "Sample of final clean data:\n",
      "+-------+------+-------+--------------+--------------+------------------------------+-----------------------------+-----------+-----------+---------+---------+-------+----------+------+----------+--------+-------+-----------+------+--------+--------------+--------------+---------------------+\n",
      "|tbl    |Year  |quarter|citymarketid_1|citymarketid_2|city1                         |city2                        |airportid_1|airportid_2|airport_1|airport_2|nsmiles|passengers|fare  |carrier_lg|large_ms|fare_lg|carrier_low|lf_ms |fare_low|Geocoded_City1|Geocoded_City2|tbl1apk              |\n",
      "+-------+------+-------+--------------+--------------+------------------------------+-----------------------------+-----------+-----------+---------+---------+-------+----------+------+----------+--------+-------+-----------+------+--------+--------------+--------------+---------------------+\n",
      "|Table1a|2021.0|3.0    |30135.0       |33195.0       |Allentown/Bethlehem/Easton, PA|Tampa, FL (Metropolitan Area)|10135.0    |14112.0    |ABE      |PIE      |970.0  |180.0     |81.43 |G4        |1.0     |81.43  |G4         |1.0   |81.43   |NULL          |NULL          |202131013514112ABEPIE|\n",
      "|Table1a|2021.0|3.0    |30135.0       |33195.0       |Allentown/Bethlehem/Easton, PA|Tampa, FL (Metropolitan Area)|10135.0    |15304.0    |ABE      |TPA      |970.0  |19.0      |208.93|DL        |0.4659  |219.98 |UA         |0.1193|154.11  |NULL          |NULL          |202131013515304ABETPA|\n",
      "|Table1a|2021.0|3.0    |30140.0       |30194.0       |Albuquerque, NM               |Dallas/Fort Worth, TX        |10140.0    |11259.0    |ABQ      |DAL      |580.0  |204.0     |184.56|WN        |0.9968  |184.44 |WN         |0.9968|184.44  |NULL          |NULL          |202131014011259ABQDAL|\n",
      "|Table1a|2021.0|3.0    |30140.0       |30194.0       |Albuquerque, NM               |Dallas/Fort Worth, TX        |10140.0    |11298.0    |ABQ      |DFW      |580.0  |264.0     |182.64|AA        |0.9774  |183.09 |AA         |0.9774|183.09  |NULL          |NULL          |202131014011298ABQDFW|\n",
      "|Table1a|2021.0|3.0    |30140.0       |30466.0       |Albuquerque, NM               |Phoenix, AZ                  |10140.0    |14107.0    |ABQ      |PHX      |328.0  |398.0     |177.11|WN        |0.6061  |184.49 |AA         |0.3939|165.77  |NULL          |NULL          |202131014014107ABQPHX|\n",
      "+-------+------+-------+--------------+--------------+------------------------------+-----------------------------+-----------+-----------+---------+---------+-------+----------+------+----------+--------+-------+-----------+------+--------+--------------+--------------+---------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "\n",
      "Caching cleaned dataset for better performance...\n",
      "✓ Dataset cleaned and cached successfully!\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Remove Missing Values (Excluding Geocoded_City columns)\n",
    "from pyspark.sql.functions import col, isnan, isnull, count, when\n",
    "\n",
    "print(\"=== REMOVING MISSING VALUES ===\")\n",
    "print(f\"Dataset before removing missing values: {df.count():,} rows\")\n",
    "\n",
    "# 1. Count missing values in each column\n",
    "print(\"\\nMissing values analysis:\")\n",
    "missing_summary = []\n",
    "for col_name in df.columns:\n",
    "    null_count = df.filter(col(col_name).isNull()).count()\n",
    "    nan_count = df.filter(isnan(col(col_name))).count()\n",
    "    total_missing = null_count + nan_count\n",
    "    missing_summary.append((col_name, total_missing))\n",
    "    if total_missing > 0:\n",
    "        print(f\"  {col_name}: {total_missing:,} missing values\")\n",
    "\n",
    "# 2. Define columns to exclude from missing value removal\n",
    "exclude_columns = ['Geocoded_City1', 'Geocoded_City2']\n",
    "print(f\"\\nExcluding columns from missing value removal: {exclude_columns}\")\n",
    "\n",
    "# 3. Remove rows with missing values (excluding Geocoded_City columns)\n",
    "print(f\"\\nRemoving rows with missing values (excluding {exclude_columns})...\")\n",
    "\n",
    "# Create condition to check for missing values in all columns except excluded ones\n",
    "missing_condition = None\n",
    "for col_name in df.columns:\n",
    "    if col_name not in exclude_columns:\n",
    "        if missing_condition is None:\n",
    "            missing_condition = col(col_name).isNull() | isnan(col(col_name))\n",
    "        else:\n",
    "            missing_condition = missing_condition | col(col_name).isNull() | isnan(col(col_name))\n",
    "\n",
    "# Filter out rows with missing values in non-excluded columns\n",
    "df_clean = df.filter(~missing_condition)\n",
    "\n",
    "print(f\"Dataset after removing missing values: {df_clean.count():,} rows\")\n",
    "print(f\"Removed {df.count() - df_clean.count():,} rows ({(df.count() - df_clean.count())/df.count()*100:.2f}%)\")\n",
    "\n",
    "# 4. Verify missing values in important columns (excluding Geocoded_City)\n",
    "print(\"\\n=== VERIFICATION: Missing values after cleaning ===\")\n",
    "important_columns = [col for col in df_clean.columns if col not in exclude_columns]\n",
    "all_clean = True\n",
    "for col_name in important_columns:\n",
    "    null_count = df_clean.filter(col(col_name).isNull()).count()\n",
    "    nan_count = df_clean.filter(isnan(col(col_name))).count()\n",
    "    total_missing = null_count + nan_count\n",
    "    if total_missing > 0:\n",
    "        print(f\"  {col_name}: {total_missing:,} missing values\")\n",
    "        all_clean = False\n",
    "\n",
    "if all_clean:\n",
    "    print(\"✓ No missing values found in important columns!\")\n",
    "\n",
    "# Show status of excluded columns\n",
    "print(f\"\\nStatus of excluded columns:\")\n",
    "for col_name in exclude_columns:\n",
    "    if col_name in df_clean.columns:\n",
    "        null_count = df_clean.filter(col(col_name).isNull()).count()\n",
    "        print(f\"  {col_name}: {null_count:,} missing values (kept as-is)\")\n",
    "\n",
    "# 5. Show final dataset statistics\n",
    "print(f\"\\n=== FINAL DATASET STATISTICS ===\")\n",
    "print(f\"Total rows: {df_clean.count():,}\")\n",
    "print(f\"Total columns: {len(df_clean.columns)}\")\n",
    "print(f\"Columns: {df_clean.columns}\")\n",
    "\n",
    "# 6. Show sample of final clean data\n",
    "print(\"\\nSample of final clean data:\")\n",
    "df_clean.show(5, truncate=False)\n",
    "\n",
    "# 7. Cache the cleaned dataset for better performance\n",
    "print(\"\\nCaching cleaned dataset for better performance...\")\n",
    "df_clean.cache()\n",
    "df_clean.count()  # Trigger caching\n",
    "\n",
    "print(\"✓ Dataset cleaned and cached successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da828f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Data Processing and COVID Labeling for Crisis Prediction\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import Window\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "from pyspark.ml.classification import LogisticRegression, RandomForestClassifier, GBTClassifier\n",
    "from pyspark.ml.regression import LinearRegression, RandomForestRegressor, GBTRegressor\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, RegressionEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml import Pipeline\n",
    "import time\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"XỬ LÝ DỮ LIỆU VÀ DÁN NHÃN COVID CHO CRISIS PREDICTION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Bước 1: Aggregate data theo Year-Quarter\n",
    "print(\"\\n1. AGGREGATE DATA THEO YEAR-QUARTER...\")\n",
    "\n",
    "df_quarterly = df_clean.groupBy('Year', 'quarter').agg(\n",
    "    # Volume metrics\n",
    "    count('*').alias('num_routes'),\n",
    "    sum('passengers').alias('total_passengers'),\n",
    "    avg('passengers').alias('avg_passengers_per_route'),\n",
    "\n",
    "    # Price metrics\n",
    "    avg('fare').alias('avg_fare'),\n",
    "    stddev('fare').alias('fare_std'),\n",
    "    min('fare').alias('fare_min'),\n",
    "    max('fare').alias('fare_max'),\n",
    "\n",
    "    # Distance metrics\n",
    "    avg('nsmiles').alias('avg_distance'),\n",
    "    stddev('nsmiles').alias('distance_std'),\n",
    "\n",
    "    # Competition metrics\n",
    "    countDistinct('carrier_lg').alias('num_carriers'),\n",
    "    avg('large_ms').alias('avg_market_share_large'),\n",
    "    avg('lf_ms').alias('avg_market_share_lowcost')\n",
    ").orderBy('Year', 'quarter')\n",
    "\n",
    "# Tạo time_period identifier\n",
    "df_quarterly = df_quarterly.withColumn('time_period',\n",
    "    concat(col('Year').cast('string'), lit('-Q'), col('quarter').cast('string'))\n",
    ")\n",
    "\n",
    "print(f\"  ✓ Aggregated to {df_quarterly.count()} quarters\")\n",
    "\n",
    "# Bước 2: Tạo labels (COVID = crisis)\n",
    "print(\"\\n2. TẠO LABELS (COVID = CRISIS)...\")\n",
    "\n",
    "df_quarterly = df_quarterly.withColumn('is_crisis',\n",
    "    when((col('Year') >= 2020) & (col('Year') <= 2021), 1.0)\n",
    "    .otherwise(0.0)\n",
    ")\n",
    "\n",
    "# Kiểm tra phân bố labels\n",
    "label_distribution = df_quarterly.groupBy('is_crisis').count()\n",
    "print(\"\\n  Phân bố labels:\")\n",
    "label_distribution.show()\n",
    "\n",
    "# Bước 3: Feature Engineering - Rate of Change\n",
    "print(\"\\n3. FEATURE ENGINEERING - RATE OF CHANGE...\")\n",
    "\n",
    "# Window specs\n",
    "window_qoq = Window.orderBy('Year', 'quarter')\n",
    "\n",
    "# QoQ (Quarter-over-Quarter) changes\n",
    "change_cols = ['num_routes', 'total_passengers', 'avg_fare', 'avg_distance']\n",
    "\n",
    "for col_name in change_cols:\n",
    "    # Lấy giá trị quarter trước\n",
    "    df_quarterly = df_quarterly.withColumn(\n",
    "        f'{col_name}_prev_q',\n",
    "        lag(col(col_name), 1).over(window_qoq)\n",
    "    )\n",
    "\n",
    "    # Tính % change\n",
    "    df_quarterly = df_quarterly.withColumn(\n",
    "        f'{col_name}_change_qoq',\n",
    "        when(col(f'{col_name}_prev_q').isNotNull() & (col(f'{col_name}_prev_q') != 0),\n",
    "             (col(col_name) - col(f'{col_name}_prev_q')) / col(f'{col_name}_prev_q'))\n",
    "        .otherwise(0.0)\n",
    "    )\n",
    "\n",
    "    # Drop temp column\n",
    "    df_quarterly = df_quarterly.drop(f'{col_name}_prev_q')\n",
    "\n",
    "print(\"  ✓ QoQ changes calculated\")\n",
    "\n",
    "# YoY (Year-over-Year) changes\n",
    "yoy_cols = ['num_routes', 'total_passengers', 'avg_fare']\n",
    "\n",
    "for col_name in yoy_cols:\n",
    "    df_quarterly = df_quarterly.withColumn(\n",
    "        f'{col_name}_prev_year',\n",
    "        lag(col(col_name), 4).over(window_qoq)\n",
    "    )\n",
    "\n",
    "    df_quarterly = df_quarterly.withColumn(\n",
    "        f'{col_name}_change_yoy',\n",
    "        when(col(f'{col_name}_prev_year').isNotNull() & (col(f'{col_name}_prev_year') != 0),\n",
    "             (col(col_name) - col(f'{col_name}_prev_year')) / col(f'{col_name}_prev_year'))\n",
    "        .otherwise(0.0)\n",
    "    )\n",
    "\n",
    "    df_quarterly = df_quarterly.drop(f'{col_name}_prev_year')\n",
    "\n",
    "print(\"  ✓ YoY changes calculated\")\n",
    "\n",
    "# Bước 4: Derived features\n",
    "print(\"\\n4. CREATING DERIVED FEATURES...\")\n",
    "\n",
    "# Volatility metrics\n",
    "df_quarterly = df_quarterly.withColumn('fare_volatility',\n",
    "    when(col('avg_fare') != 0, col('fare_std') / col('avg_fare')).otherwise(0.0)\n",
    ")\n",
    "\n",
    "df_quarterly = df_quarterly.withColumn('distance_volatility',\n",
    "    when(col('avg_distance') != 0, col('distance_std') / col('avg_distance')).otherwise(0.0)\n",
    ")\n",
    "\n",
    "# Range metrics\n",
    "df_quarterly = df_quarterly.withColumn('fare_range',\n",
    "    col('fare_max') - col('fare_min')\n",
    ")\n",
    "\n",
    "# Passenger efficiency\n",
    "df_quarterly = df_quarterly.withColumn('passenger_efficiency',\n",
    "    when(col('num_routes') != 0, col('total_passengers') / col('num_routes')).otherwise(0.0)\n",
    ")\n",
    "\n",
    "print(\"  ✓ Derived features created\")\n",
    "\n",
    "# Bước 5: Handle missing values\n",
    "print(\"\\n5. HANDLING MISSING VALUES...\")\n",
    "\n",
    "# Fill NaN với 0 (từ lag functions cho first rows)\n",
    "df_quarterly = df_quarterly.fillna(0.0)\n",
    "\n",
    "# Bước 6: Select final features\n",
    "print(\"\\n6. SELECTING FEATURES FOR MODEL...\")\n",
    "\n",
    "feature_cols_final = [\n",
    "    # Core volume metrics (3)\n",
    "    'num_routes',\n",
    "    'total_passengers',\n",
    "    'avg_passengers_per_route',\n",
    "\n",
    "    # Core price metrics (2)\n",
    "    'avg_fare',\n",
    "    'avg_distance',\n",
    "\n",
    "    # Volatility metrics (2)\n",
    "    'fare_volatility',\n",
    "    'fare_range',\n",
    "\n",
    "    # QoQ changes - IMPORTANT! (4)\n",
    "    'num_routes_change_qoq',\n",
    "    'total_passengers_change_qoq',\n",
    "    'avg_fare_change_qoq',\n",
    "    'avg_distance_change_qoq',\n",
    "\n",
    "    # YoY changes (3)\n",
    "    'num_routes_change_yoy',\n",
    "    'total_passengers_change_yoy',\n",
    "    'avg_fare_change_yoy',\n",
    "\n",
    "    # Competition (2)\n",
    "    'num_carriers',\n",
    "    'avg_market_share_large',\n",
    "\n",
    "    # Seasonal (1)\n",
    "    'quarter',\n",
    "]\n",
    "\n",
    "print(f\"\\n  Tổng số features: {len(feature_cols_final)}\")\n",
    "print(f\"\\n  Danh sách features:\")\n",
    "for i, feat in enumerate(feature_cols_final, 1):\n",
    "    print(f\"    {i:2}. {feat}\")\n",
    "\n",
    "# Hiển thị final dataset\n",
    "print(\"\\n7. FINAL DATASET:\")\n",
    "df_quarterly.select(['time_period', 'Year', 'quarter', 'is_crisis'] +\n",
    "                    feature_cols_final[:5]).show(15)\n",
    "\n",
    "print(f\"\\n✓ Data preparation completed!\")\n",
    "print(f\"  Total quarters: {df_quarterly.count()}\")\n",
    "print(f\"  Total features: {len(feature_cols_final)}\")\n",
    "print(f\"  Target: is_crisis (0 = Normal, 1 = COVID)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc25d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Simple Models Experiment (Logistic Regression, Linear Regression)\n",
    "print(\"=\" * 70)\n",
    "print(\"THÍ NGHIỆM VỚI MÔ HÌNH ĐƠN GIẢN\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Bước 1: Prepare data for modeling\n",
    "print(\"\\n1. PREPARING DATA FOR MODELING...\")\n",
    "\n",
    "# Vector Assembler\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=feature_cols_final,\n",
    "    outputCol=\"features\"\n",
    ")\n",
    "\n",
    "# Standard Scaler\n",
    "scaler = StandardScaler(\n",
    "    inputCol=\"features\",\n",
    "    outputCol=\"scaled_features\",\n",
    "    withStd=True,\n",
    "    withMean=True\n",
    ")\n",
    "\n",
    "# Train-Test Split\n",
    "print(\"\\n2. TRAIN-TEST SPLIT...\")\n",
    "train_data, test_data = df_quarterly.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "print(f\"  Training set: {train_data.count()} quarters\")\n",
    "print(f\"  Test set:     {test_data.count()} quarters\")\n",
    "\n",
    "# Cache data for better performance\n",
    "train_data.cache()\n",
    "test_data.cache()\n",
    "\n",
    "# Bước 3: Model 1 - Logistic Regression\n",
    "print(\"\\n3. MODEL 1: LOGISTIC REGRESSION...\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Create pipeline\n",
    "lr_pipeline = Pipeline(stages=[\n",
    "    assembler,\n",
    "    scaler,\n",
    "    LogisticRegression(\n",
    "        featuresCol=\"scaled_features\",\n",
    "        labelCol=\"is_crisis\",\n",
    "        maxIter=100,\n",
    "        regParam=0.01\n",
    "    )\n",
    "])\n",
    "\n",
    "# Train model\n",
    "lr_model = lr_pipeline.fit(train_data)\n",
    "lr_predictions = lr_model.transform(test_data)\n",
    "\n",
    "# Evaluate\n",
    "lr_evaluator = BinaryClassificationEvaluator(labelCol=\"is_crisis\")\n",
    "lr_auc = lr_evaluator.evaluate(lr_predictions)\n",
    "\n",
    "lr_time = time.time() - start_time\n",
    "\n",
    "print(f\"  ✓ Logistic Regression - AUC: {lr_auc:.4f} (Time: {lr_time:.2f}s)\")\n",
    "\n",
    "# Bước 4: Model 2 - Linear Regression (for comparison)\n",
    "print(\"\\n4. MODEL 2: LINEAR REGRESSION...\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Create pipeline\n",
    "linear_pipeline = Pipeline(stages=[\n",
    "    assembler,\n",
    "    scaler,\n",
    "    LinearRegression(\n",
    "        featuresCol=\"scaled_features\",\n",
    "        labelCol=\"is_crisis\",\n",
    "        maxIter=100,\n",
    "        regParam=0.01\n",
    "    )\n",
    "])\n",
    "\n",
    "# Train model\n",
    "linear_model = linear_pipeline.fit(train_data)\n",
    "linear_predictions = linear_model.transform(test_data)\n",
    "\n",
    "# Evaluate\n",
    "linear_evaluator = RegressionEvaluator(\n",
    "    labelCol=\"is_crisis\",\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"rmse\"\n",
    ")\n",
    "linear_rmse = linear_evaluator.evaluate(linear_predictions)\n",
    "\n",
    "linear_time = time.time() - start_time\n",
    "\n",
    "print(f\"  ✓ Linear Regression - RMSE: {linear_rmse:.4f} (Time: {linear_time:.2f}s)\")\n",
    "\n",
    "# Bước 5: Model 3 - Simple Random Forest (baseline)\n",
    "print(\"\\n5. MODEL 3: SIMPLE RANDOM FOREST...\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Create pipeline\n",
    "rf_simple_pipeline = Pipeline(stages=[\n",
    "    assembler,\n",
    "    RandomForestClassifier(\n",
    "        featuresCol=\"features\",\n",
    "        labelCol=\"is_crisis\",\n",
    "        numTrees=50,\n",
    "        maxDepth=10,\n",
    "        seed=42\n",
    "    )\n",
    "])\n",
    "\n",
    "# Train model\n",
    "rf_simple_model = rf_simple_pipeline.fit(train_data)\n",
    "rf_simple_predictions = rf_simple_model.transform(test_data)\n",
    "\n",
    "# Evaluate\n",
    "rf_simple_evaluator = BinaryClassificationEvaluator(labelCol=\"is_crisis\")\n",
    "rf_simple_auc = rf_simple_evaluator.evaluate(rf_simple_predictions)\n",
    "\n",
    "rf_simple_time = time.time() - start_time\n",
    "\n",
    "print(f\"  ✓ Simple Random Forest - AUC: {rf_simple_auc:.4f} (Time: {rf_simple_time:.2f}s)\")\n",
    "\n",
    "# Bước 6: Show results summary\n",
    "print(\"\\n6. SIMPLE MODELS SUMMARY:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"{'Model':<25} {'Metric':<10} {'Score':<10} {'Time(s)':<10}\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"{'Logistic Regression':<25} {'AUC':<10} {lr_auc:<10.4f} {lr_time:<10.2f}\")\n",
    "print(f\"{'Linear Regression':<25} {'RMSE':<10} {linear_rmse:<10.4f} {linear_time:<10.2f}\")\n",
    "print(f\"{'Simple Random Forest':<25} {'AUC':<10} {rf_simple_auc:<10.4f} {rf_simple_time:<10.2f}\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Show feature importance for Random Forest\n",
    "print(\"\\n7. FEATURE IMPORTANCE (Simple Random Forest):\")\n",
    "rf_simple_feature_importance = rf_simple_model.stages[-1].featureImportances.toArray()\n",
    "feature_importance_df = spark.createDataFrame([\n",
    "    (feature_cols_final[i], float(rf_simple_feature_importance[i]))\n",
    "    for i in range(len(feature_cols_final))\n",
    "], [\"feature\", \"importance\"]).orderBy(col(\"importance\").desc())\n",
    "\n",
    "print(\"Top 10 most important features:\")\n",
    "feature_importance_df.show(10, truncate=False)\n",
    "\n",
    "print(f\"\\n✓ Simple models experiment completed!\")\n",
    "print(f\"  Best simple model: {'Logistic Regression' if lr_auc > rf_simple_auc else 'Simple Random Forest'}\")\n",
    "print(f\"  Best AUC: {max(lr_auc, rf_simple_auc):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c797fcc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Advanced Models Experiment (Random Forest, Gradient Boosting)\n",
    "print(\"=\" * 70)\n",
    "print(\"THÍ NGHIỆM VỚI MÔ HÌNH NÂNG CAO\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Bước 1: Model 4 - Advanced Random Forest\n",
    "print(\"\\n1. MODEL 4: ADVANCED RANDOM FOREST...\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Create pipeline with more trees and depth\n",
    "rf_advanced_pipeline = Pipeline(stages=[\n",
    "    assembler,\n",
    "    RandomForestClassifier(\n",
    "        featuresCol=\"features\",\n",
    "        labelCol=\"is_crisis\",\n",
    "        numTrees=200,\n",
    "        maxDepth=15,\n",
    "        maxBins=32,\n",
    "        subsamplingRate=0.8,\n",
    "        seed=42\n",
    "    )\n",
    "])\n",
    "\n",
    "# Train model\n",
    "rf_advanced_model = rf_advanced_pipeline.fit(train_data)\n",
    "rf_advanced_predictions = rf_advanced_model.transform(test_data)\n",
    "\n",
    "# Evaluate\n",
    "rf_advanced_evaluator = BinaryClassificationEvaluator(labelCol=\"is_crisis\")\n",
    "rf_advanced_auc = rf_advanced_evaluator.evaluate(rf_advanced_predictions)\n",
    "\n",
    "rf_advanced_time = time.time() - start_time\n",
    "\n",
    "print(f\"  ✓ Advanced Random Forest - AUC: {rf_advanced_auc:.4f} (Time: {rf_advanced_time:.2f}s)\")\n",
    "\n",
    "# Bước 2: Model 5 - Gradient Boosting Trees\n",
    "print(\"\\n2. MODEL 5: GRADIENT BOOSTING TREES...\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Create pipeline\n",
    "gbt_pipeline = Pipeline(stages=[\n",
    "    assembler,\n",
    "    GBTClassifier(\n",
    "        featuresCol=\"features\",\n",
    "        labelCol=\"is_crisis\",\n",
    "        maxIter=100,\n",
    "        maxDepth=6,\n",
    "        stepSize=0.1,\n",
    "        seed=42\n",
    "    )\n",
    "])\n",
    "\n",
    "# Train model\n",
    "gbt_model = gbt_pipeline.fit(train_data)\n",
    "gbt_predictions = gbt_model.transform(test_data)\n",
    "\n",
    "# Evaluate\n",
    "gbt_evaluator = BinaryClassificationEvaluator(labelCol=\"is_crisis\")\n",
    "gbt_auc = gbt_evaluator.evaluate(gbt_predictions)\n",
    "\n",
    "gbt_time = time.time() - start_time\n",
    "\n",
    "print(f\"  ✓ Gradient Boosting Trees - AUC: {gbt_auc:.4f} (Time: {gbt_time:.2f}s)\")\n",
    "\n",
    "# Bước 3: Model 6 - Random Forest with Feature Scaling\n",
    "print(\"\\n3. MODEL 6: RANDOM FOREST WITH SCALING...\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Create pipeline with scaling\n",
    "rf_scaled_pipeline = Pipeline(stages=[\n",
    "    assembler,\n",
    "    scaler,\n",
    "    RandomForestClassifier(\n",
    "        featuresCol=\"scaled_features\",\n",
    "        labelCol=\"is_crisis\",\n",
    "        numTrees=150,\n",
    "        maxDepth=12,\n",
    "        maxBins=32,\n",
    "        subsamplingRate=0.9,\n",
    "        seed=42\n",
    "    )\n",
    "])\n",
    "\n",
    "# Train model\n",
    "rf_scaled_model = rf_scaled_pipeline.fit(train_data)\n",
    "rf_scaled_predictions = rf_scaled_model.transform(test_data)\n",
    "\n",
    "# Evaluate\n",
    "rf_scaled_evaluator = BinaryClassificationEvaluator(labelCol=\"is_crisis\")\n",
    "rf_scaled_auc = rf_scaled_evaluator.evaluate(rf_scaled_predictions)\n",
    "\n",
    "rf_scaled_time = time.time() - start_time\n",
    "\n",
    "print(f\"  ✓ Random Forest with Scaling - AUC: {rf_scaled_auc:.4f} (Time: {rf_scaled_time:.2f}s)\")\n",
    "\n",
    "# Bước 4: Model 7 - Gradient Boosting with Scaling\n",
    "print(\"\\n4. MODEL 7: GRADIENT BOOSTING WITH SCALING...\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Create pipeline with scaling\n",
    "gbt_scaled_pipeline = Pipeline(stages=[\n",
    "    assembler,\n",
    "    scaler,\n",
    "    GBTClassifier(\n",
    "        featuresCol=\"scaled_features\",\n",
    "        labelCol=\"is_crisis\",\n",
    "        maxIter=150,\n",
    "        maxDepth=8,\n",
    "        stepSize=0.05,\n",
    "        seed=42\n",
    "    )\n",
    "])\n",
    "\n",
    "# Train model\n",
    "gbt_scaled_model = gbt_scaled_pipeline.fit(train_data)\n",
    "gbt_scaled_predictions = gbt_scaled_model.transform(test_data)\n",
    "\n",
    "# Evaluate\n",
    "gbt_scaled_evaluator = BinaryClassificationEvaluator(labelCol=\"is_crisis\")\n",
    "gbt_scaled_auc = gbt_scaled_evaluator.evaluate(gbt_scaled_predictions)\n",
    "\n",
    "gbt_scaled_time = time.time() - start_time\n",
    "\n",
    "print(f\"  ✓ Gradient Boosting with Scaling - AUC: {gbt_scaled_auc:.4f} (Time: {gbt_scaled_time:.2f}s)\")\n",
    "\n",
    "# Bước 5: Show results summary\n",
    "print(\"\\n5. ADVANCED MODELS SUMMARY:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"{'Model':<30} {'AUC':<10} {'Time(s)':<10}\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"{'Advanced Random Forest':<30} {rf_advanced_auc:<10.4f} {rf_advanced_time:<10.2f}\")\n",
    "print(f\"{'Gradient Boosting Trees':<30} {gbt_auc:<10.4f} {gbt_time:<10.2f}\")\n",
    "print(f\"{'Random Forest + Scaling':<30} {rf_scaled_auc:<10.4f} {rf_scaled_time:<10.2f}\")\n",
    "print(f\"{'Gradient Boosting + Scaling':<30} {gbt_scaled_auc:<10.4f} {gbt_scaled_time:<10.2f}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Bước 6: Feature Importance Analysis\n",
    "print(\"\\n6. FEATURE IMPORTANCE ANALYSIS:\")\n",
    "\n",
    "# Random Forest Feature Importance\n",
    "print(\"\\n  Random Forest Feature Importance:\")\n",
    "rf_advanced_feature_importance = rf_advanced_model.stages[-1].featureImportances.toArray()\n",
    "rf_importance_df = spark.createDataFrame([\n",
    "    (feature_cols_final[i], float(rf_advanced_feature_importance[i]))\n",
    "    for i in range(len(feature_cols_final))\n",
    "], [\"feature\", \"importance\"]).orderBy(col(\"importance\").desc())\n",
    "\n",
    "print(\"  Top 10 most important features (Random Forest):\")\n",
    "rf_importance_df.show(10, truncate=False)\n",
    "\n",
    "# Gradient Boosting Feature Importance\n",
    "print(\"\\n  Gradient Boosting Feature Importance:\")\n",
    "gbt_feature_importance = gbt_model.stages[-1].featureImportances.toArray()\n",
    "gbt_importance_df = spark.createDataFrame([\n",
    "    (feature_cols_final[i], float(gbt_feature_importance[i]))\n",
    "    for i in range(len(feature_cols_final))\n",
    "], [\"feature\", \"importance\"]).orderBy(col(\"importance\").desc())\n",
    "\n",
    "print(\"  Top 10 most important features (Gradient Boosting):\")\n",
    "gbt_importance_df.show(10, truncate=False)\n",
    "\n",
    "# Bước 7: Find best model so far\n",
    "all_models = {\n",
    "    'Logistic Regression': lr_auc,\n",
    "    'Simple Random Forest': rf_simple_auc,\n",
    "    'Advanced Random Forest': rf_advanced_auc,\n",
    "    'Gradient Boosting Trees': gbt_auc,\n",
    "    'Random Forest + Scaling': rf_scaled_auc,\n",
    "    'Gradient Boosting + Scaling': gbt_scaled_auc\n",
    "}\n",
    "\n",
    "best_model_name = max(all_models, key=all_models.get)\n",
    "best_auc = all_models[best_model_name]\n",
    "\n",
    "print(f\"\\n7. BEST MODEL SO FAR:\")\n",
    "print(f\"  Model: {best_model_name}\")\n",
    "print(f\"  AUC: {best_auc:.4f}\")\n",
    "\n",
    "print(f\"\\n✓ Advanced models experiment completed!\")\n",
    "print(f\"  Best advanced model: {best_model_name}\")\n",
    "print(f\"  Best AUC: {best_auc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce05861c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Hyperparameter Tuning for Random Forest\n",
    "print(\"=\" * 70)\n",
    "print(\"HYPERPARAMETER TUNING CHO RANDOM FOREST\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Bước 1: Prepare for Cross Validation\n",
    "print(\"\\n1. PREPARING FOR CROSS VALIDATION...\")\n",
    "\n",
    "# Create base pipeline\n",
    "base_rf = RandomForestClassifier(\n",
    "    featuresCol=\"features\",\n",
    "    labelCol=\"is_crisis\",\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# Create pipeline\n",
    "rf_tuning_pipeline = Pipeline(stages=[assembler, base_rf])\n",
    "\n",
    "# Bước 2: Define Parameter Grid\n",
    "print(\"\\n2. DEFINING PARAMETER GRID...\")\n",
    "\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(base_rf.numTrees, [100, 200, 300]) \\\n",
    "    .addGrid(base_rf.maxDepth, [10, 15, 20]) \\\n",
    "    .addGrid(base_rf.maxBins, [16, 32, 64]) \\\n",
    "    .addGrid(base_rf.subsamplingRate, [0.8, 0.9, 1.0]) \\\n",
    "    .build()\n",
    "\n",
    "print(f\"  Total combinations: {len(paramGrid)}\")\n",
    "print(\"  Parameters to test:\")\n",
    "print(\"    - numTrees: [100, 200, 300]\")\n",
    "print(\"    - maxDepth: [10, 15, 20]\")\n",
    "print(\"    - maxBins: [16, 32, 64]\")\n",
    "print(\"    - subsamplingRate: [0.8, 0.9, 1.0]\")\n",
    "\n",
    "# Bước 3: Cross Validation Setup\n",
    "print(\"\\n3. SETTING UP CROSS VALIDATION...\")\n",
    "\n",
    "# Create evaluator\n",
    "evaluator = BinaryClassificationEvaluator(labelCol=\"is_crisis\")\n",
    "\n",
    "# Create CrossValidator\n",
    "cv = CrossValidator(\n",
    "    estimator=rf_tuning_pipeline,\n",
    "    estimatorParamMaps=paramGrid,\n",
    "    evaluator=evaluator,\n",
    "    numFolds=3,  # 3-fold CV\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "print(\"  ✓ Cross Validator created with 3-fold CV\")\n",
    "\n",
    "# Bước 4: Run Hyperparameter Tuning\n",
    "print(\"\\n4. RUNNING HYPERPARAMETER TUNING...\")\n",
    "print(\"  This may take several minutes...\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Fit the model\n",
    "cv_model = cv.fit(train_data)\n",
    "\n",
    "tuning_time = time.time() - start_time\n",
    "\n",
    "print(f\"  ✓ Hyperparameter tuning completed in {tuning_time:.2f} seconds\")\n",
    "\n",
    "# Bước 5: Get Best Model\n",
    "print(\"\\n5. GETTING BEST MODEL...\")\n",
    "\n",
    "# Get best model\n",
    "best_model = cv_model.bestModel\n",
    "best_params = best_model.stages[-1].extractParamMap()\n",
    "\n",
    "# Get best score\n",
    "best_score = cv_model.avgMetrics[cv_model.bestIndex]\n",
    "\n",
    "print(f\"  ✓ Best model found!\")\n",
    "print(f\"  Best AUC: {best_score:.4f}\")\n",
    "\n",
    "# Bước 6: Show Best Parameters\n",
    "print(\"\\n6. BEST PARAMETERS:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "for param, value in best_params.items():\n",
    "    print(f\"  {param.name}: {value}\")\n",
    "\n",
    "# Bước 7: Evaluate Best Model on Test Set\n",
    "print(\"\\n7. EVALUATING BEST MODEL ON TEST SET...\")\n",
    "\n",
    "# Make predictions\n",
    "best_predictions = best_model.transform(test_data)\n",
    "\n",
    "# Evaluate\n",
    "best_auc = evaluator.evaluate(best_predictions)\n",
    "\n",
    "print(f\"  ✓ Best model test AUC: {best_auc:.4f}\")\n",
    "\n",
    "# Bước 8: Compare with Previous Models\n",
    "print(\"\\n8. COMPARISON WITH PREVIOUS MODELS:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"{'Model':<30} {'AUC':<10} {'Improvement':<15}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Get previous best\n",
    "previous_best_auc = max(all_models.values())\n",
    "improvement = best_auc - previous_best_auc\n",
    "\n",
    "print(f\"{'Previous Best':<30} {previous_best_auc:<10.4f} {'-':<15}\")\n",
    "print(f\"{'Tuned Random Forest':<30} {best_auc:<10.4f} {improvement:+.4f}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if improvement > 0:\n",
    "    print(f\"  ✓ Improvement: +{improvement:.4f} AUC\")\n",
    "else:\n",
    "    print(f\"  ⚠ No improvement: {improvement:.4f} AUC\")\n",
    "\n",
    "# Bước 9: Feature Importance of Best Model\n",
    "print(\"\\n9. FEATURE IMPORTANCE OF BEST MODEL:\")\n",
    "\n",
    "best_feature_importance = best_model.stages[-1].featureImportances.toArray()\n",
    "best_importance_df = spark.createDataFrame([\n",
    "    (feature_cols_final[i], float(best_feature_importance[i]))\n",
    "    for i in range(len(feature_cols_final))\n",
    "], [\"feature\", \"importance\"]).orderBy(col(\"importance\").desc())\n",
    "\n",
    "print(\"Top 10 most important features (Tuned Random Forest):\")\n",
    "best_importance_df.show(10, truncate=False)\n",
    "\n",
    "# Bước 10: Save Best Model\n",
    "print(\"\\n10. SAVING BEST MODEL...\")\n",
    "\n",
    "# Save the best model\n",
    "best_model.write().overwrite().save(\"./model/best_random_forest_model\")\n",
    "\n",
    "print(\"  ✓ Best model saved to ./model/best_random_forest_model\")\n",
    "\n",
    "print(f\"\\n✓ Hyperparameter tuning completed!\")\n",
    "print(f\"  Best model: Tuned Random Forest\")\n",
    "print(f\"  Best AUC: {best_auc:.4f}\")\n",
    "print(f\"  Tuning time: {tuning_time:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc189a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Final Model Comparison and Selection\n",
    "print(\"=\" * 70)\n",
    "print(\"SO SÁNH KẾT QUẢ VÀ CHỌN MÔ HÌNH TỐT NHẤT\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Bước 1: Collect All Results\n",
    "print(\"\\n1. COLLECTING ALL MODEL RESULTS...\")\n",
    "\n",
    "# Update all_models with tuned model\n",
    "all_models['Tuned Random Forest'] = best_auc\n",
    "\n",
    "# Create comprehensive results table\n",
    "results_data = []\n",
    "for model_name, auc_score in all_models.items():\n",
    "    results_data.append((model_name, auc_score))\n",
    "\n",
    "results_df = spark.createDataFrame(results_data, [\"Model\", \"AUC\"]).orderBy(col(\"AUC\").desc())\n",
    "\n",
    "print(\"  ✓ All model results collected\")\n",
    "\n",
    "# Bước 2: Display Final Results Table\n",
    "print(\"\\n2. FINAL MODEL COMPARISON:\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"{'Rank':<5} {'Model':<35} {'AUC':<10} {'Performance':<15}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "# Show ranked results\n",
    "ranked_results = results_df.collect()\n",
    "for i, row in enumerate(ranked_results, 1):\n",
    "    model_name = row['Model']\n",
    "    auc_score = row['AUC']\n",
    "    \n",
    "    # Performance category\n",
    "    if auc_score >= 0.9:\n",
    "        performance = \"Excellent\"\n",
    "    elif auc_score >= 0.8:\n",
    "        performance = \"Good\"\n",
    "    elif auc_score >= 0.7:\n",
    "        performance = \"Fair\"\n",
    "    else:\n",
    "        performance = \"Poor\"\n",
    "    \n",
    "    print(f\"{i:<5} {model_name:<35} {auc_score:<10.4f} {performance:<15}\")\n",
    "\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Bước 3: Statistical Analysis\n",
    "print(\"\\n3. STATISTICAL ANALYSIS:\")\n",
    "\n",
    "# Calculate statistics\n",
    "auc_scores = [row['AUC'] for row in ranked_results]\n",
    "mean_auc = sum(auc_scores) / len(auc_scores)\n",
    "max_auc = max(auc_scores)\n",
    "min_auc = min(auc_scores)\n",
    "std_auc = (sum([(x - mean_auc) ** 2 for x in auc_scores]) / len(auc_scores)) ** 0.5\n",
    "\n",
    "print(f\"  Mean AUC: {mean_auc:.4f}\")\n",
    "print(f\"  Max AUC:  {max_auc:.4f}\")\n",
    "print(f\"  Min AUC:  {min_auc:.4f}\")\n",
    "print(f\"  Std AUC:  {std_auc:.4f}\")\n",
    "\n",
    "# Bước 4: Best Model Analysis\n",
    "print(\"\\n4. BEST MODEL ANALYSIS:\")\n",
    "\n",
    "best_model_name = ranked_results[0]['Model']\n",
    "best_auc_score = ranked_results[0]['AUC']\n",
    "\n",
    "print(f\"  🏆 BEST MODEL: {best_model_name}\")\n",
    "print(f\"  🎯 BEST AUC: {best_auc_score:.4f}\")\n",
    "\n",
    "# Performance interpretation\n",
    "if best_auc_score >= 0.9:\n",
    "    print(\"  📈 Performance: EXCELLENT - Model can reliably predict crisis periods\")\n",
    "elif best_auc_score >= 0.8:\n",
    "    print(\"  📈 Performance: GOOD - Model shows strong predictive capability\")\n",
    "elif best_auc_score >= 0.7:\n",
    "    print(\"  📈 Performance: FAIR - Model has moderate predictive power\")\n",
    "else:\n",
    "    print(\"  📈 Performance: POOR - Model needs improvement\")\n",
    "\n",
    "# Bước 5: Feature Importance Analysis (Best Model)\n",
    "print(\"\\n5. FEATURE IMPORTANCE ANALYSIS (BEST MODEL):\")\n",
    "\n",
    "if best_model_name == 'Tuned Random Forest':\n",
    "    # Use the tuned model's feature importance\n",
    "    best_importance_df = best_importance_df\n",
    "else:\n",
    "    # Get feature importance from the best model\n",
    "    if 'Random Forest' in best_model_name:\n",
    "        if best_model_name == 'Advanced Random Forest':\n",
    "            best_importance_df = rf_importance_df\n",
    "        elif best_model_name == 'Simple Random Forest':\n",
    "            best_importance_df = feature_importance_df\n",
    "        else:\n",
    "            best_importance_df = rf_scaled_model.stages[-1].featureImportances.toArray()\n",
    "            best_importance_df = spark.createDataFrame([\n",
    "                (feature_cols_final[i], float(best_importance_df[i]))\n",
    "                for i in range(len(feature_cols_final))\n",
    "            ], [\"feature\", \"importance\"]).orderBy(col(\"importance\").desc())\n",
    "    else:\n",
    "        print(\"  Feature importance not available for this model type\")\n",
    "        best_importance_df = None\n",
    "\n",
    "if best_importance_df is not None:\n",
    "    print(\"\\n  Top 10 Most Important Features:\")\n",
    "    best_importance_df.show(10, truncate=False)\n",
    "    \n",
    "    # Get top 5 features\n",
    "    top_5_features = best_importance_df.limit(5).collect()\n",
    "    print(\"\\n  🎯 Top 5 Most Critical Features for Crisis Prediction:\")\n",
    "    for i, row in enumerate(top_5_features, 1):\n",
    "        print(f\"    {i}. {row['feature']} (Importance: {row['importance']:.4f})\")\n",
    "\n",
    "# Bước 6: Model Recommendations\n",
    "print(\"\\n6. MODEL RECOMMENDATIONS:\")\n",
    "\n",
    "print(\"  📋 RECOMMENDATIONS:\")\n",
    "print(\"    ✓ Use the best performing model for production\")\n",
    "print(\"    ✓ Monitor model performance over time\")\n",
    "print(\"    ✓ Retrain model with new data periodically\")\n",
    "print(\"    ✓ Consider ensemble methods for improved accuracy\")\n",
    "\n",
    "if best_auc_score < 0.8:\n",
    "    print(\"    ⚠ Consider collecting more features or data\")\n",
    "    print(\"    ⚠ Try different algorithms or ensemble methods\")\n",
    "\n",
    "# Bước 7: Business Impact Analysis\n",
    "print(\"\\n7. BUSINESS IMPACT ANALYSIS:\")\n",
    "\n",
    "print(\"  💼 BUSINESS IMPACT:\")\n",
    "print(f\"    • Model can predict crisis periods with {best_auc_score:.1%} accuracy\")\n",
    "print(\"    • Early warning system for aviation industry\")\n",
    "print(\"    • Helps in strategic planning and risk management\")\n",
    "print(\"    • Supports decision making during uncertain times\")\n",
    "\n",
    "# Bước 8: Next Steps\n",
    "print(\"\\n8. NEXT STEPS:\")\n",
    "\n",
    "print(\"  🚀 NEXT STEPS:\")\n",
    "print(\"    1. Deploy the best model to production\")\n",
    "print(\"    2. Set up monitoring and alerting systems\")\n",
    "print(\"    3. Create dashboard for real-time crisis prediction\")\n",
    "print(\"    4. Integrate with existing business systems\")\n",
    "print(\"    5. Plan for model retraining schedule\")\n",
    "\n",
    "# Bước 9: Final Summary\n",
    "print(\"\\n9. FINAL SUMMARY:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(f\"🎯 EXPERIMENT COMPLETED SUCCESSFULLY!\")\n",
    "print(f\"📊 Total models tested: {len(all_models)}\")\n",
    "print(f\"🏆 Best model: {best_model_name}\")\n",
    "print(f\"🎯 Best AUC: {best_auc_score:.4f}\")\n",
    "print(f\"⏱️ Total experiment time: ~{sum([lr_time, linear_time, rf_simple_time, rf_advanced_time, gbt_time, rf_scaled_time, gbt_scaled_time, tuning_time]):.0f} seconds\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"✅ CRISIS PREDICTION MODEL READY FOR DEPLOYMENT!\")\n",
    "print(\"=\" * 50)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
