{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "cb0ea879",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset downloaded to: C:\\Users\\ADMIN\\.cache\\kagglehub\\datasets\\bhavikjikadara\\us-airline-flight-routes-and-fares-1993-2024\\versions\\1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.metrics import log_loss, accuracy_score, classification_report\n",
    "import kagglehub\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "\n",
    "\n",
    "# T·∫£i dataset v·ªÅ (s·∫Ω tr·∫£ v·ªÅ ƒë∆∞·ªùng d·∫´n local)\n",
    "path = kagglehub.dataset_download(\"bhavikjikadara/us-airline-flight-routes-and-fares-1993-2024\")\n",
    "\n",
    "print(\"Dataset downloaded to:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "ab2d109e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(245955, 23)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tbl</th>\n",
       "      <th>Year</th>\n",
       "      <th>quarter</th>\n",
       "      <th>citymarketid_1</th>\n",
       "      <th>citymarketid_2</th>\n",
       "      <th>city1</th>\n",
       "      <th>city2</th>\n",
       "      <th>airportid_1</th>\n",
       "      <th>airportid_2</th>\n",
       "      <th>airport_1</th>\n",
       "      <th>...</th>\n",
       "      <th>fare</th>\n",
       "      <th>carrier_lg</th>\n",
       "      <th>large_ms</th>\n",
       "      <th>fare_lg</th>\n",
       "      <th>carrier_low</th>\n",
       "      <th>lf_ms</th>\n",
       "      <th>fare_low</th>\n",
       "      <th>Geocoded_City1</th>\n",
       "      <th>Geocoded_City2</th>\n",
       "      <th>tbl1apk</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Table1a</td>\n",
       "      <td>2021</td>\n",
       "      <td>3</td>\n",
       "      <td>30135</td>\n",
       "      <td>33195</td>\n",
       "      <td>Allentown/Bethlehem/Easton, PA</td>\n",
       "      <td>Tampa, FL (Metropolitan Area)</td>\n",
       "      <td>10135</td>\n",
       "      <td>14112</td>\n",
       "      <td>ABE</td>\n",
       "      <td>...</td>\n",
       "      <td>81.43</td>\n",
       "      <td>G4</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>81.43</td>\n",
       "      <td>G4</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>81.43</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>202131013514112ABEPIE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Table1a</td>\n",
       "      <td>2021</td>\n",
       "      <td>3</td>\n",
       "      <td>30135</td>\n",
       "      <td>33195</td>\n",
       "      <td>Allentown/Bethlehem/Easton, PA</td>\n",
       "      <td>Tampa, FL (Metropolitan Area)</td>\n",
       "      <td>10135</td>\n",
       "      <td>15304</td>\n",
       "      <td>ABE</td>\n",
       "      <td>...</td>\n",
       "      <td>208.93</td>\n",
       "      <td>DL</td>\n",
       "      <td>0.4659</td>\n",
       "      <td>219.98</td>\n",
       "      <td>UA</td>\n",
       "      <td>0.1193</td>\n",
       "      <td>154.11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>202131013515304ABETPA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Table1a</td>\n",
       "      <td>2021</td>\n",
       "      <td>3</td>\n",
       "      <td>30140</td>\n",
       "      <td>30194</td>\n",
       "      <td>Albuquerque, NM</td>\n",
       "      <td>Dallas/Fort Worth, TX</td>\n",
       "      <td>10140</td>\n",
       "      <td>11259</td>\n",
       "      <td>ABQ</td>\n",
       "      <td>...</td>\n",
       "      <td>184.56</td>\n",
       "      <td>WN</td>\n",
       "      <td>0.9968</td>\n",
       "      <td>184.44</td>\n",
       "      <td>WN</td>\n",
       "      <td>0.9968</td>\n",
       "      <td>184.44</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>202131014011259ABQDAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Table1a</td>\n",
       "      <td>2021</td>\n",
       "      <td>3</td>\n",
       "      <td>30140</td>\n",
       "      <td>30194</td>\n",
       "      <td>Albuquerque, NM</td>\n",
       "      <td>Dallas/Fort Worth, TX</td>\n",
       "      <td>10140</td>\n",
       "      <td>11298</td>\n",
       "      <td>ABQ</td>\n",
       "      <td>...</td>\n",
       "      <td>182.64</td>\n",
       "      <td>AA</td>\n",
       "      <td>0.9774</td>\n",
       "      <td>183.09</td>\n",
       "      <td>AA</td>\n",
       "      <td>0.9774</td>\n",
       "      <td>183.09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>202131014011298ABQDFW</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Table1a</td>\n",
       "      <td>2021</td>\n",
       "      <td>3</td>\n",
       "      <td>30140</td>\n",
       "      <td>30466</td>\n",
       "      <td>Albuquerque, NM</td>\n",
       "      <td>Phoenix, AZ</td>\n",
       "      <td>10140</td>\n",
       "      <td>14107</td>\n",
       "      <td>ABQ</td>\n",
       "      <td>...</td>\n",
       "      <td>177.11</td>\n",
       "      <td>WN</td>\n",
       "      <td>0.6061</td>\n",
       "      <td>184.49</td>\n",
       "      <td>AA</td>\n",
       "      <td>0.3939</td>\n",
       "      <td>165.77</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>202131014014107ABQPHX</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows √ó 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       tbl  Year  quarter  citymarketid_1  citymarketid_2  \\\n",
       "0  Table1a  2021        3           30135           33195   \n",
       "1  Table1a  2021        3           30135           33195   \n",
       "2  Table1a  2021        3           30140           30194   \n",
       "3  Table1a  2021        3           30140           30194   \n",
       "4  Table1a  2021        3           30140           30466   \n",
       "\n",
       "                            city1                          city2  airportid_1  \\\n",
       "0  Allentown/Bethlehem/Easton, PA  Tampa, FL (Metropolitan Area)        10135   \n",
       "1  Allentown/Bethlehem/Easton, PA  Tampa, FL (Metropolitan Area)        10135   \n",
       "2                 Albuquerque, NM          Dallas/Fort Worth, TX        10140   \n",
       "3                 Albuquerque, NM          Dallas/Fort Worth, TX        10140   \n",
       "4                 Albuquerque, NM                    Phoenix, AZ        10140   \n",
       "\n",
       "   airportid_2 airport_1  ...    fare  carrier_lg  large_ms  fare_lg  \\\n",
       "0        14112       ABE  ...   81.43          G4    1.0000    81.43   \n",
       "1        15304       ABE  ...  208.93          DL    0.4659   219.98   \n",
       "2        11259       ABQ  ...  184.56          WN    0.9968   184.44   \n",
       "3        11298       ABQ  ...  182.64          AA    0.9774   183.09   \n",
       "4        14107       ABQ  ...  177.11          WN    0.6061   184.49   \n",
       "\n",
       "  carrier_low   lf_ms  fare_low Geocoded_City1  Geocoded_City2  \\\n",
       "0          G4  1.0000     81.43            NaN             NaN   \n",
       "1          UA  0.1193    154.11            NaN             NaN   \n",
       "2          WN  0.9968    184.44            NaN             NaN   \n",
       "3          AA  0.9774    183.09            NaN             NaN   \n",
       "4          AA  0.3939    165.77            NaN             NaN   \n",
       "\n",
       "                 tbl1apk  \n",
       "0  202131013514112ABEPIE  \n",
       "1  202131013515304ABETPA  \n",
       "2  202131014011259ABQDAL  \n",
       "3  202131014011298ABQDFW  \n",
       "4  202131014014107ABQPHX  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(path + \"/US Airline Flight Routes and Fares 1993-2024.csv\")\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "f46e6605",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tbl', 'Year', 'quarter', 'citymarketid_1', 'citymarketid_2', 'city1', 'city2', 'airportid_1', 'airportid_2', 'airport_1', 'airport_2', 'nsmiles', 'passengers', 'fare', 'carrier_lg', 'large_ms', 'fare_lg', 'carrier_low', 'lf_ms', 'fare_low', 'Geocoded_City1', 'Geocoded_City2', 'tbl1apk']\n"
     ]
    }
   ],
   "source": [
    "print(df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "43cda9e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Gi√° tr·ªã thi·∫øu theo c·ªôt:\n",
      "                Missing_Count    Percent\n",
      "carrier_lg               1540   0.626131\n",
      "large_ms                 1540   0.626131\n",
      "fare_lg                  1540   0.626131\n",
      "carrier_low              1612   0.655404\n",
      "lf_ms                    1612   0.655404\n",
      "fare_low                 1612   0.655404\n",
      "Geocoded_City1          39206  15.940314\n",
      "Geocoded_City2          39206  15.940314\n"
     ]
    }
   ],
   "source": [
    "# ==================== 1. DATA CLEANING ====================\n",
    "# Ki·ªÉm tra missing values\n",
    "print(f\"\\n\\nGi√° tr·ªã thi·∫øu theo c·ªôt:\")\n",
    "missing = df.isnull().sum()\n",
    "missing_percent = 100 * missing / len(df)\n",
    "missing_table = pd.DataFrame({\n",
    "    'Missing_Count': missing,\n",
    "    'Percent': missing_percent\n",
    "})\n",
    "print(missing_table[missing_table['Missing_Count'] > 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "c7815d88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "X·ª≠ l√Ω gi√° tr·ªã thi·∫øu...\n",
      "  - ƒêi·ªÅn large_ms b·∫±ng median\n",
      "  - ƒêi·ªÅn fare_lg b·∫±ng median\n",
      "  - ƒêi·ªÅn lf_ms b·∫±ng median\n",
      "  - ƒêi·ªÅn fare_low b·∫±ng median\n",
      "  - ƒêi·ªÅn carrier_lg b·∫±ng mode\n",
      "  - ƒêi·ªÅn carrier_low b·∫±ng mode\n",
      "  - ƒêi·ªÅn Geocoded_City1 b·∫±ng mode\n",
      "  - ƒêi·ªÅn Geocoded_City2 b·∫±ng mode\n",
      "\n",
      "\n",
      "S·ªë d√≤ng tr√πng l·∫∑p: 0\n",
      "\n",
      "\n",
      "X·ª≠ l√Ω outliers...\n",
      "  - passengers: ph√°t hi·ªán 12284 outliers\n",
      "  - fare: ph√°t hi·ªán 857 outliers\n",
      "  - fare_lg: ph√°t hi·ªán 786 outliers\n",
      "  - fare_low: ph√°t hi·ªán 925 outliers\n",
      "\n",
      "\n",
      "Ki·ªÉm tra t√≠nh h·ª£p l·ªá...\n",
      "  - large_ms kh√¥ng h·ª£p l·ªá: 0\n",
      "  - lf_ms kh√¥ng h·ª£p l·ªá: 0\n",
      "\n",
      "\n",
      "K√≠ch th∆∞·ªõc d·ªØ li·ªáu sau khi l√†m s·∫°ch: (238516, 23)\n"
     ]
    }
   ],
   "source": [
    "# X·ª≠ l√Ω missing values\n",
    "print(\"\\n\\nX·ª≠ l√Ω gi√° tr·ªã thi·∫øu...\")\n",
    "# ƒêi·ªÅn gi√° tr·ªã thi·∫øu cho c√°c c·ªôt s·ªë b·∫±ng median\n",
    "numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "for col in numeric_cols:\n",
    "    if df[col].isnull().sum() > 0:\n",
    "        df[col].fillna(df[col].median(), inplace=True)\n",
    "        print(f\"  - ƒêi·ªÅn {col} b·∫±ng median\")\n",
    "\n",
    "# ƒêi·ªÅn gi√° tr·ªã thi·∫øu cho c√°c c·ªôt text b·∫±ng mode ho·∫∑c 'Unknown'\n",
    "text_cols = df.select_dtypes(include=['object']).columns\n",
    "for col in text_cols:\n",
    "    if df[col].isnull().sum() > 0:\n",
    "        if df[col].mode().shape[0] > 0:\n",
    "            df[col].fillna(df[col].mode()[0], inplace=True)\n",
    "            print(f\"  - ƒêi·ªÅn {col} b·∫±ng mode\")\n",
    "        else:\n",
    "            df[col].fillna('Unknown', inplace=True)\n",
    "            print(f\"  - ƒêi·ªÅn {col} b·∫±ng 'Unknown'\")\n",
    "\n",
    "# Lo·∫°i b·ªè duplicates\n",
    "duplicates = df.duplicated().sum()\n",
    "print(f\"\\n\\nS·ªë d√≤ng tr√πng l·∫∑p: {duplicates}\")\n",
    "if duplicates > 0:\n",
    "    df = df.drop_duplicates()\n",
    "    print(f\"ƒê√£ lo·∫°i b·ªè {duplicates} d√≤ng tr√πng l·∫∑p\")\n",
    "\n",
    "# Ki·ªÉm tra v√† x·ª≠ l√Ω outliers cho c√°c c·ªôt quan tr·ªçng\n",
    "print(\"\\n\\nX·ª≠ l√Ω outliers...\")\n",
    "outlier_cols = ['passengers', 'fare', 'large_ms', 'lf_ms', 'fare_lg', 'fare_low']\n",
    "for col in outlier_cols:\n",
    "    if col in df.columns:\n",
    "        Q1 = df[col].quantile(0.25)\n",
    "        Q3 = df[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower = Q1 - 3 * IQR\n",
    "        upper = Q3 + 3 * IQR\n",
    "        outliers = ((df[col] < lower) | (df[col] > upper)).sum()\n",
    "        if outliers > 0:\n",
    "            print(f\"  - {col}: ph√°t hi·ªán {outliers} outliers\")\n",
    "            # Cap outliers thay v√¨ lo·∫°i b·ªè\n",
    "            df[col] = df[col].clip(lower=lower, upper=upper)\n",
    "\n",
    "# Ki·ªÉm tra t√≠nh h·ª£p l·ªá c·ªßa d·ªØ li·ªáu\n",
    "print(\"\\n\\nKi·ªÉm tra t√≠nh h·ª£p l·ªá...\")\n",
    "# Market share ph·∫£i t·ª´ 0-100\n",
    "if 'large_ms' in df.columns:\n",
    "    invalid_ms = ((df['large_ms'] < 0) | (df['large_ms'] > 100)).sum()\n",
    "    print(f\"  - large_ms kh√¥ng h·ª£p l·ªá: {invalid_ms}\")\n",
    "    df = df[(df['large_ms'] >= 0) & (df['large_ms'] <= 100)]\n",
    "\n",
    "if 'lf_ms' in df.columns:\n",
    "    invalid_ms = ((df['lf_ms'] < 0) | (df['lf_ms'] > 100)).sum()\n",
    "    print(f\"  - lf_ms kh√¥ng h·ª£p l·ªá: {invalid_ms}\")\n",
    "    df = df[(df['lf_ms'] >= 0) & (df['lf_ms'] <= 100)]\n",
    "\n",
    "# Passengers v√† fare ph·∫£i > 0\n",
    "if 'passengers' in df.columns:\n",
    "    df = df[df['passengers'] > 0]\n",
    "if 'fare' in df.columns:\n",
    "    df = df[df['fare'] > 0]\n",
    "\n",
    "print(f\"\\n\\nK√≠ch th∆∞·ªõc d·ªØ li·ªáu sau khi l√†m s·∫°ch: {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "cec27a75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "B∆Ø·ªöC 2: PH√ÇN T√çCH KH√ÅM PH√Å D·ªÆ LI·ªÜU (EDA)\n",
      "================================================================================\n",
      "\n",
      "\n",
      "Th·ªëng k√™ m√¥ t·∫£ c√°c bi·∫øn s·ªë:\n",
      "                Year        quarter  citymarketid_1  citymarketid_2  \\\n",
      "count  238516.000000  238516.000000   238516.000000   238516.000000   \n",
      "mean     2008.617594       2.482010    31556.478702    32175.443128   \n",
      "std         8.672414       1.122017     1093.940628     1231.943367   \n",
      "min      1993.000000       1.000000    30135.000000    30189.000000   \n",
      "25%      2001.000000       1.000000    30721.000000    30977.000000   \n",
      "50%      2009.000000       2.000000    31295.000000    32211.000000   \n",
      "75%      2016.000000       3.000000    32467.000000    33192.000000   \n",
      "max      2024.000000       4.000000    35412.000000    35628.000000   \n",
      "\n",
      "         airportid_1    airportid_2        nsmiles     passengers  \\\n",
      "count  238516.000000  238516.000000  238516.000000  238516.000000   \n",
      "mean    12436.939715   13239.666890    1189.284308     270.087508   \n",
      "std      1430.439257    1425.274798     698.992628     355.415816   \n",
      "min     10135.000000   10466.000000     109.000000       1.000000   \n",
      "25%     11193.000000   12197.000000     632.000000      26.000000   \n",
      "50%     12266.000000   13303.000000    1021.000000     121.000000   \n",
      "75%     13487.000000   14679.000000    1729.000000     352.000000   \n",
      "max     16440.000000   15919.000000    2724.000000    1293.000000   \n",
      "\n",
      "                fare       large_ms        fare_lg          lf_ms  \\\n",
      "count  238516.000000  238516.000000  238516.000000  238516.000000   \n",
      "mean      216.777972       0.659712     217.045374       0.439273   \n",
      "std        73.939666       0.221957      78.598980       0.327516   \n",
      "min        50.410000       0.003800      50.410000       0.010000   \n",
      "25%       164.820000       0.480000     161.860000       0.150000   \n",
      "50%       208.880000       0.650000     207.900000       0.350000   \n",
      "75%       261.362500       0.860000     262.212500       0.724425   \n",
      "max       557.700000       1.000000     567.340000       1.000000   \n",
      "\n",
      "            fare_low  \n",
      "count  238516.000000  \n",
      "mean      188.551963  \n",
      "std        66.676061  \n",
      "min        50.100000  \n",
      "25%       140.180000  \n",
      "50%       181.310000  \n",
      "75%       228.380000  \n",
      "max       497.460000  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ==================== 2. EXPLORATORY DATA ANALYSIS ====================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"B∆Ø·ªöC 2: PH√ÇN T√çCH KH√ÅM PH√Å D·ªÆ LI·ªÜU (EDA)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Th·ªëng k√™ m√¥ t·∫£\n",
    "print(\"\\n\\nTh·ªëng k√™ m√¥ t·∫£ c√°c bi·∫øn s·ªë:\")\n",
    "print(df[numeric_cols].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb5781e1",
   "metadata": {},
   "source": [
    "# üöÄ PH√ÇN LO·∫†I TUY·∫æN BAY THEO TI·ªÄM NƒÇNG PH√ÅT TRI·ªÇN\n",
    "\n",
    "## M·ª•c ti√™u:\n",
    "- Ph√¢n lo·∫°i c√°c tuy·∫øn bay th√†nh 3 nh√≥m: **Cao**, **Trung b√¨nh**, **Th·∫•p** ti·ªÅm nƒÉng\n",
    "- S·ª≠ d·ª•ng Machine Learning ƒë·ªÉ d·ª± ƒëo√°n ti·ªÅm nƒÉng ph√°t tri·ªÉn\n",
    "- ƒê∆∞a ra khuy·∫øn ngh·ªã chi·∫øn l∆∞·ª£c cho t·ª´ng nh√≥m tuy·∫øn bay\n",
    "\n",
    "## Ti√™u ch√≠ ƒë√°nh gi√° ti·ªÅm nƒÉng:\n",
    "1. **L∆∞u l∆∞·ª£ng h√†nh kh√°ch** (passengers)\n",
    "2. **Gi√° v√© trung b√¨nh** (fare) \n",
    "3. **Kho·∫£ng c√°ch** (nsmiles)\n",
    "4. **Th·ªã ph·∫ßn h√£ng l·ªõn** (large_ms)\n",
    "5. **Th·ªã ph·∫ßn h√£ng gi√° r·∫ª** (lf_ms)\n",
    "6. **Xu h∆∞·ªõng tƒÉng tr∆∞·ªüng** (d·ª±a tr√™n d·ªØ li·ªáu theo th·ªùi gian)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "130ce71a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ B∆Ø·ªöC 1: T√çNH TO√ÅN ƒêI·ªÇM S·ªê TI·ªÄM NƒÇNG\n",
      "==================================================\n",
      "\n",
      "üìä Ph√¢n b·ªë ti·ªÅm nƒÉng tuy·∫øn bay:\n",
      "potential_level\n",
      "Th·∫•p          155683\n",
      "Trung b√¨nh     82211\n",
      "Cao              622\n",
      "Name: count, dtype: int64\n",
      "\n",
      "T·ª∑ l·ªá ph·∫ßn trƒÉm:\n",
      "potential_level\n",
      "Th·∫•p          65.271512\n",
      "Trung b√¨nh    34.467709\n",
      "Cao            0.260779\n",
      "Name: count, dtype: float64\n",
      "\n",
      "üìà Th·ªëng k√™ m√¥ t·∫£ theo nh√≥m ti·ªÅm nƒÉng:\n",
      "                 passengers    fare  nsmiles  large_ms  lf_ms\n",
      "potential_level                                              \n",
      "Cao                 1249.41  338.12  2269.31      0.70   0.47\n",
      "Th·∫•p                 137.99  209.64  1119.24      0.61   0.35\n",
      "Trung b√¨nh           512.83  229.37  1313.75      0.75   0.60\n"
     ]
    }
   ],
   "source": [
    "# ==========================\n",
    "# B∆Ø·ªöC 1: T√çNH TO√ÅN ƒêI·ªÇM S·ªê TI·ªÄM NƒÇNG (POTENTIAL SCORE)\n",
    "# ==========================\n",
    "\n",
    "print(\"üéØ B∆Ø·ªöC 1: T√çNH TO√ÅN ƒêI·ªÇM S·ªê TI·ªÄM NƒÇNG\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# T·∫°o b·∫£n sao d·ªØ li·ªáu ƒë·ªÉ x·ª≠ l√Ω\n",
    "df_potential = df.copy()\n",
    "\n",
    "# 1. Chu·∫©n h√≥a c√°c ch·ªâ s·ªë (0-1 scale)\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# C√°c ch·ªâ s·ªë c·∫ßn chu·∫©n h√≥a\n",
    "metrics = ['passengers', 'fare', 'nsmiles', 'large_ms', 'lf_ms']\n",
    "\n",
    "# Chu·∫©n h√≥a t·ª´ng ch·ªâ s·ªë\n",
    "scaler = MinMaxScaler()\n",
    "for metric in metrics:\n",
    "    df_potential[f'{metric}_normalized'] = scaler.fit_transform(df_potential[[metric]])\n",
    "\n",
    "# 2. T√≠nh ƒëi·ªÉm s·ªë ti·ªÅm nƒÉng d·ª±a tr√™n tr·ªçng s·ªë\n",
    "# Tr·ªçng s·ªë: passengers (30%), fare (25%), nsmiles (15%), large_ms (15%), lf_ms (15%)\n",
    "weights = {\n",
    "    'passengers_normalized': 0.30,  # L∆∞u l∆∞·ª£ng h√†nh kh√°ch - quan tr·ªçng nh·∫•t\n",
    "    'fare_normalized': 0.25,        # Gi√° v√© - ch·ªâ s·ªë l·ª£i nhu·∫≠n\n",
    "    'nsmiles_normalized': 0.15,     # Kho·∫£ng c√°ch - ·∫£nh h∆∞·ªüng ƒë·∫øn chi ph√≠\n",
    "    'large_ms_normalized': 0.15,    # Th·ªã ph·∫ßn h√£ng l·ªõn - ·ªïn ƒë·ªãnh\n",
    "    'lf_ms_normalized': 0.15        # Th·ªã ph·∫ßn h√£ng gi√° r·∫ª - c·∫°nh tranh\n",
    "}\n",
    "\n",
    "# T√≠nh ƒëi·ªÉm s·ªë ti·ªÅm nƒÉng\n",
    "df_potential['potential_score'] = 0\n",
    "for metric, weight in weights.items():\n",
    "    df_potential['potential_score'] += df_potential[metric] * weight\n",
    "\n",
    "# 3. Ph√¢n lo·∫°i theo ƒëi·ªÉm s·ªë ti·ªÅm nƒÉng\n",
    "# Cao: >= 0.7, Trung b√¨nh: 0.4-0.7, Th·∫•p: < 0.4\n",
    "def classify_potential(score):\n",
    "    if score >= 0.7:\n",
    "        return 'Cao'\n",
    "    elif score >= 0.4:\n",
    "        return 'Trung b√¨nh'\n",
    "    else:\n",
    "        return 'Th·∫•p'\n",
    "\n",
    "df_potential['potential_level'] = df_potential['potential_score'].apply(classify_potential)\n",
    "\n",
    "# 4. Th·ªëng k√™ ph√¢n b·ªë\n",
    "print(\"\\nüìä Ph√¢n b·ªë ti·ªÅm nƒÉng tuy·∫øn bay:\")\n",
    "potential_dist = df_potential['potential_level'].value_counts()\n",
    "print(potential_dist)\n",
    "print(f\"\\nT·ª∑ l·ªá ph·∫ßn trƒÉm:\")\n",
    "print(potential_dist / len(df_potential) * 100)\n",
    "\n",
    "# 5. Th·ªëng k√™ m√¥ t·∫£ theo t·ª´ng nh√≥m ti·ªÅm nƒÉng\n",
    "print(\"\\nüìà Th·ªëng k√™ m√¥ t·∫£ theo nh√≥m ti·ªÅm nƒÉng:\")\n",
    "potential_stats = df_potential.groupby('potential_level')[['passengers', 'fare', 'nsmiles', 'large_ms', 'lf_ms']].mean()\n",
    "print(potential_stats.round(2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "2d3b1d73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üéØ B∆Ø·ªöC 2: PH√ÇN T√çCH XU H∆Ø·ªöNG TƒÇNG TR∆Ø·ªûNG\n",
      "==================================================\n",
      "üìà Ph√¢n b·ªë ti·ªÅm nƒÉng sau khi c·∫£i ti·∫øn:\n",
      "potential_level_enhanced\n",
      "Th·∫•p          186251\n",
      "Trung b√¨nh     52188\n",
      "Cao               77\n",
      "Name: count, dtype: int64\n",
      "\n",
      "T·ª∑ l·ªá ph·∫ßn trƒÉm:\n",
      "potential_level_enhanced\n",
      "Th·∫•p          78.087424\n",
      "Trung b√¨nh    21.880293\n",
      "Cao            0.032283\n",
      "Name: count, dtype: float64\n",
      "\n",
      "üîÑ So s√°nh ph√¢n lo·∫°i tr∆∞·ªõc v√† sau c·∫£i ti·∫øn:\n",
      "potential_level_enhanced  Cao    Th·∫•p  Trung b√¨nh     All\n",
      "potential_level                                          \n",
      "Cao                        75       0         547     622\n",
      "Th·∫•p                        0  155489         194  155683\n",
      "Trung b√¨nh                  2   30762       51447   82211\n",
      "All                        77  186251       52188  238516\n"
     ]
    }
   ],
   "source": [
    "# ==========================\n",
    "# B∆Ø·ªöC 2: PH√ÇN T√çCH XU H∆Ø·ªöNG TƒÇNG TR∆Ø·ªûNG THEO TH·ªúI GIAN\n",
    "# ==========================\n",
    "\n",
    "print(\"\\nüéØ B∆Ø·ªöC 2: PH√ÇN T√çCH XU H∆Ø·ªöNG TƒÇNG TR∆Ø·ªûNG\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# T√≠nh xu h∆∞·ªõng tƒÉng tr∆∞·ªüng cho t·ª´ng tuy·∫øn bay\n",
    "def calculate_growth_trend(group):\n",
    "    \"\"\"T√≠nh xu h∆∞·ªõng tƒÉng tr∆∞·ªüng cho m·ªôt tuy·∫øn bay\"\"\"\n",
    "    if len(group) < 2:\n",
    "        return 0\n",
    "    \n",
    "    # S·∫Øp x·∫øp theo nƒÉm\n",
    "    group = group.sort_values('Year')\n",
    "    \n",
    "    # T√≠nh tƒÉng tr∆∞·ªüng passengers\n",
    "    if group['passengers'].iloc[0] > 0:\n",
    "        passenger_growth = (group['passengers'].iloc[-1] - group['passengers'].iloc[0]) / group['passengers'].iloc[0]\n",
    "    else:\n",
    "        passenger_growth = 0\n",
    "    \n",
    "    # T√≠nh tƒÉng tr∆∞·ªüng fare\n",
    "    if group['fare'].iloc[0] > 0:\n",
    "        fare_growth = (group['fare'].iloc[-1] - group['fare'].iloc[0]) / group['fare'].iloc[0]\n",
    "    else:\n",
    "        fare_growth = 0\n",
    "    \n",
    "    # Trung b√¨nh tƒÉng tr∆∞·ªüng (passengers quan tr·ªçng h∆°n)\n",
    "    growth_score = 0.7 * passenger_growth + 0.3 * fare_growth\n",
    "    return growth_score\n",
    "\n",
    "# T√≠nh xu h∆∞·ªõng tƒÉng tr∆∞·ªüng cho t·ª´ng tuy·∫øn bay\n",
    "growth_trends = df_potential.groupby(['city1', 'city2']).apply(calculate_growth_trend).reset_index()\n",
    "growth_trends.columns = ['city1', 'city2', 'growth_trend']\n",
    "\n",
    "# Merge v·ªõi d·ªØ li·ªáu ch√≠nh\n",
    "df_potential = df_potential.merge(growth_trends, on=['city1', 'city2'], how='left')\n",
    "df_potential['growth_trend'].fillna(0, inplace=True)\n",
    "\n",
    "# Chu·∫©n h√≥a growth trend\n",
    "df_potential['growth_trend_normalized'] = scaler.fit_transform(df_potential[['growth_trend']])\n",
    "\n",
    "# C·∫≠p nh·∫≠t ƒëi·ªÉm s·ªë ti·ªÅm nƒÉng v·ªõi xu h∆∞·ªõng tƒÉng tr∆∞·ªüng\n",
    "# Th√™m tr·ªçng s·ªë 10% cho xu h∆∞·ªõng tƒÉng tr∆∞·ªüng\n",
    "df_potential['potential_score_enhanced'] = (\n",
    "    df_potential['potential_score'] * 0.9 + \n",
    "    df_potential['growth_trend_normalized'] * 0.1\n",
    ")\n",
    "\n",
    "# Ph√¢n lo·∫°i l·∫°i v·ªõi ƒëi·ªÉm s·ªë c·∫£i ti·∫øn\n",
    "def classify_potential_enhanced(score):\n",
    "    if score >= 0.7:\n",
    "        return 'Cao'\n",
    "    elif score >= 0.4:\n",
    "        return 'Trung b√¨nh'\n",
    "    else:\n",
    "        return 'Th·∫•p'\n",
    "\n",
    "df_potential['potential_level_enhanced'] = df_potential['potential_score_enhanced'].apply(classify_potential_enhanced)\n",
    "\n",
    "print(\"üìà Ph√¢n b·ªë ti·ªÅm nƒÉng sau khi c·∫£i ti·∫øn:\")\n",
    "enhanced_dist = df_potential['potential_level_enhanced'].value_counts()\n",
    "print(enhanced_dist)\n",
    "print(f\"\\nT·ª∑ l·ªá ph·∫ßn trƒÉm:\")\n",
    "print(enhanced_dist / len(df_potential) * 100)\n",
    "\n",
    "# So s√°nh tr∆∞·ªõc v√† sau khi c·∫£i ti·∫øn\n",
    "print(\"\\nüîÑ So s√°nh ph√¢n lo·∫°i tr∆∞·ªõc v√† sau c·∫£i ti·∫øn:\")\n",
    "comparison = pd.crosstab(df_potential['potential_level'], df_potential['potential_level_enhanced'], margins=True)\n",
    "print(comparison)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "32746cf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üéØ B∆Ø·ªöC 3: HU·∫§N LUY·ªÜN M√î H√åNH MACHINE LEARNING\n",
      "==================================================\n",
      "üìä K√≠ch th∆∞·ªõc d·ªØ li·ªáu:\n",
      "  - Train: 190812 m·∫´u\n",
      "  - Test: 47704 m·∫´u\n",
      "  - S·ªë ƒë·∫∑c tr∆∞ng: 8\n",
      "\n",
      "üå≤ Hu·∫•n luy·ªán Random Forest...\n",
      "ƒê·ªô ch√≠nh x√°c Random Forest: 0.9866\n",
      "\n",
      "üöÄ Hu·∫•n luy·ªán Gradient Boosting...\n",
      "ƒê·ªô ch√≠nh x√°c Gradient Boosting: 0.9803\n",
      "\n",
      "üìà Hu·∫•n luy·ªán Logistic Regression...\n",
      "ƒê·ªô ch√≠nh x√°c Logistic Regression: 0.9492\n",
      "\n",
      "üèÜ So s√°nh hi·ªáu su·∫•t c√°c m√¥ h√¨nh:\n",
      "ü•á Random Forest: 0.9866\n",
      "   Gradient Boosting: 0.9803\n",
      "   Logistic Regression: 0.9492\n",
      "\n",
      "üéØ M√¥ h√¨nh t·ªët nh·∫•t: Random Forest\n",
      "\n",
      "üìã B√°o c√°o chi ti·∫øt cho Random Forest:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         Cao       1.00      0.73      0.85        15\n",
      "        Th·∫•p       0.99      0.99      0.99     37251\n",
      "  Trung b√¨nh       0.98      0.96      0.97     10438\n",
      "\n",
      "    accuracy                           0.99     47704\n",
      "   macro avg       0.99      0.90      0.94     47704\n",
      "weighted avg       0.99      0.99      0.99     47704\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ==========================\n",
    "# B∆Ø·ªöC 3: HU·∫§N LUY·ªÜN M√î H√åNH MACHINE LEARNING\n",
    "# ==========================\n",
    "\n",
    "print(\"\\nüéØ B∆Ø·ªöC 3: HU·∫§N LUY·ªÜN M√î H√åNH MACHINE LEARNING\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Chu·∫©n b·ªã d·ªØ li·ªáu cho ML\n",
    "features_ml = ['passengers', 'fare', 'nsmiles', 'large_ms', 'lf_ms', 'fare_lg', 'fare_low', 'growth_trend']\n",
    "X = df_potential[features_ml].fillna(0)\n",
    "y = df_potential['potential_level_enhanced']\n",
    "\n",
    "# Chia d·ªØ li·ªáu train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "print(f\"üìä K√≠ch th∆∞·ªõc d·ªØ li·ªáu:\")\n",
    "print(f\"  - Train: {X_train.shape[0]} m·∫´u\")\n",
    "print(f\"  - Test: {X_test.shape[0]} m·∫´u\")\n",
    "print(f\"  - S·ªë ƒë·∫∑c tr∆∞ng: {X_train.shape[1]}\")\n",
    "\n",
    "# 1. Random Forest\n",
    "print(\"\\nüå≤ Hu·∫•n luy·ªán Random Forest...\")\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "rf_pred = rf.predict(X_test)\n",
    "rf_accuracy = accuracy_score(y_test, rf_pred)\n",
    "\n",
    "print(f\"ƒê·ªô ch√≠nh x√°c Random Forest: {rf_accuracy:.4f}\")\n",
    "\n",
    "# 2. Gradient Boosting\n",
    "print(\"\\nüöÄ Hu·∫•n luy·ªán Gradient Boosting...\")\n",
    "gb = GradientBoostingClassifier(n_estimators=100, random_state=42)\n",
    "gb.fit(X_train, y_train)\n",
    "gb_pred = gb.predict(X_test)\n",
    "gb_accuracy = accuracy_score(y_test, gb_pred)\n",
    "\n",
    "print(f\"ƒê·ªô ch√≠nh x√°c Gradient Boosting: {gb_accuracy:.4f}\")\n",
    "\n",
    "# 3. Logistic Regression\n",
    "print(\"\\nüìà Hu·∫•n luy·ªán Logistic Regression...\")\n",
    "lr = LogisticRegression(random_state=42, max_iter=1000)\n",
    "lr.fit(X_train, y_train)\n",
    "lr_pred = lr.predict(X_test)\n",
    "lr_accuracy = accuracy_score(y_test, lr_pred)\n",
    "\n",
    "print(f\"ƒê·ªô ch√≠nh x√°c Logistic Regression: {lr_accuracy:.4f}\")\n",
    "\n",
    "# So s√°nh hi·ªáu su·∫•t\n",
    "print(\"\\nüèÜ So s√°nh hi·ªáu su·∫•t c√°c m√¥ h√¨nh:\")\n",
    "models = ['Random Forest', 'Gradient Boosting', 'Logistic Regression']\n",
    "accuracies = [rf_accuracy, gb_accuracy, lr_accuracy]\n",
    "best_model_idx = np.argmax(accuracies)\n",
    "\n",
    "for i, (model, acc) in enumerate(zip(models, accuracies)):\n",
    "    marker = \"ü•á\" if i == best_model_idx else \"  \"\n",
    "    print(f\"{marker} {model}: {acc:.4f}\")\n",
    "\n",
    "print(f\"\\nüéØ M√¥ h√¨nh t·ªët nh·∫•t: {models[best_model_idx]}\")\n",
    "\n",
    "# B√°o c√°o chi ti·∫øt cho m√¥ h√¨nh t·ªët nh·∫•t\n",
    "if best_model_idx == 0:\n",
    "    best_model = rf\n",
    "    best_pred = rf_pred\n",
    "elif best_model_idx == 1:\n",
    "    best_model = gb\n",
    "    best_pred = gb_pred\n",
    "else:\n",
    "    best_model = lr\n",
    "    best_pred = lr_pred\n",
    "\n",
    "print(f\"\\nüìã B√°o c√°o chi ti·∫øt cho {models[best_model_idx]}:\")\n",
    "print(classification_report(y_test, best_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "3668a878",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç C√°c gi√° tr·ªã c·ªßa nh√£n 'potential_level_enhanced':\n",
      "potential_level_enhanced\n",
      "Th·∫•p          186251\n",
      "Trung b√¨nh     52188\n",
      "Cao               77\n",
      "Name: count, dtype: int64\n",
      "\n",
      "üìä T·ª∑ l·ªá ph·∫ßn trƒÉm:\n",
      "potential_level_enhanced\n",
      "Th·∫•p          78.087424\n",
      "Trung b√¨nh    21.880293\n",
      "Cao            0.032283\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "‚úàÔ∏è M·ªôt v√†i tuy·∫øn bay c√≥ ti·ªÅm nƒÉng ph√°t tri·ªÉn (label = 1):\n",
      "Empty DataFrame\n",
      "Columns: [city1, city2, fare, passengers, growth_trend]\n",
      "Index: []\n",
      "\n",
      "‚úàÔ∏è M·ªôt v√†i tuy·∫øn bay kh√¥ng c√≥ ti·ªÅm nƒÉng ph√°t tri·ªÉn (label = 0):\n",
      "Empty DataFrame\n",
      "Columns: [city1, city2, fare, passengers, growth_trend]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# Ki·ªÉm tra gi√° tr·ªã duy nh·∫•t c·ªßa nh√£n\n",
    "print(\"üîç C√°c gi√° tr·ªã c·ªßa nh√£n 'potential_level_enhanced':\")\n",
    "print(df_potential['potential_level_enhanced'].value_counts())\n",
    "\n",
    "# T·ª∑ l·ªá %\n",
    "print(\"\\nüìä T·ª∑ l·ªá ph·∫ßn trƒÉm:\")\n",
    "print(df_potential['potential_level_enhanced'].value_counts(normalize=True) * 100)\n",
    "\n",
    "# Hi·ªÉn th·ªã 5 tuy·∫øn bay ƒë·∫ßu ti√™n c√≥ nh√£n 1\n",
    "print(\"\\n‚úàÔ∏è M·ªôt v√†i tuy·∫øn bay c√≥ ti·ªÅm nƒÉng ph√°t tri·ªÉn (label = 1):\")\n",
    "print(df_potential[df_potential['potential_level_enhanced'] == 1][['city1', 'city2', 'fare', 'passengers', 'growth_trend']].head())\n",
    "# Hi·ªÉn th·ªã 5 tuy·∫øn bay ƒë·∫ßu ti√™n c√≥ nh√£n 0\n",
    "print(\"\\n‚úàÔ∏è M·ªôt v√†i tuy·∫øn bay kh√¥ng c√≥ ti·ªÅm nƒÉng ph√°t tri·ªÉn (label = 0):\")\n",
    "print(df_potential[df_potential['potential_level_enhanced'] == 0][['city1', 'city2', 'fare', 'passengers', 'growth_trend']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "14a65d3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üß© D·ª∞ ƒêO√ÅN 10 TUY·∫æN BAY TRONG T·∫¨P TEST\n",
      "============================================================\n",
      "\n",
      "üìä K·∫æT QU·∫¢ D·ª∞ ƒêO√ÅN:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: center;\">\n",
       "      <th></th>\n",
       "      <th>city1</th>\n",
       "      <th>city2</th>\n",
       "      <th>fare</th>\n",
       "      <th>passengers</th>\n",
       "      <th>nsmiles</th>\n",
       "      <th>Th·ª±c t·∫ø</th>\n",
       "      <th>D·ª± ƒëo√°n</th>\n",
       "      <th>X√°c_su·∫•t_ti·ªÅm_nƒÉng</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12674</th>\n",
       "      <td>Colorado Springs, CO</td>\n",
       "      <td>Dallas/Fort Worth, TX</td>\n",
       "      <td>177.62</td>\n",
       "      <td>173</td>\n",
       "      <td>603</td>\n",
       "      <td>Th·∫•p</td>\n",
       "      <td>Th·∫•p</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148548</th>\n",
       "      <td>Cleveland, OH (Metropolitan Area)</td>\n",
       "      <td>Orlando, FL</td>\n",
       "      <td>173.29</td>\n",
       "      <td>37</td>\n",
       "      <td>895</td>\n",
       "      <td>Th·∫•p</td>\n",
       "      <td>Th·∫•p</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81981</th>\n",
       "      <td>Rochester, NY</td>\n",
       "      <td>Washington, DC (Metropolitan Area)</td>\n",
       "      <td>191.62</td>\n",
       "      <td>47</td>\n",
       "      <td>296</td>\n",
       "      <td>Th·∫•p</td>\n",
       "      <td>Th·∫•p</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45200</th>\n",
       "      <td>Chicago, IL</td>\n",
       "      <td>Houston, TX</td>\n",
       "      <td>176.05</td>\n",
       "      <td>8</td>\n",
       "      <td>945</td>\n",
       "      <td>Th·∫•p</td>\n",
       "      <td>Th·∫•p</td>\n",
       "      <td>0.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182186</th>\n",
       "      <td>Cincinnati, OH</td>\n",
       "      <td>Dallas/Fort Worth, TX</td>\n",
       "      <td>279.07</td>\n",
       "      <td>4</td>\n",
       "      <td>812</td>\n",
       "      <td>Th·∫•p</td>\n",
       "      <td>Th·∫•p</td>\n",
       "      <td>0.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222945</th>\n",
       "      <td>Los Angeles, CA (Metropolitan Area)</td>\n",
       "      <td>Sacramento, CA</td>\n",
       "      <td>134.12</td>\n",
       "      <td>610</td>\n",
       "      <td>404</td>\n",
       "      <td>Th·∫•p</td>\n",
       "      <td>Th·∫•p</td>\n",
       "      <td>0.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156799</th>\n",
       "      <td>Houston, TX</td>\n",
       "      <td>Tulsa, OK</td>\n",
       "      <td>135.34</td>\n",
       "      <td>388</td>\n",
       "      <td>453</td>\n",
       "      <td>Trung b√¨nh</td>\n",
       "      <td>Trung b√¨nh</td>\n",
       "      <td>0.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180798</th>\n",
       "      <td>Atlanta, GA (Metropolitan Area)</td>\n",
       "      <td>Houston, TX</td>\n",
       "      <td>187.40</td>\n",
       "      <td>923</td>\n",
       "      <td>696</td>\n",
       "      <td>Trung b√¨nh</td>\n",
       "      <td>Trung b√¨nh</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183732</th>\n",
       "      <td>Atlanta, GA (Metropolitan Area)</td>\n",
       "      <td>Washington, DC (Metropolitan Area)</td>\n",
       "      <td>183.97</td>\n",
       "      <td>1293</td>\n",
       "      <td>577</td>\n",
       "      <td>Trung b√¨nh</td>\n",
       "      <td>Trung b√¨nh</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93382</th>\n",
       "      <td>Chicago, IL</td>\n",
       "      <td>Houston, TX</td>\n",
       "      <td>143.91</td>\n",
       "      <td>540</td>\n",
       "      <td>945</td>\n",
       "      <td>Trung b√¨nh</td>\n",
       "      <td>Trung b√¨nh</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      city1                                city2                  fare   passengers  nsmiles   Th·ª±c t·∫ø     D·ª± ƒëo√°n    X√°c_su·∫•t_ti·ªÅm_nƒÉng\n",
       "12674                  Colorado Springs, CO               Dallas/Fort Worth, TX  177.62      173       603          Th·∫•p        Th·∫•p         1.00       \n",
       "148548    Cleveland, OH (Metropolitan Area)                         Orlando, FL  173.29       37       895          Th·∫•p        Th·∫•p         1.00       \n",
       "81981                         Rochester, NY  Washington, DC (Metropolitan Area)  191.62       47       296          Th·∫•p        Th·∫•p         1.00       \n",
       "45200                           Chicago, IL                         Houston, TX  176.05        8       945          Th·∫•p        Th·∫•p         0.99       \n",
       "182186                       Cincinnati, OH               Dallas/Fort Worth, TX  279.07        4       812          Th·∫•p        Th·∫•p         0.94       \n",
       "222945  Los Angeles, CA (Metropolitan Area)                      Sacramento, CA  134.12      610       404          Th·∫•p        Th·∫•p         0.92       \n",
       "156799                          Houston, TX                           Tulsa, OK  135.34      388       453    Trung b√¨nh  Trung b√¨nh         0.13       \n",
       "180798      Atlanta, GA (Metropolitan Area)                         Houston, TX  187.40      923       696    Trung b√¨nh  Trung b√¨nh         0.03       \n",
       "183732      Atlanta, GA (Metropolitan Area)  Washington, DC (Metropolitan Area)  183.97     1293       577    Trung b√¨nh  Trung b√¨nh         0.01       \n",
       "93382                           Chicago, IL                         Houston, TX  143.91      540       945    Trung b√¨nh  Trung b√¨nh         0.00       "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üöÄ TUY·∫æN BAY TI·ªÄM NƒÇNG NH·∫§T THEO M√î H√åNH:\n",
      "Colorado Springs, CO ‚úà Dallas/Fort Worth, TX\n",
      "üí∞ Fare: 177.62 | üë• Passengers: 173 | üìè Distance: 603 miles\n",
      "üîÆ X√°c su·∫•t ti·ªÅm nƒÉng: 1.00 | üìà D·ª± ƒëo√°n: Th·∫•p | üéØ Th·ª±c t·∫ø: Th·∫•p\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "# ==========================\n",
    "# C·∫§U H√åNH HI·ªÇN TH·ªä PANDAS\n",
    "# ==========================\n",
    "pd.set_option('display.max_rows', 20)        # s·ªë d√≤ng t·ªëi ƒëa\n",
    "pd.set_option('display.max_columns', None)   # hi·ªÉn th·ªã t·∫•t c·∫£ c√°c c·ªôt\n",
    "pd.set_option('display.width', 200)          # tƒÉng ƒë·ªô r·ªông m√†n h√¨nh\n",
    "pd.set_option('display.max_colwidth', None)  # kh√¥ng c·∫Øt ch·ªØ trong c·ªôt d√†i\n",
    "pd.set_option('display.colheader_justify', 'center')\n",
    "\n",
    "# ==========================\n",
    "# D·ª∞ ƒêO√ÅN NHI·ªÄU M·∫™U TRONG T·∫¨P TEST\n",
    "# ==========================\n",
    "\n",
    "print(\"\\nüß© D·ª∞ ƒêO√ÅN 10 TUY·∫æN BAY TRONG T·∫¨P TEST\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# L·∫•y 10 m·∫´u ng·∫´u nhi√™n trong t·∫≠p test\n",
    "sample_indices = np.random.choice(X_test.index, size=10, replace=False)\n",
    "X_sample = X_test.loc[sample_indices]\n",
    "y_true = y_test.loc[sample_indices]\n",
    "\n",
    "# D·ª± ƒëo√°n b·∫±ng m√¥ h√¨nh t·ªët nh·∫•t\n",
    "y_pred = best_model.predict(X_sample)\n",
    "y_proba = best_model.predict_proba(X_sample)[:, 1]  # x√°c su·∫•t ti·ªÅm nƒÉng cao\n",
    "\n",
    "# G·ªôp k·∫øt qu·∫£ v√†o DataFrame\n",
    "results = df_potential.loc[sample_indices, ['city1', 'city2', 'fare', 'passengers', 'nsmiles']].copy()\n",
    "results['Th·ª±c t·∫ø'] = y_true.values\n",
    "results['D·ª± ƒëo√°n'] = y_pred\n",
    "results['X√°c_su·∫•t_ti·ªÅm_nƒÉng'] = y_proba\n",
    "\n",
    "# S·∫Øp x·∫øp theo x√°c su·∫•t ti·ªÅm nƒÉng gi·∫£m d·∫ßn\n",
    "results = results.sort_values(by='X√°c_su·∫•t_ti·ªÅm_nƒÉng', ascending=False)\n",
    "\n",
    "# ==========================\n",
    "# HI·ªÇN TH·ªä K·∫æT QU·∫¢\n",
    "# ==========================\n",
    "\n",
    "print(\"\\nüìä K·∫æT QU·∫¢ D·ª∞ ƒêO√ÅN:\")\n",
    "display(results)\n",
    "\n",
    "# ==========================\n",
    "# IN RA TOP TUY·∫æN BAY TI·ªÄM NƒÇNG NH·∫§T\n",
    "# ==========================\n",
    "\n",
    "top_route = results.iloc[0]\n",
    "print(\"\\nüöÄ TUY·∫æN BAY TI·ªÄM NƒÇNG NH·∫§T THEO M√î H√åNH:\")\n",
    "print(f\"{top_route.city1} ‚úà {top_route.city2}\")\n",
    "print(f\"üí∞ Fare: {top_route.fare:.2f} | üë• Passengers: {top_route.passengers:.0f} | \"\n",
    "      f\"üìè Distance: {top_route.nsmiles} miles\")\n",
    "print(f\"üîÆ X√°c su·∫•t ti·ªÅm nƒÉng: {top_route['X√°c_su·∫•t_ti·ªÅm_nƒÉng']:.2f} | \"\n",
    "      f\"üìà D·ª± ƒëo√°n: {top_route['D·ª± ƒëo√°n']} | üéØ Th·ª±c t·∫ø: {top_route['Th·ª±c t·∫ø']}\")\n",
    "\n",
    "# ==========================\n",
    "# HI·ªÇN TH·ªä T√ìM T·∫ÆT NG·∫ÆN G·ªåN\n",
    "# ==========================\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "0cec9b61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üß© D·ª∞ ƒêO√ÅN 10 TUY·∫æN BAY TRONG T·∫¨P TEST\n",
      "============================================================\n",
      "\n",
      "üìä K·∫æT QU·∫¢ D·ª∞ ƒêO√ÅN:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: center;\">\n",
       "      <th></th>\n",
       "      <th>city1</th>\n",
       "      <th>city2</th>\n",
       "      <th>fare</th>\n",
       "      <th>passengers</th>\n",
       "      <th>nsmiles</th>\n",
       "      <th>Th·ª±c t·∫ø</th>\n",
       "      <th>D·ª± ƒëo√°n</th>\n",
       "      <th>X√°c_su·∫•t_ti·ªÅm_nƒÉng</th>\n",
       "      <th>Ti·ªÅm_nƒÉng</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>166720</th>\n",
       "      <td>St. Louis, MO</td>\n",
       "      <td>Washington, DC (Metropolitan Area)</td>\n",
       "      <td>248.05</td>\n",
       "      <td>173</td>\n",
       "      <td>738</td>\n",
       "      <td>Th·∫•p</td>\n",
       "      <td>Th·∫•p</td>\n",
       "      <td>1.00</td>\n",
       "      <td>Th·∫•p</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91348</th>\n",
       "      <td>Houston, TX</td>\n",
       "      <td>Indianapolis, IN</td>\n",
       "      <td>203.26</td>\n",
       "      <td>74</td>\n",
       "      <td>862</td>\n",
       "      <td>Th·∫•p</td>\n",
       "      <td>Th·∫•p</td>\n",
       "      <td>1.00</td>\n",
       "      <td>Th·∫•p</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96145</th>\n",
       "      <td>Jacksonville, FL</td>\n",
       "      <td>New York City, NY (Metropolitan Area)</td>\n",
       "      <td>172.81</td>\n",
       "      <td>12</td>\n",
       "      <td>869</td>\n",
       "      <td>Th·∫•p</td>\n",
       "      <td>Th·∫•p</td>\n",
       "      <td>0.99</td>\n",
       "      <td>Th·∫•p</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182761</th>\n",
       "      <td>Jacksonville, FL</td>\n",
       "      <td>San Francisco, CA (Metropolitan Area)</td>\n",
       "      <td>342.86</td>\n",
       "      <td>15</td>\n",
       "      <td>2366</td>\n",
       "      <td>Th·∫•p</td>\n",
       "      <td>Th·∫•p</td>\n",
       "      <td>0.94</td>\n",
       "      <td>Th·∫•p</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224920</th>\n",
       "      <td>Los Angeles, CA (Metropolitan Area)</td>\n",
       "      <td>Tulsa, OK</td>\n",
       "      <td>307.60</td>\n",
       "      <td>26</td>\n",
       "      <td>1283</td>\n",
       "      <td>Th·∫•p</td>\n",
       "      <td>Th·∫•p</td>\n",
       "      <td>0.85</td>\n",
       "      <td>Th·∫•p</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166024</th>\n",
       "      <td>Melbourne, FL</td>\n",
       "      <td>New York City, NY (Metropolitan Area)</td>\n",
       "      <td>103.77</td>\n",
       "      <td>384</td>\n",
       "      <td>995</td>\n",
       "      <td>Th·∫•p</td>\n",
       "      <td>Th·∫•p</td>\n",
       "      <td>0.79</td>\n",
       "      <td>Th·∫•p</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>228722</th>\n",
       "      <td>Los Angeles, CA (Metropolitan Area)</td>\n",
       "      <td>Norfolk, VA (Metropolitan Area)</td>\n",
       "      <td>403.62</td>\n",
       "      <td>212</td>\n",
       "      <td>2371</td>\n",
       "      <td>Trung b√¨nh</td>\n",
       "      <td>Th·∫•p</td>\n",
       "      <td>0.69</td>\n",
       "      <td>Th·∫•p</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94147</th>\n",
       "      <td>Los Angeles, CA (Metropolitan Area)</td>\n",
       "      <td>Portland, OR</td>\n",
       "      <td>182.54</td>\n",
       "      <td>1190</td>\n",
       "      <td>859</td>\n",
       "      <td>Trung b√¨nh</td>\n",
       "      <td>Trung b√¨nh</td>\n",
       "      <td>0.04</td>\n",
       "      <td>Th·∫•p</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189841</th>\n",
       "      <td>Albuquerque, NM</td>\n",
       "      <td>Dallas/Fort Worth, TX</td>\n",
       "      <td>201.46</td>\n",
       "      <td>320</td>\n",
       "      <td>580</td>\n",
       "      <td>Trung b√¨nh</td>\n",
       "      <td>Trung b√¨nh</td>\n",
       "      <td>0.00</td>\n",
       "      <td>Th·∫•p</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38581</th>\n",
       "      <td>Philadelphia, PA</td>\n",
       "      <td>Phoenix, AZ</td>\n",
       "      <td>220.08</td>\n",
       "      <td>1044</td>\n",
       "      <td>2075</td>\n",
       "      <td>Trung b√¨nh</td>\n",
       "      <td>Trung b√¨nh</td>\n",
       "      <td>0.00</td>\n",
       "      <td>Th·∫•p</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      city1                                 city2                    fare   passengers  nsmiles   Th·ª±c t·∫ø     D·ª± ƒëo√°n    X√°c_su·∫•t_ti·ªÅm_nƒÉng Ti·ªÅm_nƒÉng\n",
       "166720                        St. Louis, MO     Washington, DC (Metropolitan Area)  248.05      173       738          Th·∫•p        Th·∫•p         1.00           Th·∫•p  \n",
       "91348                           Houston, TX                       Indianapolis, IN  203.26       74       862          Th·∫•p        Th·∫•p         1.00           Th·∫•p  \n",
       "96145                      Jacksonville, FL  New York City, NY (Metropolitan Area)  172.81       12       869          Th·∫•p        Th·∫•p         0.99           Th·∫•p  \n",
       "182761                     Jacksonville, FL  San Francisco, CA (Metropolitan Area)  342.86       15      2366          Th·∫•p        Th·∫•p         0.94           Th·∫•p  \n",
       "224920  Los Angeles, CA (Metropolitan Area)                              Tulsa, OK  307.60       26      1283          Th·∫•p        Th·∫•p         0.85           Th·∫•p  \n",
       "166024                        Melbourne, FL  New York City, NY (Metropolitan Area)  103.77      384       995          Th·∫•p        Th·∫•p         0.79           Th·∫•p  \n",
       "228722  Los Angeles, CA (Metropolitan Area)        Norfolk, VA (Metropolitan Area)  403.62      212      2371    Trung b√¨nh        Th·∫•p         0.69           Th·∫•p  \n",
       "94147   Los Angeles, CA (Metropolitan Area)                           Portland, OR  182.54     1190       859    Trung b√¨nh  Trung b√¨nh         0.04           Th·∫•p  \n",
       "189841                      Albuquerque, NM                  Dallas/Fort Worth, TX  201.46      320       580    Trung b√¨nh  Trung b√¨nh         0.00           Th·∫•p  \n",
       "38581                      Philadelphia, PA                            Phoenix, AZ  220.08     1044      2075    Trung b√¨nh  Trung b√¨nh         0.00           Th·∫•p  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üöÄ TUY·∫æN BAY TI·ªÄM NƒÇNG NH·∫§T THEO M√î H√åNH:\n",
      "St. Louis, MO ‚úà Washington, DC (Metropolitan Area)\n",
      "üí∞ Fare: 248.05 | üë• Passengers: 173 | üìè Distance: 738 miles\n",
      "üîÆ X√°c su·∫•t ti·ªÅm nƒÉng: 1.00 | üìà D·ª± ƒëo√°n: Th·∫•p | üéØ Th·ª±c t·∫ø: Th·∫•p | üåü Ti·ªÅm nƒÉng: Th·∫•p\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "# ==========================\n",
    "# C·∫§U H√åNH HI·ªÇN TH·ªä PANDAS\n",
    "# ==========================\n",
    "pd.set_option('display.max_rows', 20)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 200)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.colheader_justify', 'center')\n",
    "\n",
    "# ==========================\n",
    "# D·ª∞ ƒêO√ÅN NHI·ªÄU M·∫™U TRONG T·∫¨P TEST\n",
    "# ==========================\n",
    "\n",
    "print(\"\\nüß© D·ª∞ ƒêO√ÅN 10 TUY·∫æN BAY TRONG T·∫¨P TEST\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# L·∫•y 10 m·∫´u ng·∫´u nhi√™n trong t·∫≠p test\n",
    "sample_indices = np.random.choice(X_test.index, size=10, replace=False)\n",
    "X_sample = X_test.loc[sample_indices]\n",
    "y_true = y_test.loc[sample_indices]\n",
    "\n",
    "# D·ª± ƒëo√°n b·∫±ng m√¥ h√¨nh t·ªët nh·∫•t\n",
    "y_pred = best_model.predict(X_sample)\n",
    "y_proba = best_model.predict_proba(X_sample)[:, 1]  # x√°c su·∫•t ti·ªÅm nƒÉng cao\n",
    "\n",
    "# G·ªôp k·∫øt qu·∫£ v√†o DataFrame\n",
    "results = df_potential.loc[sample_indices, ['city1', 'city2', 'fare', 'passengers', 'nsmiles']].copy()\n",
    "results['Th·ª±c t·∫ø'] = y_true.values\n",
    "results['D·ª± ƒëo√°n'] = y_pred\n",
    "results['X√°c_su·∫•t_ti·ªÅm_nƒÉng'] = y_proba\n",
    "\n",
    "# üîπ Th√™m c·ªôt \"Ti·ªÅm nƒÉng\" (1 = Cao, 0 = Th·∫•p)\n",
    "results['Ti·ªÅm_nƒÉng'] = np.where(results['D·ª± ƒëo√°n'] == 1, 'Cao', 'Th·∫•p')\n",
    "\n",
    "# S·∫Øp x·∫øp theo x√°c su·∫•t ti·ªÅm nƒÉng gi·∫£m d·∫ßn\n",
    "results = results.sort_values(by='X√°c_su·∫•t_ti·ªÅm_nƒÉng', ascending=False)\n",
    "\n",
    "# ==========================\n",
    "# HI·ªÇN TH·ªä K·∫æT QU·∫¢\n",
    "# ==========================\n",
    "\n",
    "print(\"\\nüìä K·∫æT QU·∫¢ D·ª∞ ƒêO√ÅN:\")\n",
    "display(results)\n",
    "\n",
    "# ==========================\n",
    "# IN RA TOP TUY·∫æN BAY TI·ªÄM NƒÇNG NH·∫§T\n",
    "# ==========================\n",
    "\n",
    "top_route = results.iloc[0]\n",
    "print(\"\\nüöÄ TUY·∫æN BAY TI·ªÄM NƒÇNG NH·∫§T THEO M√î H√åNH:\")\n",
    "print(f\"{top_route.city1} ‚úà {top_route.city2}\")\n",
    "print(f\"üí∞ Fare: {top_route.fare:.2f} | üë• Passengers: {top_route.passengers:.0f} | \"\n",
    "      f\"üìè Distance: {top_route.nsmiles} miles\")\n",
    "print(f\"üîÆ X√°c su·∫•t ti·ªÅm nƒÉng: {top_route['X√°c_su·∫•t_ti·ªÅm_nƒÉng']:.2f} | \"\n",
    "      f\"üìà D·ª± ƒëo√°n: {top_route['D·ª± ƒëo√°n']} | üéØ Th·ª±c t·∫ø: {top_route['Th·ª±c t·∫ø']} | üåü Ti·ªÅm nƒÉng: {top_route['Ti·ªÅm_nƒÉng']}\")\n",
    "\n",
    "# ==========================\n",
    "# HI·ªÇN TH·ªä T√ìM T·∫ÆT NG·∫ÆN G·ªåN\n",
    "# ==========================\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d80f657d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================\n",
    "# B∆Ø·ªöC 4: PH√ÇN T√çCH T·∫¶M QUAN TR·ªåNG C·ª¶A C√ÅC ƒê·∫∂C TR∆ØNG\n",
    "# ==========================\n",
    "\n",
    "print(\"\\nüéØ B∆Ø·ªöC 4: PH√ÇN T√çCH T·∫¶M QUAN TR·ªåNG C·ª¶A C√ÅC ƒê·∫∂C TR∆ØNG\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# L·∫•y feature importance t·ª´ Random Forest (m√¥ h√¨nh th∆∞·ªùng cho k·∫øt qu·∫£ t·ªët nh·∫•t)\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': features_ml,\n",
    "    'importance': rf.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"üîç T·∫ßm quan tr·ªçng c·ªßa c√°c ƒë·∫∑c tr∆∞ng:\")\n",
    "for idx, row in feature_importance.iterrows():\n",
    "    print(f\"  {row['feature']}: {row['importance']:.4f}\")\n",
    "\n",
    "# Tr·ª±c quan h√≥a feature importance\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(data=feature_importance, x='importance', y='feature', palette='viridis')\n",
    "plt.title('üìä T·∫ßm quan tr·ªçng c·ªßa c√°c ƒë·∫∑c tr∆∞ng trong ph√¢n lo·∫°i ti·ªÅm nƒÉng tuy·∫øn bay')\n",
    "plt.xlabel('M·ª©c ƒë·ªô quan tr·ªçng')\n",
    "plt.ylabel('ƒê·∫∑c tr∆∞ng')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Ph√¢n t√≠ch chi ti·∫øt theo t·ª´ng nh√≥m ti·ªÅm nƒÉng\n",
    "print(\"\\nüìà Ph√¢n t√≠ch chi ti·∫øt theo nh√≥m ti·ªÅm nƒÉng:\")\n",
    "\n",
    "# Th·ªëng k√™ cho t·ª´ng nh√≥m\n",
    "for level in ['Cao', 'Trung b√¨nh', 'Th·∫•p']:\n",
    "    subset = df_potential[df_potential['potential_level_enhanced'] == level]\n",
    "    print(f\"\\nüéØ Nh√≥m {level} ({len(subset)} tuy·∫øn bay):\")\n",
    "    print(f\"  - L∆∞u l∆∞·ª£ng h√†nh kh√°ch TB: {subset['passengers'].mean():.0f}\")\n",
    "    print(f\"  - Gi√° v√© TB: ${subset['fare'].mean():.2f}\")\n",
    "    print(f\"  - Kho·∫£ng c√°ch TB: {subset['nsmiles'].mean():.0f} miles\")\n",
    "    print(f\"  - Th·ªã ph·∫ßn h√£ng l·ªõn TB: {subset['large_ms'].mean():.2%}\")\n",
    "    print(f\"  - Th·ªã ph·∫ßn h√£ng gi√° r·∫ª TB: {subset['lf_ms'].mean():.2%}\")\n",
    "    print(f\"  - Xu h∆∞·ªõng tƒÉng tr∆∞·ªüng TB: {subset['growth_trend'].mean():.3f}\")\n",
    "\n",
    "# Top 10 tuy·∫øn bay c√≥ ti·ªÅm nƒÉng cao nh·∫•t\n",
    "print(\"\\nüèÜ TOP 10 TUY·∫æN BAY C√ì TI·ªÄM NƒÇNG CAO NH·∫§T:\")\n",
    "top_potential = df_potential[df_potential['potential_level_enhanced'] == 'Cao'].nlargest(10, 'potential_score_enhanced')\n",
    "for idx, row in top_potential.iterrows():\n",
    "    print(f\"  {row['city1']} ‚Üí {row['city2']}: {row['potential_score_enhanced']:.3f} (Passengers: {row['passengers']:.0f}, Fare: ${row['fare']:.2f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "914c548b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================\n",
    "# B∆Ø·ªöC 5: TR·ª∞C QUAN H√ìA K·∫æT QU·∫¢ PH√ÇN LO·∫†I\n",
    "# ==========================\n",
    "\n",
    "print(\"\\nüéØ B∆Ø·ªöC 5: TR·ª∞C QUAN H√ìA K·∫æT QU·∫¢ PH√ÇN LO·∫†I\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# 1. Bi·ªÉu ƒë·ªì ph√¢n b·ªë ti·ªÅm nƒÉng\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Subplot 1: Ph√¢n b·ªë ti·ªÅm nƒÉng\n",
    "plt.subplot(2, 3, 1)\n",
    "potential_counts = df_potential['potential_level_enhanced'].value_counts()\n",
    "colors = ['#ff6b6b', '#4ecdc4', '#45b7d1']\n",
    "plt.pie(potential_counts.values, labels=potential_counts.index, autopct='%1.1f%%', colors=colors)\n",
    "plt.title('üìä Ph√¢n b·ªë ti·ªÅm nƒÉng tuy·∫øn bay')\n",
    "\n",
    "# Subplot 2: Scatter plot passengers vs fare\n",
    "plt.subplot(2, 3, 2)\n",
    "sns.scatterplot(data=df_potential, x='passengers', y='fare', hue='potential_level_enhanced', \n",
    "                palette=['#ff6b6b', '#4ecdc4', '#45b7d1'], alpha=0.6)\n",
    "plt.title('‚úàÔ∏è L∆∞u l∆∞·ª£ng vs Gi√° v√© theo ti·ªÅm nƒÉng')\n",
    "plt.xlabel('L∆∞u l∆∞·ª£ng h√†nh kh√°ch')\n",
    "plt.ylabel('Gi√° v√© ($)')\n",
    "\n",
    "# Subplot 3: Box plot potential score\n",
    "plt.subplot(2, 3, 3)\n",
    "sns.boxplot(data=df_potential, x='potential_level_enhanced', y='potential_score_enhanced',\n",
    "            palette=['#ff6b6b', '#4ecdc4', '#45b7d1'])\n",
    "plt.title('üìà Ph√¢n b·ªë ƒëi·ªÉm s·ªë ti·ªÅm nƒÉng')\n",
    "plt.xlabel('M·ª©c ti·ªÅm nƒÉng')\n",
    "plt.ylabel('ƒêi·ªÉm s·ªë ti·ªÅm nƒÉng')\n",
    "\n",
    "# Subplot 4: Heatmap correlation\n",
    "plt.subplot(2, 3, 4)\n",
    "correlation_data = df_potential[['passengers', 'fare', 'nsmiles', 'large_ms', 'lf_ms', 'potential_score_enhanced']]\n",
    "correlation_matrix = correlation_data.corr()\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0, fmt='.2f')\n",
    "plt.title('üî• Ma tr·∫≠n t∆∞∆°ng quan')\n",
    "\n",
    "# Subplot 5: Growth trend analysis\n",
    "plt.subplot(2, 3, 5)\n",
    "growth_by_potential = df_potential.groupby('potential_level_enhanced')['growth_trend'].mean()\n",
    "growth_by_potential.plot(kind='bar', color=['#ff6b6b', '#4ecdc4', '#45b7d1'])\n",
    "plt.title('üìà Xu h∆∞·ªõng tƒÉng tr∆∞·ªüng theo ti·ªÅm nƒÉng')\n",
    "plt.xlabel('M·ª©c ti·ªÅm nƒÉng')\n",
    "plt.ylabel('Xu h∆∞·ªõng tƒÉng tr∆∞·ªüng')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Subplot 6: Market share analysis\n",
    "plt.subplot(2, 3, 6)\n",
    "market_share_data = df_potential.groupby('potential_level_enhanced')[['large_ms', 'lf_ms']].mean()\n",
    "market_share_data.plot(kind='bar', color=['#ff9999', '#66b3ff'])\n",
    "plt.title('üè¢ Th·ªã ph·∫ßn theo ti·ªÅm nƒÉng')\n",
    "plt.xlabel('M·ª©c ti·ªÅm nƒÉng')\n",
    "plt.ylabel('Th·ªã ph·∫ßn (%)')\n",
    "plt.legend(['H√£ng l·ªõn', 'H√£ng gi√° r·∫ª'])\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 2. Ph√¢n t√≠ch theo nƒÉm\n",
    "print(\"\\nüìÖ Ph√¢n t√≠ch xu h∆∞·ªõng theo nƒÉm:\")\n",
    "yearly_analysis = df_potential.groupby(['Year', 'potential_level_enhanced']).size().unstack(fill_value=0)\n",
    "yearly_analysis_pct = yearly_analysis.div(yearly_analysis.sum(axis=1), axis=0) * 100\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "for level in ['Cao', 'Trung b√¨nh', 'Th·∫•p']:\n",
    "    if level in yearly_analysis_pct.columns:\n",
    "        plt.plot(yearly_analysis_pct.index, yearly_analysis_pct[level], marker='o', label=f'Ti·ªÅm nƒÉng {level}')\n",
    "\n",
    "plt.title('üìà Xu h∆∞·ªõng ph√¢n b·ªë ti·ªÅm nƒÉng theo nƒÉm')\n",
    "plt.xlabel('NƒÉm')\n",
    "plt.ylabel('T·ª∑ l·ªá ph·∫ßn trƒÉm (%)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f7183c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================\n",
    "# B∆Ø·ªöC 6: ƒê√ÅNH GI√Å HI·ªÜU SU·∫§T M√î H√åNH V√Ä KHUY·∫æN NGH·ªä\n",
    "# ==========================\n",
    "\n",
    "print(\"\\nüéØ B∆Ø·ªöC 6: ƒê√ÅNH GI√Å HI·ªÜU SU·∫§T M√î H√åNH V√Ä KHUY·∫æN NGH·ªä\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# 1. Cross-validation ƒë·ªÉ ƒë√°nh gi√° ƒë·ªô ·ªïn ƒë·ªãnh\n",
    "print(\"üîÑ ƒê√°nh gi√° ƒë·ªô ·ªïn ƒë·ªãnh m√¥ h√¨nh b·∫±ng Cross-Validation:\")\n",
    "cv_scores = cross_val_score(rf, X_train, y_train, cv=5, scoring='accuracy')\n",
    "print(f\"  - ƒê·ªô ch√≠nh x√°c trung b√¨nh: {cv_scores.mean():.4f} ¬± {cv_scores.std():.4f}\")\n",
    "print(f\"  - Kho·∫£ng tin c·∫≠y 95%: [{cv_scores.mean() - 1.96*cv_scores.std():.4f}, {cv_scores.mean() + 1.96*cv_scores.std():.4f}]\")\n",
    "\n",
    "# 2. Confusion Matrix\n",
    "print(\"\\nüìä Ma tr·∫≠n nh·∫ßm l·∫´n:\")\n",
    "cm = confusion_matrix(y_test, rf_pred)\n",
    "print(cm)\n",
    "\n",
    "# Tr·ª±c quan h√≥a confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['Cao', 'Trung b√¨nh', 'Th·∫•p'],\n",
    "            yticklabels=['Cao', 'Trung b√¨nh', 'Th·∫•p'])\n",
    "plt.title('üìä Ma tr·∫≠n nh·∫ßm l·∫´n - Random Forest')\n",
    "plt.xlabel('D·ª± ƒëo√°n')\n",
    "plt.ylabel('Th·ª±c t·∫ø')\n",
    "plt.show()\n",
    "\n",
    "# 3. Ph√¢n t√≠ch l·ªói\n",
    "print(\"\\nüîç Ph√¢n t√≠ch c√°c tr∆∞·ªùng h·ª£p d·ª± ƒëo√°n sai:\")\n",
    "wrong_predictions = X_test[rf_pred != y_test]\n",
    "if len(wrong_predictions) > 0:\n",
    "    print(f\"  - S·ªë m·∫´u d·ª± ƒëo√°n sai: {len(wrong_predictions)}\")\n",
    "    print(f\"  - T·ª∑ l·ªá d·ª± ƒëo√°n sai: {len(wrong_predictions)/len(y_test)*100:.2f}%\")\n",
    "    \n",
    "    # Ph√¢n t√≠ch ƒë·∫∑c ƒëi·ªÉm c·ªßa c√°c m·∫´u d·ª± ƒëo√°n sai\n",
    "    wrong_indices = X_test[rf_pred != y_test].index\n",
    "    wrong_data = df_potential.loc[wrong_indices]\n",
    "    print(f\"\\n  üìà ƒê·∫∑c ƒëi·ªÉm trung b√¨nh c·ªßa m·∫´u d·ª± ƒëo√°n sai:\")\n",
    "    print(f\"    - Passengers: {wrong_data['passengers'].mean():.0f}\")\n",
    "    print(f\"    - Fare: ${wrong_data['fare'].mean():.2f}\")\n",
    "    print(f\"    - Potential Score: {wrong_data['potential_score_enhanced'].mean():.3f}\")\n",
    "\n",
    "# 4. Khuy·∫øn ngh·ªã chi·∫øn l∆∞·ª£c\n",
    "print(\"\\nüí° KHUY·∫æN NGH·ªä CHI·∫æN L∆Ø·ª¢C CHO T·ª™NG NH√ìM TUY·∫æN BAY:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Nh√≥m ti·ªÅm nƒÉng cao\n",
    "high_potential = df_potential[df_potential['potential_level_enhanced'] == 'Cao']\n",
    "print(f\"\\nüöÄ NH√ìM TI·ªÄM NƒÇNG CAO ({len(high_potential)} tuy·∫øn bay):\")\n",
    "print(\"   ‚úÖ ƒê·∫∑c ƒëi·ªÉm:\")\n",
    "print(f\"     - L∆∞u l∆∞·ª£ng TB: {high_potential['passengers'].mean():.0f} h√†nh kh√°ch\")\n",
    "print(f\"     - Gi√° v√© TB: ${high_potential['fare'].mean():.2f}\")\n",
    "print(f\"     - Xu h∆∞·ªõng tƒÉng tr∆∞·ªüng: {high_potential['growth_trend'].mean():.3f}\")\n",
    "print(\"   üéØ Khuy·∫øn ngh·ªã:\")\n",
    "print(\"     ‚Ä¢ TƒÉng t·∫ßn su·∫•t bay v√† m·ªü r·ªông ƒë·ªôi bay\")\n",
    "print(\"     ‚Ä¢ ƒê·∫ßu t∆∞ v√†o d·ªãch v·ª• cao c·∫•p ƒë·ªÉ tƒÉng gi√° v√©\")\n",
    "print(\"     ‚Ä¢ Ph√°t tri·ªÉn th√™m tuy·∫øn k·∫øt n·ªëi\")\n",
    "print(\"     ‚Ä¢ ∆Øu ti√™n marketing v√† qu·∫£ng b√°\")\n",
    "\n",
    "# Nh√≥m ti·ªÅm nƒÉng trung b√¨nh\n",
    "medium_potential = df_potential[df_potential['potential_level_enhanced'] == 'Trung b√¨nh']\n",
    "print(f\"\\n‚öñÔ∏è NH√ìM TI·ªÄM NƒÇNG TRUNG B√åNH ({len(medium_potential)} tuy·∫øn bay):\")\n",
    "print(\"   ‚úÖ ƒê·∫∑c ƒëi·ªÉm:\")\n",
    "print(f\"     - L∆∞u l∆∞·ª£ng TB: {medium_potential['passengers'].mean():.0f} h√†nh kh√°ch\")\n",
    "print(f\"     - Gi√° v√© TB: ${medium_potential['fare'].mean():.2f}\")\n",
    "print(f\"     - Xu h∆∞·ªõng tƒÉng tr∆∞·ªüng: {medium_potential['growth_trend'].mean():.3f}\")\n",
    "print(\"   üéØ Khuy·∫øn ngh·ªã:\")\n",
    "print(\"     ‚Ä¢ T·ªëi ∆∞u h√≥a chi ph√≠ v·∫≠n h√†nh\")\n",
    "print(\"     ‚Ä¢ C·∫£i thi·ªán d·ªãch v·ª• ƒë·ªÉ tƒÉng l∆∞u l∆∞·ª£ng\")\n",
    "print(\"     ‚Ä¢ Ph√¢n t√≠ch c·∫°nh tranh v√† ƒëi·ªÅu ch·ªânh gi√°\")\n",
    "print(\"     ‚Ä¢ Th·ª≠ nghi·ªám c√°c chi·∫øn l∆∞·ª£c marketing m·ªõi\")\n",
    "\n",
    "# Nh√≥m ti·ªÅm nƒÉng th·∫•p\n",
    "low_potential = df_potential[df_potential['potential_level_enhanced'] == 'Th·∫•p']\n",
    "print(f\"\\n‚ö†Ô∏è NH√ìM TI·ªÄM NƒÇNG TH·∫§P ({len(low_potential)} tuy·∫øn bay):\")\n",
    "print(\"   ‚úÖ ƒê·∫∑c ƒëi·ªÉm:\")\n",
    "print(f\"     - L∆∞u l∆∞·ª£ng TB: {low_potential['passengers'].mean():.0f} h√†nh kh√°ch\")\n",
    "print(f\"     - Gi√° v√© TB: ${low_potential['fare'].mean():.2f}\")\n",
    "print(f\"     - Xu h∆∞·ªõng tƒÉng tr∆∞·ªüng: {low_potential['growth_trend'].mean():.3f}\")\n",
    "print(\"   üéØ Khuy·∫øn ngh·ªã:\")\n",
    "print(\"     ‚Ä¢ Xem x√©t gi·∫£m t·∫ßn su·∫•t ho·∫∑c ng·ª´ng khai th√°c\")\n",
    "print(\"     ‚Ä¢ T·∫≠p trung v√†o c√°c tuy·∫øn c√≥ ti·ªÅm nƒÉng cao h∆°n\")\n",
    "print(\"     ‚Ä¢ N·∫øu duy tr√¨, c·∫ßn c·∫Øt gi·∫£m chi ph√≠ t·ªëi ƒëa\")\n",
    "print(\"     ‚Ä¢ Ph√¢n t√≠ch nguy√™n nh√¢n v√† t√¨m gi·∫£i ph√°p c·∫£i thi·ªán\")\n",
    "\n",
    "# 5. T√≥m t·∫Øt k·∫øt qu·∫£\n",
    "print(f\"\\nüìã T√ìM T·∫ÆT K·∫æT QU·∫¢ PH√ÇN LO·∫†I:\")\n",
    "print(\"=\"*40)\n",
    "print(f\"üéØ T·ªïng s·ªë tuy·∫øn bay ƒë∆∞·ª£c ph√¢n t√≠ch: {len(df_potential):,}\")\n",
    "print(f\"üìä Ph√¢n b·ªë:\")\n",
    "for level in ['Cao', 'Trung b√¨nh', 'Th·∫•p']:\n",
    "    count = len(df_potential[df_potential['potential_level_enhanced'] == level])\n",
    "    percentage = count / len(df_potential) * 100\n",
    "    print(f\"   ‚Ä¢ Ti·ªÅm nƒÉng {level}: {count:,} tuy·∫øn ({percentage:.1f}%)\")\n",
    "\n",
    "print(f\"\\nü§ñ Hi·ªáu su·∫•t m√¥ h√¨nh Random Forest:\")\n",
    "print(f\"   ‚Ä¢ ƒê·ªô ch√≠nh x√°c: {rf_accuracy:.4f}\")\n",
    "print(f\"   ‚Ä¢ ƒê·ªô ·ªïn ƒë·ªãnh (CV): {cv_scores.mean():.4f} ¬± {cv_scores.std():.4f}\")\n",
    "\n",
    "print(f\"\\n‚ú® K·∫øt lu·∫≠n:\")\n",
    "print(\"   M√¥ h√¨nh ph√¢n lo·∫°i ƒë√£ ƒë∆∞·ª£c x√¢y d·ª±ng th√†nh c√¥ng v·ªõi ƒë·ªô ch√≠nh x√°c cao.\")\n",
    "print(\"   C√°c khuy·∫øn ngh·ªã chi·∫øn l∆∞·ª£c ƒë√£ ƒë∆∞·ª£c ƒë∆∞a ra cho t·ª´ng nh√≥m tuy·∫øn bay.\")\n",
    "print(\"   D·ªØ li·ªáu n√†y c√≥ th·ªÉ ƒë∆∞·ª£c s·ª≠ d·ª•ng ƒë·ªÉ h·ªó tr·ª£ quy·∫øt ƒë·ªãnh kinh doanh.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac80bf05",
   "metadata": {},
   "source": [
    "# üéØ K·∫æT LU·∫¨N V√Ä T·ªîNG K·∫æT\n",
    "\n",
    "## ‚úÖ Nh·ªØng g√¨ ƒë√£ ho√†n th√†nh:\n",
    "\n",
    "### 1. **Ph√¢n t√≠ch d·ªØ li·ªáu to√†n di·ªán**\n",
    "- L√†m s·∫°ch v√† x·ª≠ l√Ω 238,516 b·∫£n ghi d·ªØ li·ªáu tuy·∫øn bay\n",
    "- X·ª≠ l√Ω missing values v√† outliers\n",
    "- Chu·∫©n h√≥a d·ªØ li·ªáu cho ph√¢n t√≠ch\n",
    "\n",
    "### 2. **X√¢y d·ª±ng h·ªá th·ªëng ƒë√°nh gi√° ti·ªÅm nƒÉng**\n",
    "- T·∫°o ƒëi·ªÉm s·ªë ti·ªÅm nƒÉng (Potential Score) d·ª±a tr√™n 5 y·∫øu t·ªë ch√≠nh\n",
    "- Ph√¢n t√≠ch xu h∆∞·ªõng tƒÉng tr∆∞·ªüng theo th·ªùi gian\n",
    "- Ph√¢n lo·∫°i tuy·∫øn bay th√†nh 3 nh√≥m: **Cao**, **Trung b√¨nh**, **Th·∫•p**\n",
    "\n",
    "### 3. **√Åp d·ª•ng Machine Learning**\n",
    "- Hu·∫•n luy·ªán 3 m√¥ h√¨nh: Random Forest, Gradient Boosting, Logistic Regression\n",
    "- ƒê√°nh gi√° hi·ªáu su·∫•t v√† ch·ªçn m√¥ h√¨nh t·ªët nh·∫•t\n",
    "- Ph√¢n t√≠ch t·∫ßm quan tr·ªçng c·ªßa c√°c ƒë·∫∑c tr∆∞ng\n",
    "\n",
    "### 4. **Tr·ª±c quan h√≥a v√† ph√¢n t√≠ch**\n",
    "- 6 bi·ªÉu ƒë·ªì ph√¢n t√≠ch ƒëa chi·ªÅu\n",
    "- Ma tr·∫≠n t∆∞∆°ng quan\n",
    "- Ph√¢n t√≠ch xu h∆∞·ªõng theo nƒÉm\n",
    "- Confusion matrix v√† ƒë√°nh gi√° hi·ªáu su·∫•t\n",
    "\n",
    "### 5. **Khuy·∫øn ngh·ªã chi·∫øn l∆∞·ª£c**\n",
    "- Khuy·∫øn ngh·ªã c·ª• th·ªÉ cho t·ª´ng nh√≥m ti·ªÅm nƒÉng\n",
    "- Ph√¢n t√≠ch ƒë·∫∑c ƒëi·ªÉm v√† h∆∞·ªõng ph√°t tri·ªÉn\n",
    "- H·ªó tr·ª£ quy·∫øt ƒë·ªãnh kinh doanh\n",
    "\n",
    "## üöÄ ·ª®ng d·ª•ng th·ª±c t·∫ø:\n",
    "\n",
    "1. **L·∫≠p k·∫ø ho·∫°ch m·ªü r·ªông m·∫°ng l∆∞·ªõi**: T·∫≠p trung v√†o c√°c tuy·∫øn c√≥ ti·ªÅm nƒÉng cao\n",
    "2. **T·ªëi ∆∞u h√≥a t√†i nguy√™n**: Ph√¢n b·ªï m√°y bay v√† nh√¢n l·ª±c hi·ªáu qu·∫£\n",
    "3. **Chi·∫øn l∆∞·ª£c gi√°**: ƒêi·ªÅu ch·ªânh gi√° v√© theo ti·ªÅm nƒÉng t·ª´ng tuy·∫øn\n",
    "4. **Marketing**: T·∫≠p trung qu·∫£ng b√° v√†o c√°c tuy·∫øn c√≥ ti·ªÅm nƒÉng\n",
    "5. **Qu·∫£n l√Ω r·ªßi ro**: X√°c ƒë·ªãnh c√°c tuy·∫øn c·∫ßn c·∫£i thi·ªán ho·∫∑c ng·ª´ng khai th√°c\n",
    "\n",
    "## üìä K·∫øt qu·∫£ ch√≠nh:\n",
    "- **ƒê·ªô ch√≠nh x√°c m√¥ h√¨nh**: > 99%\n",
    "- **Ph√¢n b·ªë ti·ªÅm nƒÉng**: C√¢n b·∫±ng gi·ªØa 3 nh√≥m\n",
    "- **T√≠nh kh·∫£ thi**: C√≥ th·ªÉ √°p d·ª•ng ngay v√†o th·ª±c t·∫ø\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
